[
  {
    "objectID": "website_docs/data_snps.html",
    "href": "website_docs/data_snps.html",
    "title": "Single nucleotide polymorphism (SNP) data",
    "section": "",
    "text": "SNP barcode data from the sanger 100 SNP Plasmodium falciparum barcode (Chang et al. 2019).\nsanger101_snp_barcode_withGenes.bed\n\n\nThe barcode was subsetted from the above WGS data to just the sanger barcode for the Vietnam and DRC data. The results file can be found within directory snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz, snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.DRCongo.vcf.gz\n\n\n\nThe barcode was also explicitly called with several monoclonal lab isolates and then lab created mixtures of these isolates. Data can be found snp_barcode/controls_sanger100.vcf.gz with meta data with what mixtures are what found snp_barcode/allControlMixtures.tab.txt and snp_barcode/allControlSampNameToMixName.tab.txt\n\n\n\nThe barcode was also simulated for 100 samples (50 Bangladesh and 50 Ghana). Data can be found snp_barcode/SpotMalariapfPanel_simData_sanger100.vcf.gz. The simulations were created by simulating super infections by sampling the barcode from each of these countries and selecting COIs based on the COIs observed for each country. To use data without indels, the data can be found snp_barcode/SpotMalariapfPanel_simData_snponly_sanger100.vcf.gz.",
    "crumbs": [
      "Data",
      "SNPs"
    ]
  },
  {
    "objectID": "website_docs/data_snps.html#snp-barcoding-data",
    "href": "website_docs/data_snps.html#snp-barcoding-data",
    "title": "Single nucleotide polymorphism (SNP) data",
    "section": "",
    "text": "SNP barcode data from the sanger 100 SNP Plasmodium falciparum barcode (Chang et al. 2019).\nsanger101_snp_barcode_withGenes.bed\n\n\nThe barcode was subsetted from the above WGS data to just the sanger barcode for the Vietnam and DRC data. The results file can be found within directory snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz, snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.DRCongo.vcf.gz\n\n\n\nThe barcode was also explicitly called with several monoclonal lab isolates and then lab created mixtures of these isolates. Data can be found snp_barcode/controls_sanger100.vcf.gz with meta data with what mixtures are what found snp_barcode/allControlMixtures.tab.txt and snp_barcode/allControlSampNameToMixName.tab.txt\n\n\n\nThe barcode was also simulated for 100 samples (50 Bangladesh and 50 Ghana). Data can be found snp_barcode/SpotMalariapfPanel_simData_sanger100.vcf.gz. The simulations were created by simulating super infections by sampling the barcode from each of these countries and selecting COIs based on the COIs observed for each country. To use data without indels, the data can be found snp_barcode/SpotMalariapfPanel_simData_snponly_sanger100.vcf.gz.",
    "crumbs": [
      "Data",
      "SNPs"
    ]
  },
  {
    "objectID": "website_docs/software_standards.html",
    "href": "website_docs/software_standards.html",
    "title": "Software standards",
    "section": "",
    "text": "PGEforge aims to foster an ecosystem of high-quality, user-friendly tools that can be seamlessly integrated into genomic analysis workflows. One of the biggest challenges is the variability and lack of systematic assessment of existing tools, which often do not adhere to best practices in software development, including FAIR standards, maintenance, and usability.\nWorking towards this goal, a robust software standards evaluation framework was formulated to guide the development and assessment of tools used in Plasmodium genomic data analysis from both the end-user and developer perspective. This framework is crucial in addressing the variability and challenges associated with existing software tools but also to guide development of new tools, ensuring that they meet high standards of usability, accessibility, and reliability.\n\n\nOne of the primary objectives of this framework is to define ‘ideal’ software practices that are not tool-specific but applicable across a range of genomic analysis tools. These practices encompass:\n\nComprehensive documentation\nEase of installation\nReliable and maintainable software\n\nAdditionally, during the 2023 RADISH23 hackathon, focused discussions highlighted the following software practices that are not necessarily essential, but are “nice-to-have’s”:\n\nUses standard data input formats\nComputationally efficient\nInformative error handling\nMultiple languages for tutorials\nMinimal dependencies\nModular code (eg split into functions)\nWell annotated code\n\nThese practices can guide development of new tools and/or improvement of existing tools.\n\n\n\nTo implement these standards and facilitate tool evaluation and development, PGEforge has developed a set of measurable criteria that can be applied to evaluate the performance and usability of various tools. There are two categories:\n\nUser-facing: criteria to evaluate the tool from an end-user perspective, for example whether installation instructions are available and easy-to-follow\nDeveloper-facing: criteria to evaluate the tool from a developer perspective, for example whether unit tests are implemented\n\n\n\nThe evaluation criteria encompass the following key themes, in line with the ‘ideal’ software practices for both end-users and developers:\n\nQuality and comprehensiveness of documentation\nSimplicity of installation processes\nQuality assurance and maintenance\n\n\n\n\n\n\nCriteria\nType\nNotes\n\n\n\n\nUser-facing\n\n\nInstallation instructions exist\nbinary\n\n\n\nUsage instructions exist\nbinary\n\n\n\nTutorials exist\nbinary\nMust have explanation of inputs, outputs, and test data set in a worked example\n\n\nTest data sets and results available\nbinary\n\n\n\nDeveloper-facing\n\n\nOpen source\nbinary\n\n\n\nHas software tests\nbinary\nEg, unit tests\n\n\nMore than 90% code coverage reported\nbinary\n\n\n\nClear channels for software maintenance and issues\nbinary\nEg, GitHub issues, author contact information\n\n\n\n\n\n\n\n  Every criteria is scored on the following scale:\n\n0: Criteria not fulfilled\n1: Criteria fulfilled but not entirely\n2: Criteria fulfilled\n\n This is then translated to an end-user score and development score for the tool.\n\n\n\nEvery Plasmodium genomic analysis tool can be evaluated against these objective software standards to provide these scores. The resulting evaluation matrix and overview of each tool can be found here.",
    "crumbs": [
      "Software standards"
    ]
  },
  {
    "objectID": "website_docs/software_standards.html#framework-for-evaluating-software-standards-in-plasmodium-genomics",
    "href": "website_docs/software_standards.html#framework-for-evaluating-software-standards-in-plasmodium-genomics",
    "title": "Software standards",
    "section": "",
    "text": "PGEforge aims to foster an ecosystem of high-quality, user-friendly tools that can be seamlessly integrated into genomic analysis workflows. One of the biggest challenges is the variability and lack of systematic assessment of existing tools, which often do not adhere to best practices in software development, including FAIR standards, maintenance, and usability.\nWorking towards this goal, a robust software standards evaluation framework was formulated to guide the development and assessment of tools used in Plasmodium genomic data analysis from both the end-user and developer perspective. This framework is crucial in addressing the variability and challenges associated with existing software tools but also to guide development of new tools, ensuring that they meet high standards of usability, accessibility, and reliability.\n\n\nOne of the primary objectives of this framework is to define ‘ideal’ software practices that are not tool-specific but applicable across a range of genomic analysis tools. These practices encompass:\n\nComprehensive documentation\nEase of installation\nReliable and maintainable software\n\nAdditionally, during the 2023 RADISH23 hackathon, focused discussions highlighted the following software practices that are not necessarily essential, but are “nice-to-have’s”:\n\nUses standard data input formats\nComputationally efficient\nInformative error handling\nMultiple languages for tutorials\nMinimal dependencies\nModular code (eg split into functions)\nWell annotated code\n\nThese practices can guide development of new tools and/or improvement of existing tools.\n\n\n\nTo implement these standards and facilitate tool evaluation and development, PGEforge has developed a set of measurable criteria that can be applied to evaluate the performance and usability of various tools. There are two categories:\n\nUser-facing: criteria to evaluate the tool from an end-user perspective, for example whether installation instructions are available and easy-to-follow\nDeveloper-facing: criteria to evaluate the tool from a developer perspective, for example whether unit tests are implemented\n\n\n\nThe evaluation criteria encompass the following key themes, in line with the ‘ideal’ software practices for both end-users and developers:\n\nQuality and comprehensiveness of documentation\nSimplicity of installation processes\nQuality assurance and maintenance\n\n\n\n\n\n\nCriteria\nType\nNotes\n\n\n\n\nUser-facing\n\n\nInstallation instructions exist\nbinary\n\n\n\nUsage instructions exist\nbinary\n\n\n\nTutorials exist\nbinary\nMust have explanation of inputs, outputs, and test data set in a worked example\n\n\nTest data sets and results available\nbinary\n\n\n\nDeveloper-facing\n\n\nOpen source\nbinary\n\n\n\nHas software tests\nbinary\nEg, unit tests\n\n\nMore than 90% code coverage reported\nbinary\n\n\n\nClear channels for software maintenance and issues\nbinary\nEg, GitHub issues, author contact information\n\n\n\n\n\n\n\n  Every criteria is scored on the following scale:\n\n0: Criteria not fulfilled\n1: Criteria fulfilled but not entirely\n2: Criteria fulfilled\n\n This is then translated to an end-user score and development score for the tool.\n\n\n\nEvery Plasmodium genomic analysis tool can be evaluated against these objective software standards to provide these scores. The resulting evaluation matrix and overview of each tool can be found here.",
    "crumbs": [
      "Software standards"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#the-pgeforge-vision",
    "href": "website_docs/how_to_contribute.html#the-pgeforge-vision",
    "title": "Get involved",
    "section": "The PGEforge vision",
    "text": "The PGEforge vision\nPGEforge is a community-driven platform created by and for malaria genomic epidemiology analysis tool developers and end-users. If you are interested in contributing to these efforts, please get involved!",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#how-to-contribute-to-pgeforge",
    "href": "website_docs/how_to_contribute.html#how-to-contribute-to-pgeforge",
    "title": "Get involved",
    "section": "How to contribute to PGEforge",
    "text": "How to contribute to PGEforge\nWe are excited to have you join us and we welcome input from all areas of the research community to continuously improve and expand our platform. PGEforge aims to contribute to an inclusive and sustainable ecosystem of existing and new tools for Plasmodium genomic data analysis. From less to more involvement, there are several ways to get involved and join us in contributing to these aims. All contributors must adhere to our community rules and will be recognized as a contributor.\nBelow are some ways to get involved with current workstreams, but we also have some ideas and plans in the works for the future beyond what is currently available in PGEforge! If you want to contribute in another way that is not currently listed, please reach out to us directly.",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#add-tool-to-landscaping-and-software-standards-matrices",
    "href": "website_docs/how_to_contribute.html#add-tool-to-landscaping-and-software-standards-matrices",
    "title": "Get involved",
    "section": "Add tool to landscaping and software standards matrices",
    "text": "Add tool to landscaping and software standards matrices\nLandscaping, documenting and benchmarking available Plasmodium genomic data analysis tools and any tools within the realm of Plasmodium genomic epidemiology (PGE) will require continuous updating and curation. As new tools become available, they need to be assessed and integrated into our existing framework. While we have made significant progress in identifying, documenting and evaluating tools commonly applied to Plasmodium genetic data, the tool landscaping and evaluation against software standards is not exhaustive. For example, our initial efforts focused on tools for downstream Plasmodium genetic analysis, specifically targeting tools that extract signals from already processed data and focusing only on tools applicable P. falciparum and P. vivax. In terms of tool evaluation, we focused our efforts on evaluating the tools for which resources were developed in the tutorials section. Future work could also involve other types of tools, however.\n\n\nSo far, PGEforge hosts a landscaping matrix of 40 identified tools, full documentation and evaluation of 17 tools, and complete resources (summary documents, installation instructions, and fully-worked through tutorials) developed for 11 tools.\nIn order to ensure this ‘live document’ remains up-to-date, we encourage contributions to the following areas:\n\nUpdating the documentation and filling in the gaps for remaining tools in the landscaping matrix\nEvaluating remaining tools based on software standards (these tools are highlighted in grey in the overview by tools based on software standards)\nUpdating tool scores against software standards if developers release tool updates\nAdd a new tool to landscaping and software standard matrices\n\nTo contribute to any of these areas, there are two ways to do this depending on your comfort level with using GitHub and R.\n\n\nDon’t forget to also add yourself as a contributor!\n\nDownload the relevant .csv files and edit them locally. Then open an issue on Github and attach your updated .csv files. The location of the files are described below:\n\n\nMMS_software_landscaping.csv: this is the tool landscaping matrix.\nTools_to_standards.csv: this is the evaluation of each tool with respect to the objective software standards. Please make sure you follow the evaluation criteria rubric when you score each of the criteria.\n\nPlease ensure you only modify the relevant cells or add new rows as needed\n\nFollow the Github contributor guidelines to create your own branch and PR your changes directly. Make sure you:\n\n\nAdd your headshot photo to the website_docs/img/people folder\nAdd your name in alphabetical order to the website_docs/contributors.qmd and reference your photo by using the following code ![](img/people/yourname.jpeg){fig-align=\"left\" width=\"100px\"}.",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#develop-resources-for-a-tool",
    "href": "website_docs/how_to_contribute.html#develop-resources-for-a-tool",
    "title": "Get involved",
    "section": "Develop resources for a tool",
    "text": "Develop resources for a tool\nIf you have developed a new Plasmodium genomic data analysis tool or identified a tool that is missing tutorial resources, you can develop them and contribute them to PGEforge! Follow these steps to contribute:\n\n\nDon’t forget to also add yourself as a contributor!\nStep 1: In PGEforge we focus on Plasmodium genomic data analysis tools. The tool should be within the scope of Plasmodium genomic epidemiology data analysis.\nStep 2: Take a look at the tool landscaping in case we have already evaluated that specific tool. In some instances, we opted to not develop tutorial resources for tools where there was difficulty installing and running the software, run-time errors, or if the tool was clearly superseded by more recent tools. You will find more details in the tool landscaping. If you identify a tool that is not in the landscaping matrix, please add it.\nStep 3: If you have reached this point and have found a tool that is missing from PGEforge, great! After adding it to the landscaping and software standards matrices you can develop the tool resources. We provide all the templates that you need to get started. For every tool, we develop the following resources:\n\nSummary Document: Provide an overview of the tool, including its main purpose, use cases, license, code repository, relevant publications, and citation details. Fill in these details in the template .csv file and then “Render” the accompanying .qmd file to display the information in a nicely formatted table.\nInstallation Instructions: Write clear, step-by-step instructions for installing the tool, ensuring it is accessible to users with varying technical skills. If the tool is an R package and not already in the PlasmoGenEpi R-universe, please let us know! Otherwise, we strongly suggest you instruct users to install from R-universe.\nFully Worked Tutorials: Develop comprehensive tutorials that guide users through the entire process of using the tool to perform a specific analysis, from data import to interpretation of results. PGEforge hosts both empirical and simulated canonical datasets for the commonly used data input formats. We strongly encourage you to use these when you develop these tutorials as this ensures that they are fully reproducible.\n\nYou can find the templates for all of these documents in our Github repository and example tutorials in the Tutorials section.\nPlease make sure to follow our GitHub contribution guidelines.",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#add-yourself-as-a-contributor",
    "href": "website_docs/how_to_contribute.html#add-yourself-as-a-contributor",
    "title": "Get involved",
    "section": "Add yourself as a contributor",
    "text": "Add yourself as a contributor\nEveryone who contributes to PGEforge is listed as a contributor.\nThere are two ways to do this:\n\nOpen an issue on Github and make sure you include the following information so we can add you to the contributors list:\n\n\nFull name\nAffiliations\nHeadshot photo\n\n\nFollow the Github contributor guidelines to create your own branch and PR your changes directly. Make sure you:\n\n\nAdd your headshot photo to the website_docs/img/people folder\nAdd your name in alphabetical order to the website_docs/contributors.qmd and reference your photo by using the following code ![](img/people/yourname.jpeg){fig-align=\"left\" width=\"100px\"}.",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#contributing-guidelines-for-github",
    "href": "website_docs/how_to_contribute.html#contributing-guidelines-for-github",
    "title": "Get involved",
    "section": "Contributing guidelines for GitHub",
    "text": "Contributing guidelines for GitHub\nTo ensure a smooth and efficient collaboration process, please follow these guidelines when contributing to our GitHub repository:\n\nCreate a New Branch: Start by creating a new branch from the develop branch. This keeps your changes separate until they are ready to be reviewed and merged.\nPull Request (PR) for Review: Once you have made your changes, create a PR into the develop branch. Our team will review your PR and provide feedback or approve it for merging. Never make any changes to the main branch, and please always PR into develop.",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#thank-you",
    "href": "website_docs/how_to_contribute.html#thank-you",
    "title": "Get involved",
    "section": "Thank you!",
    "text": "Thank you!\nThank you for your interest in contributing to PGEforge. Your efforts help us build a stronger, more inclusive research community making Plasmodium genomic analysis accessible to all!",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/how_to_contribute.html#community-rules",
    "href": "website_docs/how_to_contribute.html#community-rules",
    "title": "Get involved",
    "section": "Community rules",
    "text": "Community rules\nPGEforge is dedicated to creating an inclusive, respectful, and engaging community. We believe in open collaboration and active participation, empowering each other to share and gain knowledge, resources, and opportunities as a community. We believe in the benefits of a wide range of perspectives, experiences and ideas. We welcome contributions from everyone who shares our goals and wants to contribute, regardless of age, gender identity, sexual orientation, disability, ethnicity, nationality, race, religion, education, level of experience, career stage, or socioeconomic status. PGEforge aims to foster a harassment-free experience for everyone and expects all contributors to demonstrate empathy, kindness, and respect, and to engage constructively with differing viewpoints. Unacceptable behaviors, including harassment, discriminatory language, and personal attacks, will not be tolerated. By fostering a positive and inclusive community, we aim to empower everyone to contribute and collaborate effectively to a vibrant and growing PGEforge community.",
    "crumbs": [
      "How to contribute",
      "Get involved"
    ]
  },
  {
    "objectID": "website_docs/tutorials_overview.html",
    "href": "website_docs/tutorials_overview.html",
    "title": "Tool resources",
    "section": "",
    "text": "The aim of the PGEforge resources is to provide worked examples that show how to use a tool by working through code. This involves code showing you how to install the tool, what input data formats you need to use, how to wrangle the data (if applicable), and how to use the tool functionalities to analyse data. Summary documents detailing the purpose of each tool can also help you decide which tool you may want to use for a certain application.\nThe following resources are available for each tool:\n\nA summary document detailing the main purpose and use cases, license, code repository, relevant publication(s), citation information and links to any additional resources\nComplete installation instructions\nA fully reproducible and worked-through tutorial showing example usage of the tool and its functionalities. This often uses the canonical simulated or empirical datasets as input data\n(Coming soon) For some tools, we have undertaken systematic benchmarking on simulated data to evaluate performance and accuracy",
    "crumbs": [
      "Tool resources"
    ]
  },
  {
    "objectID": "website_docs/tutorials_overview.html#overview",
    "href": "website_docs/tutorials_overview.html#overview",
    "title": "Tool resources",
    "section": "",
    "text": "The aim of the PGEforge resources is to provide worked examples that show how to use a tool by working through code. This involves code showing you how to install the tool, what input data formats you need to use, how to wrangle the data (if applicable), and how to use the tool functionalities to analyse data. Summary documents detailing the purpose of each tool can also help you decide which tool you may want to use for a certain application.\nThe following resources are available for each tool:\n\nA summary document detailing the main purpose and use cases, license, code repository, relevant publication(s), citation information and links to any additional resources\nComplete installation instructions\nA fully reproducible and worked-through tutorial showing example usage of the tool and its functionalities. This often uses the canonical simulated or empirical datasets as input data\n(Coming soon) For some tools, we have undertaken systematic benchmarking on simulated data to evaluate performance and accuracy",
    "crumbs": [
      "Tool resources"
    ]
  },
  {
    "objectID": "website_docs/tutorials_overview.html#how-to-contribute",
    "href": "website_docs/tutorials_overview.html#how-to-contribute",
    "title": "Tool resources",
    "section": "How to contribute",
    "text": "How to contribute\nThis is a live resource and we plan to continue adding to this as new tools become available! We hope this will grow into a common resource for analysis of malaria genetic data.\nIf you are interested in contributing, there are templates available and instructions on how to get started.",
    "crumbs": [
      "Tool resources"
    ]
  },
  {
    "objectID": "website_docs/tools_to_functions.html",
    "href": "website_docs/tools_to_functions.html",
    "title": "Tools to functions",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "website_docs/workflows_overview.html",
    "href": "website_docs/workflows_overview.html",
    "title": "Analysis workflows",
    "section": "",
    "text": "Prior efforts in the community have defined use cases for genetic epidemiology informing malaria elimination efforts (Dalmat et al. 2019). When it comes down to putting these into practice, it is not always clear what analysis functionalities are required to obtain the desired results nor which tools are capable of performing them. For example, to estimate the prevalence of polymorphisms associated with drug resistance there may be a series of steps that we need to perform to estimate various parameters and obtain the desired results from Plasmodium genetic data.\nDeveloping such analysis workflows is non-trivial as it requires mapping this out and detailing how functionalities (and available tools) can be chained together in a cohesive and flexible workflow for each use case.\nWe aimed to synthesize the main use cases based on analysis requirements, including what functionalities are required or useful in order to perform the necessary analyses. Based on expert discussion during the 2023 RADISH23 hackathon, we defined eight main use cases for Plasmodium genetic data informing malaria control and elimination (Ruybal-Pesántez et al. 2025). For each use case, we identified specific functionalities needed for analysis. For a subset of use cases, we then outlined how these functionalities could be assembled into analysis workflows to obtain required results from the initial genetic or genomic data.",
    "crumbs": [
      "Analysis workflows"
    ]
  },
  {
    "objectID": "website_docs/workflows_overview.html#overview",
    "href": "website_docs/workflows_overview.html#overview",
    "title": "Analysis workflows",
    "section": "",
    "text": "Prior efforts in the community have defined use cases for genetic epidemiology informing malaria elimination efforts (Dalmat et al. 2019). When it comes down to putting these into practice, it is not always clear what analysis functionalities are required to obtain the desired results nor which tools are capable of performing them. For example, to estimate the prevalence of polymorphisms associated with drug resistance there may be a series of steps that we need to perform to estimate various parameters and obtain the desired results from Plasmodium genetic data.\nDeveloping such analysis workflows is non-trivial as it requires mapping this out and detailing how functionalities (and available tools) can be chained together in a cohesive and flexible workflow for each use case.\nWe aimed to synthesize the main use cases based on analysis requirements, including what functionalities are required or useful in order to perform the necessary analyses. Based on expert discussion during the 2023 RADISH23 hackathon, we defined eight main use cases for Plasmodium genetic data informing malaria control and elimination (Ruybal-Pesántez et al. 2025). For each use case, we identified specific functionalities needed for analysis. For a subset of use cases, we then outlined how these functionalities could be assembled into analysis workflows to obtain required results from the initial genetic or genomic data.",
    "crumbs": [
      "Analysis workflows"
    ]
  },
  {
    "objectID": "website_docs/tutorial_guidelines.html",
    "href": "website_docs/tutorial_guidelines.html",
    "title": "Tutorial guidelines",
    "section": "",
    "text": "Some guidelines here:\n\nexplanation of directory structure (ie tutorials/MALECOT)\ndescribe example tutorial and what should be included"
  },
  {
    "objectID": "website_docs/tutorial_guidelines.html#how-to-prepare-your-tutorial",
    "href": "website_docs/tutorial_guidelines.html#how-to-prepare-your-tutorial",
    "title": "Tutorial guidelines",
    "section": "",
    "text": "Some guidelines here:\n\nexplanation of directory structure (ie tutorials/MALECOT)\ndescribe example tutorial and what should be included"
  },
  {
    "objectID": "website_docs/use_cases.html",
    "href": "website_docs/use_cases.html",
    "title": "Main use cases for Plasmodium genetic data informing malaria control and elimination",
    "section": "",
    "text": "These use cases have been described in (Ruybal-Pesántez et al. 2025). Each use case will require a different set of analysis functionalities to obtain the desired results from Plasmodium genetic data (see next page).\n\nIdentifying the molecular mechanism/origin of drug and diagnostic resistance\nStudies are initially needed to link parasite genetic variation to phenotypes of drug and diagnostic resistance, for example by sequencing parasites with corresponding data on in vitro response to antimalarials or with evidence of clinical failure of drugs or diagnostic tests. Related studies can identify potential polymorphisms of interest via signals of selection and identify how they are evolving and spreading at various temporal and spatial scales.\n\n\nMonitoring the prevalence/frequency of drug or diagnostic resistance markers\nOnce genetic polymorphisms have been associated with resistance phenotypes, they can be used as markers of resistance. Marker prevalence (fraction of infections that contain parasites with the resistance marker) and/or frequency (relative abundance of the resistance marker in the parasite population) can be monitored for surveillance of resistance. Various statistical methods may be needed to estimate prevalence/frequency because polyclonal infections complicate otherwise simple analyses.\n\n\nMeasuring human immune selection on the parasite population\nIdentifying areas of the Plasmodium genome under immune selection can help identify potential targets for vaccines and related immunologic interventions such as monoclonal antibodies. Related to this, monitoring sequences of such targets can be useful in the context of large-scale implementation to evaluate for selection and monitor for potential escape mutations.\n\n\nClassifying outcomes in therapeutic efficacy studies (TESs) as reinfection, recrudescence or, in the case of P. vivax, relapse\nA TES performed by prospective evaluation of patients’ responses to treatment for uncomplicated malaria is the gold standard for assessing the efficacy of antimalarial drugs. Genotyping of participants with infections during follow up is needed to distinguish whether they failed therapy (recrudescence), were infected again (reinfection), or had a relapse of a dormant parasite.\n\n\nEstimating transmission intensity\nPopulation level measures derived from parasite genetic data, like other indicators such as parasite prevalence, may be used to estimate malaria transmission intensity for surveillance purposes such as stratification of interventions or to evaluate the effect of interventions.\n\n\nEstimating the connectivity and movement of parasites between geographically distinct populations\nIdentifying whether and to what extent parasites from one geographic area are moving to another can help predict the spread of malaria, as well as the spread of parasites with specific concerning phenotypes such as drug resistance. This in turn can inform planning of the scale and timing of interventions. For example, it may be easier to reduce the burden of malaria in an isolated area compared to one that is more connected to areas of higher transmission. This information may be identifiable by comparing genetic measures between populations.\n\n\nClassifying malaria cases as locally acquired or imported from another population\nIn elimination or pre-elimination settings, understanding if cases are imported is relevant to 1) assessing the feasibility of elimination (i.e. elimination is less feasible in areas with a high number of imported infections that may lead to local transmission) and 2) determining if local elimination has been achieved despite reported cases.\n\n\nReconstructing granular patterns of transmission\nIn areas of very low transmission, characterizing granular details of transmission events — such as chains of transmission between individuals/households, demographic groups at high risk of being infected with or transmitting malaria, hidden reservoirs of residual transmission, contribution of imported cases to local transmission, and the length of sustained transmission chains may help guide and evaluate targeted elimination strategies. This information may be identifiable through studies that sample infected people comprehensively and utilize high resolution genetic data to evaluate relationships between infections.\n\n\n\n\n\n Back to topReferences\n\nRuybal-Pesántez, Shazia, Jorge Amaya-Romero, Sophie Bérubé, Nicholas F. Brazeau, Mouhamadou Fadel Diop, Nicholas Hathaway, Jason Hendry, et al. 2025. “Towards an Open Analysis Ecosystem for Plasmodium Genomic Epidemiology.” http://dx.doi.org/10.1101/2025.04.01.25325032.",
    "crumbs": [
      "Analysis workflows",
      "Use cases"
    ]
  },
  {
    "objectID": "website_docs/tools_to_standards.html",
    "href": "website_docs/tools_to_standards.html",
    "title": "Overview of tools based on software standards",
    "section": "",
    "text": "The following Plasmodium genomic analysis tools were identified during tool landscaping and were evaluating using the software standards criteria to determine an end-user and development score for each tool.\nIn line with the scope of PGEforge, we focus our efforts on evaluating available tools that are commonly applied to Plasmodium genetic data and that focus on downstream analysis. In other words tools with the primary goal of extracting signal from pre-processed data, not those focused on upstream bioinformatic data processing.\nIf you would like to contribute to this effort, please take a look at our contributor guidelines!\nNote: tools in grey have not yet been evaluated\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Software standards",
      "Overview by tool"
    ]
  },
  {
    "objectID": "website_docs/tool_landscaping.html",
    "href": "website_docs/tool_landscaping.html",
    "title": "Tool landscaping",
    "section": "",
    "text": "In line with the scope of PGEforge, we focus our efforts on landscaping available tools that are commonly applied to Plasmodium genetic data and that focus on downstream analysis. Tools were initially considered within this scope if they:\n\nFocus on downstream analysis tools. This includes tools whose primary goal is to extract signal from pre-processed data, but does not include tools that are primarily used within upstream bioinformatic steps, such as variant callers and quality filters.\nFocus on Plasmodium genetics, including both P. falciparum and P. vivax.\n\nIn our initial landscaping, we did not consider applications to mosquito genetics or many broader population genetics tools, despite some tools and techniques being applicable for these purposes. However, we encourage contributions to this and anything else within the scope of PGEforge (i.e. Plasmodium genomic epidemiology tools), please see our contributor guidelines and some of our planned areas of future work.",
    "crumbs": [
      "Tool landscaping"
    ]
  },
  {
    "objectID": "website_docs/tool_landscaping.html#overview",
    "href": "website_docs/tool_landscaping.html#overview",
    "title": "Tool landscaping",
    "section": "",
    "text": "In line with the scope of PGEforge, we focus our efforts on landscaping available tools that are commonly applied to Plasmodium genetic data and that focus on downstream analysis. Tools were initially considered within this scope if they:\n\nFocus on downstream analysis tools. This includes tools whose primary goal is to extract signal from pre-processed data, but does not include tools that are primarily used within upstream bioinformatic steps, such as variant callers and quality filters.\nFocus on Plasmodium genetics, including both P. falciparum and P. vivax.\n\nIn our initial landscaping, we did not consider applications to mosquito genetics or many broader population genetics tools, despite some tools and techniques being applicable for these purposes. However, we encourage contributions to this and anything else within the scope of PGEforge (i.e. Plasmodium genomic epidemiology tools), please see our contributor guidelines and some of our planned areas of future work.",
    "crumbs": [
      "Tool landscaping"
    ]
  },
  {
    "objectID": "website_docs/tool_landscaping.html#landscaping-matrix",
    "href": "website_docs/tool_landscaping.html#landscaping-matrix",
    "title": "Tool landscaping",
    "section": "Landscaping matrix",
    "text": "Landscaping matrix",
    "crumbs": [
      "Tool landscaping"
    ]
  },
  {
    "objectID": "website_docs/radish23.html",
    "href": "website_docs/radish23.html",
    "title": "RADISH23",
    "section": "",
    "text": "The Reproducibility, Accessibility, Documentation and Inter-operability Standards Hackathon (RADISH23), organised by Bob Verity, Shazia Ruybal-Pesántez, Bryan Greenhouse and Amy Wesolowski took place at Johns Hopkins Bloomberg School of Public Health in Baltimore, USA from 11-14th December 2023, with 16 participants from 11 institutions.\n\n\n\n\n\nA cute radish\n\n\n For more details on RADISH23 attendees and contributors to PGEforge, see the Contributors page",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "RADISH23 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/radish23.html#background",
    "href": "website_docs/radish23.html#background",
    "title": "RADISH23",
    "section": "",
    "text": "The Reproducibility, Accessibility, Documentation and Inter-operability Standards Hackathon (RADISH23), organised by Bob Verity, Shazia Ruybal-Pesántez, Bryan Greenhouse and Amy Wesolowski took place at Johns Hopkins Bloomberg School of Public Health in Baltimore, USA from 11-14th December 2023, with 16 participants from 11 institutions.\n\n\n\n\n\nA cute radish\n\n\n For more details on RADISH23 attendees and contributors to PGEforge, see the Contributors page",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "RADISH23 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/radish23.html#main-aims",
    "href": "website_docs/radish23.html#main-aims",
    "title": "RADISH23",
    "section": "Main aims",
    "text": "Main aims\nOur aim was to take the wide range of software tools in malaria genomic epidemiology and create community resources so that more people can use them more reliably. Over the course of 4 days, we began creating a systematic framework for analysis by curating existing software tools, identifying the gaps in commonly used tools, in addition to having broader discussions about standardizing software practices.\nOne of our main aims to create community resources that allowed anyone with basic computer skills to be able to run common Plasmodium genetic analyses locally. By framing this work within the wider context of use-cases, we made progress towards harmonizing which tools need to be chained together to answer specific questions relevant to malaria control as part of well-defined and flexible workflows.",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "RADISH23 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/radish23.html#outputs",
    "href": "website_docs/radish23.html#outputs",
    "title": "RADISH23",
    "section": "Outputs",
    "text": "Outputs\nThe event was primarily coding-based and hands-on with the aim that the materials developed for each tool will allow an end-user to go from installation of the tool on their local machine to analysis using the standardized datasets we compiled.\n\n\n\nPrior to the hackathon, we carried out a scoping/landscaping exercise to identify all available analysis tools and evaluated them against a set of software standards, identifying those that are superseded or relegated and those we would prioritize during the hackathon. With those priority tools in mind, comprehensive guides were developed for the tools, including summary documents, installation aids and tutorials.\nIn addition, we compiled simulated and empirical datasets of genomic data in common formats required as input for the various tools to allow reproducibility for end-users running the tutorials and for future uses.\nAlongside the coding tasks, there were several small break-out sessions where participants defined malaria genomic surveillance data analysis use cases and sketched workflows for each of them, including functionality requirements and mapping these functionalities to available tools.",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "RADISH23 Hackathon"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_background.html#summary-sheet",
    "href": "tutorials/hmmIBD/hmmIBD_background.html#summary-sheet",
    "title": "hmmIBD",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nEstimating relatedness between parasites, detecting selective sweeps\n\n\nAuthors\nStephen Schaffner\n\n\nLatest version\nV2.1.1\n\n\nLicense\nGPL-3.0 license\n\n\nWebsite\nnone\n\n\nCode repository\nhttps://github.com/glipsnort/hmmIBD.git\n\n\nPublication\nhttps://doi.org/10.1186/s12936-018-2349-7\n\n\nTutorial authors\nSteve Schaffner\n\n\nTutorial date\n11 December, 2023",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_background.html#purpose",
    "href": "tutorials/hmmIBD/hmmIBD_background.html#purpose",
    "title": "hmmIBD",
    "section": "Purpose",
    "text": "Purpose\nhmmIBD implements a hidden Markov model (HMM) for detecting genomic regions that are identical by descent (IBD) for pairs of haploid samples. It was written to find large IBD regions in sequenced haploid P. falciparum genomes, but it can be applied to other organisms (including phased diploids) and can find shorter IBD regions as well. The package includes scripts to extract data from a VCF file into the appropriate format and to thin markers for better performance.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_background.html#existing-resources",
    "href": "tutorials/hmmIBD/hmmIBD_background.html#existing-resources",
    "title": "hmmIBD",
    "section": "Existing resources",
    "text": "Existing resources",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_background.html#citation",
    "href": "tutorials/hmmIBD/hmmIBD_background.html#citation",
    "title": "hmmIBD",
    "section": "Citation",
    "text": "Citation\n``` @article{, title = {hmmIBD: software to infer pairwise identity by descent between haploid genotypes}, author = {Schaffner, S.F., Taylor, A.R, et al.}, journal = {Malaria Journal}, volume = 17, pages = 196 year = 2018, doi = {https://doi.org/10.1186/s12936-018-2349-7} }",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_analysis.html",
    "href": "tutorials/hmmIBD/hmmIBD_analysis.html",
    "title": "PGEforge",
    "section": "",
    "text": "This describes a basic IBD analysis of whole genome sequence data using hmmIBD. It assumes you already have a C compiler and Python installed.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Basic IBD analysis with `hmmIBD`"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_analysis.html#the-data",
    "href": "tutorials/hmmIBD/hmmIBD_analysis.html#the-data",
    "title": "PGEforge",
    "section": "The data",
    "text": "The data\nWe can download sample Pf3k data from PGEforge data folder. For this demo, we will download the Vietnam data from the WGS directory. Let’s download two files - SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz and pf3k.metadata.Vietnam.csv - into a data subdirectory of our current working directory.\nBased on the ‘fws’ values in pf3k.metadata.Vietnam.csv, we can create a list of probable monogenomic samples and put it in a file named mono_samples.txt. For this exercise, we can assume that any sample with fws &gt; 0.95 is monogenomic and make a list of them in the file data/mono_samples.txt, one sample name per line.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Basic IBD analysis with `hmmIBD`"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_analysis.html#analysis",
    "href": "tutorials/hmmIBD/hmmIBD_analysis.html#analysis",
    "title": "PGEforge",
    "section": "Analysis",
    "text": "Analysis\nAssuming we have downloaded the hmmIBD package and have hmmIBD.c, vcf2hmm.py, and thin_sites.py in our current working directory, we start the analysis. First, we compile the C code:\ncc -o hmmIBD -O3 -lm -Wall hmmIBD.c\nThis should compile (possibly with an unimportant warning about an unused variable).\nThen we can use one of the Python scripts to extract data from the VCF file and look at the variants. The -s argument tells the script to pay attention only to the monogenomic samples we have listed in our file.\nvcf2hmm.py data/SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz vietnam -s data/mono_samples.txt\nThis prints out the following:\nSample source: data/mono_samples.txt\nSNP source: all                                 \nsamples: 97, of which 71 were kept\nchromosome Pf3D7_01_v3 maps to 1\nchromosome Pf3D7_02_v3 maps to 2\nchromosome Pf3D7_03_v3 maps to 3\nchromosome Pf3D7_04_v3 maps to 4\nchromosome Pf3D7_05_v3 maps to 5\nchromosome Pf3D7_06_v3 maps to 6 \nchromosome Pf3D7_07_v3 maps to 7\nchromosome Pf3D7_08_v3 maps to 8\nchromosome Pf3D7_09_v3 maps to 9\nchromosome Pf3D7_10_v3 maps to 10\nchromosome Pf3D7_11_v3 maps to 11\nchromosome Pf3D7_12_v3 maps to 12\nchromosome Pf3D7_13_v3 maps to 13\nchromosome Pf3D7_14_v3 maps to 14      \nN indels killed 0                                                       \nN failed/passed filter 0 247496\nN dropped for low call rate 1092\nN dropped for low minor allele freq 0\nHere the ‘maps to’ lines show what integer is being assigned to each chromosome. The script also produces the files vietnam_seq.txt, vietnam_freq.txt, and vietnam_allele.txt.\nExamination of vietnam_freq.seq file shows that there are hundreds of thousands of variants but that most of them are monomorphic, meaning that it is a good idea to thin these sites. This we can do using the vietnam_freq.txt file as follows:\nthin_sites.py vietnam_freq.txt vietnam_thin_sites.txt\nwhich produces a list in the file vietnam_thin_sites.txt of the sites (~20,000) to keep.\nWe can now re-extract the sequence data for just these sites, with the samples still restricted to the monogenomic samples:\nvcf2hmm.py data/SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz vietnam_thinned -s data/mono_samples.txt -l vietnam_thin_sites.txt\nThis produces a final vietnam_thinned_seq.txt file with the genotype data to feed into hmmIBD. We can now run the program:\nhmmIBD -i vietnam_thinned_seq.txt -o vietnam\nThe output should appear in the files vietnam.hmm.txt and vietnam.hmm_fract.txt.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Basic IBD analysis with `hmmIBD`"
    ]
  },
  {
    "objectID": "tutorials/estMOI/installation.html",
    "href": "tutorials/estMOI/installation.html",
    "title": "Installing estMOI",
    "section": "",
    "text": "EstMOI is written in perl. It also requires samtools at runtime. To help us install these tools we’ll use mamba. If you already have conda installed then you can skip to step 2.\nHead over to the mambaforge github page and follow the install instructions. Remember to use the relevant install script for your operating system.",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/estMOI/installation.html#step-1-installing-conda",
    "href": "tutorials/estMOI/installation.html#step-1-installing-conda",
    "title": "Installing estMOI",
    "section": "",
    "text": "EstMOI is written in perl. It also requires samtools at runtime. To help us install these tools we’ll use mamba. If you already have conda installed then you can skip to step 2.\nHead over to the mambaforge github page and follow the install instructions. Remember to use the relevant install script for your operating system.",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/estMOI/installation.html#step-2-install-dependancies",
    "href": "tutorials/estMOI/installation.html#step-2-install-dependancies",
    "title": "Installing estMOI",
    "section": "Step 2: Install dependancies",
    "text": "Step 2: Install dependancies\nOnce you have conda/mamba set up you can start insalling the dependancies. You can do this with\nmamba create -y -c conda-forge -c bioconda -n estMOI perl curl \"samtools&gt;=1.12\"\nThis will create an environment called estMOI that you can activate with\nconda activate estMOI",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/estMOI/installation.html#step-3-install-estmoi",
    "href": "tutorials/estMOI/installation.html#step-3-install-estmoi",
    "title": "Installing estMOI",
    "section": "Step 3: Install estMOI",
    "text": "Step 3: Install estMOI\nFinally we can install the script into your path and make it executable with\ncurl https://raw.githubusercontent.com/sammy-assefa/estMOI/master/estMOI_1.03 &gt; $CONDA_PREFIX/bin/estMOI\nchmod 755 $CONDA_PREFIX/bin/estMOI\nRun estMOI in your terminal and see if it runs. You should get an output showing an explanation of all the parameters.",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/estMOI/analysis.html",
    "href": "tutorials/estMOI/analysis.html",
    "title": "How to run estMOI",
    "section": "",
    "text": "To perform the analysis estMOI requires:\n\nA bam file which should be sorted\nA vcf file which can either be zipped or unzipped\nThe reference genome\n\nFor the purpose of hosting the data we have provided bam and vcf files containing data for the csp, celtos and ama1 genes. You can find these files at https://github.com/mrc-ide/PGEforge/tree/main/data/wgs/labisolate_subset. Download the calls.tar.gz file and unzip. You should now have a folder named calls which contains the bam and VCF files that we will user later in the practical.\nWe now need to download the reference genome. You can download this from PlasmoDB by clicking on this link",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Analyze data with `estMOI`"
    ]
  },
  {
    "objectID": "tutorials/estMOI/analysis.html#the-data",
    "href": "tutorials/estMOI/analysis.html#the-data",
    "title": "How to run estMOI",
    "section": "",
    "text": "To perform the analysis estMOI requires:\n\nA bam file which should be sorted\nA vcf file which can either be zipped or unzipped\nThe reference genome\n\nFor the purpose of hosting the data we have provided bam and vcf files containing data for the csp, celtos and ama1 genes. You can find these files at https://github.com/mrc-ide/PGEforge/tree/main/data/wgs/labisolate_subset. Download the calls.tar.gz file and unzip. You should now have a folder named calls which contains the bam and VCF files that we will user later in the practical.\nWe now need to download the reference genome. You can download this from PlasmoDB by clicking on this link",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Analyze data with `estMOI`"
    ]
  },
  {
    "objectID": "tutorials/estMOI/analysis.html#running-the-tool",
    "href": "tutorials/estMOI/analysis.html#running-the-tool",
    "title": "How to run estMOI",
    "section": "Running the tool",
    "text": "Running the tool\nLet’s use run the analysis for the sample PG0402-C which is sample with a known MOI &gt; 1. Let’s\nBefore we can run estMOI we need to index the reference genome. This can by done with\nsamtools faidx PlasmoDB-66_Pfalciparum3D7_Genome.fasta\nNow let’s run the pipeline with default parameters. The three positional arguments should be:\n\nThe bam file\nThe VCF file\nThe reference genome\n\nYou can also customise the output file prefix with --out\nestMOI PG0402-C.sorted.bam PG0402-C.vcf PlasmoDB-66_Pfalciparum3D7_Genome.fasta --out PG0402-C\nAfter running it will print a summary of the analysis to the terminal and you should see the output below:\n#   RUNNING estMOI version 1.03\n#   ................\n#   PRINT ESTIMATES...\n#   MOI Count   %Total\n#   1   319 66.88\n#   2   158 100.00  MOI-estimate\n    #DONE MOI-estimate using PG0402-C.sorted.bam\n\nOutput description\nRunning estMOI will produce two output files with the prefix that you specified before. Additionally, the values of the --maxsnp, --mindis, --maxdis and --minhap parameters will also be present in the output file names.\nFor example, you should have the files PG0402-C.moi.3.10.500.3.log and PG0402-C.moi.3.10.500.3.txt in your directory.\n\nLog file\nThe file ending in .log contains the individual combinations of all haplotypes that were found. For each haplotype it will give you the locations of the SNPs together with the individual haplotype combinations. For example, here is an excerpt form the log file that contains this information for one haplotype:\nPG0402-C.sorted.bam Pf3D7_03_v3 221435 221457 221554    2\n    # PG0402-C.sorted.bam Pf3D7_03_v3 221435 221457 221554 Hapotype:     T A C  48\n    # PG0402-C.sorted.bam Pf3D7_03_v3 221435 221457 221554 Hapotype:     T G C  10\n\n\nTxt file\nThe result file will contain the the same information as printed to the terminal. It lists the numbers of haplotypes and for each MOI and provides an estimate of the MOI. The the example below there are 319 locus with a single haplotypes and 158 with two haplotypes.\n#MOI    Count   %Total\n1   319 66.88\n2   158 100.00  MOI-estimate",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Analyze data with `estMOI`"
    ]
  },
  {
    "objectID": "tutorials/estMOI/analysis.html#refining-the-results",
    "href": "tutorials/estMOI/analysis.html#refining-the-results",
    "title": "How to run estMOI",
    "section": "Refining the results",
    "text": "Refining the results\nLet’s run this on another sample:\nestMOI PG0389-C.sorted.bam PG0389-C.vcf  PlasmoDB-66_Pfalciparum3D7_Genome.fasta --out PG0389-C\nYou should see that no MOI has been estimates:\n#   RUNNING estMOI version 1.03\n#   ................\n#   PRINT ESTIMATES...\n#   MOI Count   %Total\n    #DONE MOI-estimate using PG0389-C.sorted.bam\nTo troubleshoot it is a good idea to look at the log. Run the following to find information about the haplotypes that have been analysed:\ncat  PG0389-C.moi.3.10.500.3.log\nYou should see no output, indicating that no haplotypes were analysed and explains why there was no MOI estimate produced. estMOI loads the variant locations from the VCF and looks at combinations of those in the reads using the bam files.\nLooking at the bam file it is clear that there are some variant haplotypes present.\n\nNext thing to do is to check that we actually have variants in the VCF. You can do this with:\nless PG0389-C.vcf\nYou should be able to see that there are many variant positions, however you might notice that the quality values are very low. estMOI has a hard-coded cutoff for the quality value and only analyses variants with quality values greater or equal to 30. This may explain why the estMOI didn’t analyse any haplotype positions.\nThe VCFs were created with freebayes using a diploid model and low frequency variants are assigned low quality values. We can use a different variant caller, lofreq, that is more suited to low frequency variants which will hopefully assign higher quality values to these variants. First let’s install lofreq:\nmamba install -y -c conda-forge -c bioconda lofreq\nNow we can use lofreq to call variants from the bam:\nlofreq call -f PlasmoDB-66_Pfalciparum3D7_Genome.fasta PG0389-C.sorted.bam &gt; PG0389-C.lofreq.vcf\nNow we are ready to call run estMOI again using the new vcf you created:\nestMOI PG0389-C.sorted.bam PG0389-C.lofreq.vcf  PlasmoDB-66_Pfalciparum3D7_Genome.fasta  --out PG0389-C\nYou shold hopefully see an MOI estimate of 2 now after the tools finishes running:\n#   RUNNING estMOI version 1.03\n#   ................\n#   PRINT ESTIMATES...\n#   MOI Count   %Total\n#   1   98  68.06\n#   2   46  100.00  MOI-estimate\n    #DONE MOI-estimate using PG0389-C.sorted.bam",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Analyze data with `estMOI`"
    ]
  },
  {
    "objectID": "tutorials/estMOI/analysis.html#summary",
    "href": "tutorials/estMOI/analysis.html#summary",
    "title": "How to run estMOI",
    "section": "Summary",
    "text": "Summary\nTo summarise, we can use estMOI to estimate MOI using the read data stored in bams and variants stored in VCF format. The variants filtered by the pipeline based on a quality value so make sure you are happy with the variants your VCF before proceeding with the analysis.",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Analyze data with `estMOI`"
    ]
  },
  {
    "objectID": "tutorials/Template/Template_background.html#summary-sheet",
    "href": "tutorials/Template/Template_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nTODO\n\n\nAuthors\nTODO\n\n\nLatest version\nTODO\n\n\nLicense\nTODO\n\n\nWebsite\nhttps://www.google.com\n\n\nCode repository\nhttps://www.google.com\n\n\nPublication\nhttps://www.google.com\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO"
  },
  {
    "objectID": "tutorials/Template/Template_background.html#purpose",
    "href": "tutorials/Template/Template_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nA quick one paragraph description of what the tool does. For an example, see the DRpower page."
  },
  {
    "objectID": "tutorials/Template/Template_background.html#existing-resources",
    "href": "tutorials/Template/Template_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/Template/Template_background.html#citation",
    "href": "tutorials/Template/Template_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/malariaem/installation.html",
    "href": "tutorials/malariaem/installation.html",
    "title": "Installing malaria.em",
    "section": "",
    "text": "The malaria.em R package is now part of the plasmogenepi.r-universe.\nYou can install it by running the following code:\n\n# Install malaria.em in R:\ninstall.packages('malaria.em', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\n\n\n\n Back to top",
    "crumbs": [
      "Tool resources",
      "malaria.em",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/malariaem/malariaem_background.html#summary-sheet",
    "href": "tutorials/malariaem/malariaem_background.html#summary-sheet",
    "title": "malaria.em",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nestimate haplotype frequencies\n\n\nAuthors\nXiaohong Li\n\n\nLatest version\n2025-01-26\n\n\nCode repository\nhttps://github.com/PlasmoGenEpi/malaria.em\n\n\nPublication\nhttps://doi.org/10.2202/1544-6115.1321",
    "crumbs": [
      "Tool resources",
      "malaria.em",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/malariaem/malariaem_background.html#purpose",
    "href": "tutorials/malariaem/malariaem_background.html#purpose",
    "title": "malaria.em",
    "section": "Purpose",
    "text": "Purpose\nProgram designed to discern the combinations of mutations at the population level to estimate frequencies of these clonal sequences. This program utilizes an expectation maximization (EM) approach to maximum likelihood estimation of haplotype frequencies from the SNP loci per sample information.\nThe code for this program was removed from CRAN in 2014 and does not appear to be actively maintained (CRAN archive). However, we have fixed some bugs in a newer version, and it is now available through the PlasmoGenEpi GitHub repository.",
    "crumbs": [
      "Tool resources",
      "malaria.em",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/malariaem/malariaem_background.html#citation",
    "href": "tutorials/malariaem/malariaem_background.html#citation",
    "title": "malaria.em",
    "section": "Citation",
    "text": "Citation\n(Li et al. 2007)",
    "crumbs": [
      "Tool resources",
      "malaria.em",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_background.html#summary-sheet",
    "href": "tutorials/dcifer/dcifer_background.html#summary-sheet",
    "title": "dcifer",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nGenetic relatedness between polyclonal infections\n\n\nAuthors\nInna Gerlovina\n\n\nLatest version\n1.2.0\n\n\nLicense\nMIT\n\n\nWebsite\nhttps://eppicenter.github.io/dcifer/\n\n\nCode repository\nhttps://github.com/EPPIcenter/dcifer\n\n\nPublication\nhttps://doi.org/10.1093/genetics/iyac126\n\n\nTutorial authors\nShazia Ruybal-Pesántez\n\n\nTutorial date\n11-Dec-23",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_background.html#purpose",
    "href": "tutorials/dcifer/dcifer_background.html#purpose",
    "title": "dcifer",
    "section": "Purpose",
    "text": "Purpose\nThe dcifer R package is primarily designed to estimate relatedness between polyclonal infections. The data input types must be biallelic or multiallelic data.\nThe approach uses a likelihood function and statistical inference, and provides these alongside relatedness estimates.",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_background.html#existing-resources",
    "href": "tutorials/dcifer/dcifer_background.html#existing-resources",
    "title": "dcifer",
    "section": "Existing resources",
    "text": "Existing resources\nThe dcifer R package includes built-in functions for reading and reformatting data, performing preparatory steps, and visualizing the results are also included. This is documented in the dcifer R package website. There is also a tutorial outlining the analysis process using the dcifer R package with microhaplotype data from Mozambique.",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_background.html#citation",
    "href": "tutorials/dcifer/dcifer_background.html#citation",
    "title": "dcifer",
    "section": "Citation",
    "text": "Citation\nThe publication associated with the dcifer R package can be found here (Gerlovina 2022 Genetics).\n\ncitation(package = \"dcifer\")\n\nTo cite package 'dcifer' in publications use:\n\n  Gerlovina I (2023). _dcifer: Genetic Relatedness Between Polyclonal\n  Infections_. R package version 1.2.1,\n  &lt;https://CRAN.R-project.org/package=dcifer&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {dcifer: Genetic Relatedness Between Polyclonal Infections},\n    author = {Inna Gerlovina},\n    year = {2023},\n    note = {R package version 1.2.1},\n    url = {https://CRAN.R-project.org/package=dcifer},\n  }",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MLMOI/Template_installation.html",
    "href": "tutorials/MLMOI/Template_installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/MLMOI/Template_installation.html#step-n-what-to-do",
    "href": "tutorials/MLMOI/Template_installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/MLMOI/Template_analysis.html",
    "href": "tutorials/MLMOI/Template_analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/MLMOI/Template_analysis.html#the-data",
    "href": "tutorials/MLMOI/Template_analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/MLMOI/Template_analysis.html#first-analysis-section",
    "href": "tutorials/MLMOI/Template_analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages."
  },
  {
    "objectID": "tutorials/MLMOI/Template_analysis.html#summary",
    "href": "tutorials/MLMOI/Template_analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial."
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html",
    "href": "tutorials/paneljudge/paneljudge_analysis.html",
    "title": "Tutorial for Running paneljudge",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\n\n\n\nAllele\nA genetic state (a specific realization of a variant)\n\n\n\n\n\nSite\nAlso referred to as a locus. It is a genomic location that has more than one state (more than one allele)\n\n\n\n\n\nPanel\nCollection of alleles. Others may call this a barcode, fingerprint, etc.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#terminology",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#terminology",
    "title": "Tutorial for Running paneljudge",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\n\n\n\nAllele\nA genetic state (a specific realization of a variant)\n\n\n\n\n\nSite\nAlso referred to as a locus. It is a genomic location that has more than one state (more than one allele)\n\n\n\n\n\nPanel\nCollection of alleles. Others may call this a barcode, fingerprint, etc.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#the-data",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#the-data",
    "title": "Tutorial for Running paneljudge",
    "section": "The Data",
    "text": "The Data\nAs input for paneljudge, we will start with a variant call file, abbreviated as a VCF of the Sanger Barcode from Vietnam For paneljudge, we are attempting to determine the “information” contained in each site (and collective sites in a panel). In order to quantify information, we need to know the allele frequency across each site (the frequency that we see each allele at a given site). Given that we are using a monoclonal (simple genetic sample) and biallelic data (alleles have only two possible states), we can take the average across the VCF genotype calls and then the complement for first and second allele, respectively1. Below, we will use vcfR to convert the VCF into a population-level allele frequency matrix. We will then convert the PLAF to a dataframe/matrix with the frequency of each of our two alleles for each site (needed for paneljudge).\n\n#......................\n# vcf and plaf\n#......................\n# read in the VCF from the main data page \n\nvcf &lt;- vcfR::read.vcfR(\"../../data/snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz\", verbose = F)\n\n# convert the data into genotype calls\ngtcalls &lt;- vcfR::extract.gt(vcf, as.numeric = T) \n\n# population level allele frequency \n# rows are sites/alleles, columns are samples\nplaf &lt;- apply(gtcalls, 1, mean, na.rm = T) # apply the mean function across rows (\"1\" option)\n\n# now bring into a table in needed format for `paneljudge` \npjdat &lt;- data.frame(Allele.1 = plaf, Allele.2 = 1-plaf)\nrownames(pjdat) &lt;- names(plaf)\n\n#......................\n# going to need genetic positions later\n# Extensive data wrangling for genetic position \n#......................\nPOS &lt;- tibble::tibble(CHROM = vcfR::getCHROM(vcf),  \n                      POS = vcfR::getPOS(vcf))\nPOS &lt;- split(POS, factor(POS$CHROM))\nds &lt;- lapply(POS, function(x){diff(unlist(x[,2]))})\n# make last pos end of chrom \nds &lt;- lapply(ds, function(x){x[length(x)] &lt;- 1e8; return(x) })\nds &lt;- unlist(ds)\n\nPrior to using paneljudge, we can visualize our population-level allele frequencies (PLAF). If the population has undergone significant drift or is inbred, we may expect to see a clustering of PLAF around values less than 0.5 or greater than 0.5 (and more close to 0.1 or 0.9). In contrast, if a population is panmictic with multiple outbreeding populations, we may expect our PLAF values to be closer to 0.5.\n\ntibble::tibble(plaf = plaf,\n               allele = names(plaf)) %&gt;% \n  ggplot() + \n  geom_bar(aes(x = allele, y = plaf), stat = \"identity\") +\n  theme_linedraw() + \n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  xlab(\"Alleles\") + ylab(\"Pop. Lvl Allele Freq\")",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#intuition-behind-paneljudge",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#intuition-behind-paneljudge",
    "title": "Tutorial for Running paneljudge",
    "section": "Intuition Behind paneljudge",
    "text": "Intuition Behind paneljudge\nBefore discussing the functionality of paneljudge, it may be helpful to build intuition around what gives an allele more information for calculating relatedness.\nThere are two mains ways to estimate relatedness among individuals: identity by descent (IBD) or time to most recent common ancestor (TMRCA) (see Speed & Balding 2015 for further discussion). The paneljudge package focuses on IBD, which measures the amount of genome (or specific alleles) shared between two individuals (i.e. recent coalescence). If an allele is extremely rare in the population but is shared by two individuals, this carries much more “information” than if individuals shared an allele that was common in the population. For example, if two individuals were 3 meters tall, we may suspect they were more likely to be related to each other versus if they were both 1.7 meters (average height). Similar to height (versus say eye color), alleles that have more variation and “extremes” are more useful than alleles that only have a few states: multiple alleles at nearly equal frequencies (i.e. 10 alleles, each at 10% frequency).\nIn statistics, we can consider an allele a “set” of potential states, or elements. The number of states that our allele can take, or more formally, the number of elements in our set determines the cardinality. Sites and panels that have a high cardinality (many alleles) are expected to be more informative, particularly if those alleles are equifrequent. However, per Dr. Aimee Taylor, alleles are rarely equifrequent in real life (due to a variety of population genetic forces like drift, selection etc), so it is useful to consider effective cardinality, which is “an allele count that accounts for inequifrequent alleles: it is equal to cardinality if all alleles are equifrequent; otherwise, it is less”.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#using-paneljudge",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#using-paneljudge",
    "title": "Tutorial for Running paneljudge",
    "section": "Using paneljudge",
    "text": "Using paneljudge\nBelow, we will calculate the diversity, effective cardinality, and absolute cardinality of our sites.\n\ndiversities &lt;- compute_diversities(pjdat)\neff_cardinalities &lt;- compute_eff_cardinalities(pjdat,  warn_fs = FALSE)\ncardinalities &lt;- apply(pjdat, 1, function(x){sum(x &gt; 0)}) \n\nNote, the true cardinality of our data should all be 2, since we have biallelic sites. As a sanity check, we can test this assumption with the code unqiue(cardinalities).\nHere, we can compare the diversity versus the effective cardinality of our sites. Notably, per Dr. Aimee Taylor’s package vignette/point above, effective cardinality will scale linearly with cardinality (as it is an adjustment of that original measure) whereas diversity does not have a clear relationship with cardinality. As a result, while diversity is a useful population-genetic statistic, it may not be as reliable of a marker for relatedness “informativeness”.\n\ntibble::tibble(eff_cardinalities = eff_cardinalities, \n               diversities = diversities) %&gt;% \n  ggplot() + \n  geom_point(aes(y = eff_cardinalities, x = diversities)) + \n  geom_hline(yintercept = 2, color = \"red\") +\n  theme_linedraw() + \n  ylab(\"Eff. Cardinality\") + xlab(\"Diversity\") + \n  labs(caption = \"Red-Line Represent True Cardinality\")",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#comparing-panels-with-paneljudge",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#comparing-panels-with-paneljudge",
    "title": "Tutorial for Running paneljudge",
    "section": "Comparing Panels with paneljudge",
    "text": "Comparing Panels with paneljudge\n\nPanel Competitor\nFor the purpose of this tutorial, we will make a panel with the same number of sites as the Sanger Barcode but has no information (alleles fixed).\n\n# all alleles fixed, no information \nbadpjdat &lt;- pjdat\nbadpjdat[,1] &lt;- 0.99\nbadpjdat[,2] &lt;- 0.01\n\n\n\nUsing Simulation to Evaluate Panel “Informativeness”\nIn order to evaluate if a specific panel is useful and informative for determining relatedness in a population, we use simulation to determine if we can capture expected values of relatedness from realizations that are simulated from our panel.\nSimulations are created using a hidden-markov model that is described in Taylor et al. 2019, Genetics. We will create simulates at very levels of \\(r\\) but assume a fixed switch rate, \\(k\\).\nA simple conception of the hidden-markov model is to imagine two samples moving in parallel down a track, where they switch “diverge” and become unrelated based on our switch rate and relatedness factor. The samples can then re-converge again later depending on the switch rate and relatedness. In this way, region of DNA are shared from a common ancestor and sections of IBD are created.\n\n#............................................................\n# Function for Making Simulations and Capturing Output \n#...........................................................\nget_sim_ret &lt;- function(fs, ds, k, r, n) {\n  # init\n  out &lt;- c()\n  # for loops for clarity \n  for (i in 1:n) { # for each pair\n    for (j  in 1:length(rs)) { # for each relatedness value \n      \n      # First simulate a new genotype pair\n      Ys &lt;- simulate_Ys(fs, ds, k, rs[j], warn_fs = FALSE)\n      \n      # Second, estimate r and k\n      krhat &lt;- estimate_r_and_k(fs, ds, Ys, warn_fs = FALSE)\n      \n      # Third, compute confidence intervals (CIs)\n      CIs &lt;- compute_r_and_k_CIs(fs, ds, khat = krhat['khat'], rhat = krhat['rhat'],  warn_fs = FALSE)\n      \n      # fourth save it out \n      out &lt;- rbind( c(krhat['rhat'], CIs['rhat',]), out)\n    }\n  }\n  return(out)\n}\n\n#............................................................\n# Run Sim Capturer on Good and Bad Panel \n#...........................................................\n# place holders \nrs &lt;- c(\"0.01\"=0.01, \"0.25\"=0.25, \"0.50\"=0.50, \"0.75\"=0.75, \"0.99\"=0.99)\nk &lt;- 5 # Data-generating switch rate parameter value\nn &lt;- 5 # Number of pairs to simulate per r in rs\nds &lt;- runif(n = nrow(pjdat), min = 1, max = 1e6) # random distances, would actually be based on genomic location\n\n# good panel run \ngoodpanel_results &lt;- get_sim_ret(fs = pjdat, \n                                 ds = ds, \n                                 k = 5, # Data-generating switch rate parameter value\n                                 n = 5, # Number of pairs to simulate per r in rs\n                                 r = rs) \n\n\n# bad panel run \nbadpanel_results &lt;- get_sim_ret(fs = badpjdat, \n                                ds = ds, \n                                k = 5, # Data-generating switch rate parameter value\n                                n = 5, # Number of pairs to simulate per r in rs\n                                r = rs) \n\nNow we can compare the results of the simulations and expect for the better panel to have smaller (tighter) confidence intervals. Tighter confidence intervals indicate that the panel provided more information that allowed for more precise estimates of relatedness (given the predetermined population allele frequencies).\n\n# drop in R sim values\ngoodpanel_results_tb &lt;- goodpanel_results %&gt;% \n  tibble::as_tibble(.) %&gt;% \n  magrittr::set_colnames(c(\"rhat\", \"LCI\", \"HCI\")) %&gt;% \n  dplyr::mutate(r = sort(rep(rs, n))) %&gt;% \n  dplyr::mutate(CIwidth = HCI - LCI,\n                level = \"good\") \n\nbadpanel_results_tb &lt;- badpanel_results %&gt;% \n  tibble::as_tibble(.) %&gt;% \n  magrittr::set_colnames(c(\"rhat\", \"LCI\", \"HCI\")) %&gt;% \n  dplyr::mutate(r = sort(rep(rs, n))) %&gt;% \n  dplyr::mutate(CIwidth = HCI - LCI,\n                level = \"bad\") \n\n\n# now combine for easier plotting \ndplyr::bind_rows(goodpanel_results_tb, badpanel_results_tb) %&gt;% \n  dplyr::mutate(rfact = factor(r)) %&gt;% \n  ggplot() + \n  geom_boxplot(aes(x = rfact, y = CIwidth, fill = level)) +\n  facet_grid(~level) +\n  xlab(\"Simualted R Value\") + ylab(\"95% CI Width\")",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#summary",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#summary",
    "title": "Tutorial for Running paneljudge",
    "section": "Summary",
    "text": "Summary\nThis tutorial explains how one could use the paneljudge R package for identifying which panel would provide more precise and better inference of relatedness. We walked through how to organize data for paneljudge input, the intuition behind the package, and how to run panel comparisons. Finally, we discuss how to determine which panel may provide more precise inference of relatedness given the inputted population allele frequencies.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_analysis.html#footnotes",
    "href": "tutorials/paneljudge/paneljudge_analysis.html#footnotes",
    "title": "Tutorial for Running paneljudge",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMultiallelic sites lack complement p-q properties and would need to be summed respectively↩︎",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Tutorial for Running `paneljudge`"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#summary-sheet",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#summary-sheet",
    "title": "MultiLociBiallelicModel",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nMOI;prevalence estimates\n\n\nAuthors\nHenri Christian Junior Tsoungui Obama;Kristan Alexander Schneider\n\n\nLatest version\nNA\n\n\nLicense\nMIT License\n\n\nWebsite\nhttps://github.com/Maths-against-Malaria/MultiLociBiallelicModel\n\n\nCode repository\nhttps://github.com/Maths-against-Malaria/MultiLociBiallelicModel\n\n\nPublication\nhttps://www.frontiersin.org/articles/10.3389/fepid.2022.943625/full\n\n\nTutorial authors\nNicholas Hathaway\n\n\nTutorial date\n2023-12",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#purpose",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#purpose",
    "title": "MultiLociBiallelicModel",
    "section": "Purpose",
    "text": "Purpose\nThe code supplied by this paper does a maximum-likelihood (MLE) method to estimate:\n\nhaplotype frequencies and prevalence\nmultiplicity of infection (MOI/COI) from SNP data.\n\nThe functions here provide functionality to predict possible haplotype prevalence within the population that lead to the current set of data. Also estimates MOI/COI based on these estimates. Has to take only biallelic SNPs and compuationally can be limited by the number of loci supplied (in publication used 10 loci).",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#existing-resources",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#existing-resources",
    "title": "MultiLociBiallelicModel",
    "section": "Existing resources",
    "text": "Existing resources\n\nExample file can be found here",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#citation",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_background.html#citation",
    "title": "MultiLociBiallelicModel",
    "section": "Citation",
    "text": "Citation\n(Tsoungui Obama and Schneider 2022)",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_installation.html",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_installation.html",
    "title": "Installing MultiLociBiallelicModel",
    "section": "",
    "text": "The code for using MultiLociBiallelicModel can all be found within 1 R Script. This can be downloaded from their github repo from https://github.com/Maths-against-Malaria/MultiLociBiallelicModel/blob/main/src/SNPModel.R or can be downloaded from here SNPModel.R.\n\nwget https://github.com/Maths-against-Malaria/MultiLociBiallelicModel/blob/main/src/SNPModel.R",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_installation.html#step-1-downloading-script",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_installation.html#step-1-downloading-script",
    "title": "Installing MultiLociBiallelicModel",
    "section": "",
    "text": "The code for using MultiLociBiallelicModel can all be found within 1 R Script. This can be downloaded from their github repo from https://github.com/Maths-against-Malaria/MultiLociBiallelicModel/blob/main/src/SNPModel.R or can be downloaded from here SNPModel.R.\n\nwget https://github.com/Maths-against-Malaria/MultiLociBiallelicModel/blob/main/src/SNPModel.R",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/pixelate/Template_installation.html",
    "href": "tutorials/pixelate/Template_installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/pixelate/Template_installation.html#step-n-what-to-do",
    "href": "tutorials/pixelate/Template_installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/pixelate/Template_analysis.html",
    "href": "tutorials/pixelate/Template_analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/pixelate/Template_analysis.html#the-data",
    "href": "tutorials/pixelate/Template_analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/pixelate/Template_analysis.html#first-analysis-section",
    "href": "tutorials/pixelate/Template_analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages."
  },
  {
    "objectID": "tutorials/pixelate/Template_analysis.html#summary",
    "href": "tutorials/pixelate/Template_analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial."
  },
  {
    "objectID": "tutorials/IDM/background.html#summary-sheet",
    "href": "tutorials/IDM/background.html#summary-sheet",
    "title": "Incomplete data model (IDM)",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nEstimate allele frequency and MOI accounting for incomplete data\n\n\nAuthors\nMeraj Hashemi, Kristan Schneider\n\n\nLatest version\nUnknown\n\n\nLicense\nUnknown\n\n\nWebsite\nscript: https://doi.org/10.1371/journal.pone.0287161.s002 and documentation: https://doi.org/10.1371/journal.pone.0287161.s003\n\n\nCode repository\nhttps://github.com/Maths-against-Malaria/MOI---Incomplete-Data-Model.git (not active)\n\n\nPublication\nhttps://doi.org/10.1371/journal.pone.0287161\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/IDM/background.html#purpose",
    "href": "tutorials/IDM/background.html#purpose",
    "title": "Incomplete data model (IDM)",
    "section": "Purpose",
    "text": "Purpose\nThe IDM algorithm provides a statistical model to estimate MOI and lineage frequencies/prevalences from single-locus molecular data characterized by incomplete information.\nA note from the original paper: “the new method is recommendable only for data sets in which the molecular assays produced poor-quality results. This will be particularly true if the model is extended to accommodate information from multiple molecular markers at the same time, and incomplete information at one or more markers leads to a strong depletion of sample size.”",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/IDM/background.html#existing-resources",
    "href": "tutorials/IDM/background.html#existing-resources",
    "title": "Incomplete data model (IDM)",
    "section": "Existing resources",
    "text": "Existing resources\n\nThe existing tutorial is in PDF format as mentioned above (https://doi.org/10.1371/journal.pone.0287161.s003) and is from in the original paper. If you find another one, please contribute by filing an issue or submitting a pull request.\nRelated paper:\nHashemi M, Schneider KA. Bias-corrected maximum-likelihood estimation of multiplicity of infection and lineage frequencies. PLOS ONE. 2022; 16(12):1–28. (This paper describes the original model (OM))",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/IDM/background.html#citation",
    "href": "tutorials/IDM/background.html#citation",
    "title": "Incomplete data model (IDM)",
    "section": "Citation",
    "text": "Citation\nThe publications associated with the IDM method can be found at Hashemi et al 2022 PLOS One.",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/installation.html",
    "href": "tutorials/FreqEstimationModel/installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/installation.html#step-n-what-to-do",
    "href": "tutorials/FreqEstimationModel/installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/analysis.html",
    "href": "tutorials/FreqEstimationModel/analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Analyze data with `FreqEstimationModel`"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/analysis.html#the-data",
    "href": "tutorials/FreqEstimationModel/analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Analyze data with `FreqEstimationModel`"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/analysis.html#first-analysis-section",
    "href": "tutorials/FreqEstimationModel/analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages.",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Analyze data with `FreqEstimationModel`"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/analysis.html#summary",
    "href": "tutorials/FreqEstimationModel/analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial.",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Analyze data with `FreqEstimationModel`"
    ]
  },
  {
    "objectID": "tutorials/MalHaploFreq/MalHaploFreq_background.html#summary-sheet",
    "href": "tutorials/MalHaploFreq/MalHaploFreq_background.html#summary-sheet",
    "title": "MalHaploFreq About",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nprevalence\n\n\nAuthors\nIan Hastings\n\n\nLatest version\nNA\n\n\nCode repository\nhttp://pcwww.liv.ac.uk/hastings/MalHaploFreq/\n\n\nPublication\nhttps://doi.org/10.1186/1475-2875-7-130"
  },
  {
    "objectID": "tutorials/MalHaploFreq/MalHaploFreq_background.html#purpose",
    "href": "tutorials/MalHaploFreq/MalHaploFreq_background.html#purpose",
    "title": "MalHaploFreq About",
    "section": "Purpose",
    "text": "Purpose\nThis tool was created to estimate prevalence of haplotypes (linking up to 3 SNPs together) from SNP loci. This tool requires input determined MOI and does 3 loci. Since the creation of this tool other tools have been created that do similar analysis with additional features, for example MultiLociBiallelicModel also estimates prevalence but does not require MOI, can do more loci and estimates the MOI from the input data."
  },
  {
    "objectID": "tutorials/MalHaploFreq/MalHaploFreq_background.html#citation",
    "href": "tutorials/MalHaploFreq/MalHaploFreq_background.html#citation",
    "title": "MalHaploFreq About",
    "section": "Citation",
    "text": "Citation\n(Hastings and Smith 2008)"
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html",
    "href": "tutorials/coiaf/coiaf_analysis.html",
    "title": "Running coiaf",
    "section": "",
    "text": "For this tutorial, we will be using the coiaf package to estimate the complexity of infection (COI) from SNP barcode data. Instructions to install this package may be found here.",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html#introduction",
    "href": "tutorials/coiaf/coiaf_analysis.html#introduction",
    "title": "Running coiaf",
    "section": "",
    "text": "For this tutorial, we will be using the coiaf package to estimate the complexity of infection (COI) from SNP barcode data. Instructions to install this package may be found here.",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html#the-data",
    "href": "tutorials/coiaf/coiaf_analysis.html#the-data",
    "title": "Running coiaf",
    "section": "The data",
    "text": "The data\ncoiaf expects a matrix of within sample reference allele frequencies (WSAF), as well as an estimate of population level reference allele frequencies (PLAF). The WSAF matrix should have samples in rows and sites in columns. The PLAF should be a vector of length equal to the number of sites. To explore this, we will use simulated SNP barcoding data as described here. We will load the data using the vcfR package.\n\nvcf_data &lt;- vcfR::read.vcfR(\n  here::here(\"data/snp_barcode/SpotMalariapfPanel_simData_sanger100.vcf.gz\")\n)\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 76\n  header_line: 77\n  variant count: 100\n  column count: 109\n\nMeta line 76 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 100\n  Character matrix gt cols: 109\n  skip: 0\n  nrows: 100\n  row_num: 0\n\nProcessed variant: 100\nAll variants processed\n\n\nTo calculate within host allele frequencies, we need to extract the depth of coverage and allele counts\n\ncoverage &lt;- t(vcfR::extract.gt(vcf_data, element = \"DP\", as.numeric = TRUE))\ncounts_raw &lt;- t(vcfR::extract.gt(vcf_data, element = \"AD\"))\ncounts &lt;- vcfR::masplit(\n  counts_raw, record = 1, sort = FALSE, decreasing = FALSE\n)\n\n# We then directly calculate the WSAF\nwsaf &lt;- counts / coverage\n\n# and estimate the PLAF with the empirical mean across samples\nplaf &lt;- colMeans(wsaf, na.rm = TRUE)",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html#running-coiaf",
    "href": "tutorials/coiaf/coiaf_analysis.html#running-coiaf",
    "title": "Running coiaf",
    "section": "Running coiaf",
    "text": "Running coiaf\ncoiaf calcualtes the COI on a sample by sample basis, so we need to break up the WSAF matrix into a list of data frames\n\ninput_data &lt;- purrr::map(seq_len(nrow(wsaf)), function(i) {\n  tibble::tibble(wsmaf = wsaf[i, ], plmaf = plaf) |&gt;\n    tidyr::drop_na()\n})\n\nWe can then run coiaf on each sample using purrr::map_dbl\n\nresults &lt;- purrr::map_dbl(\n  input_data, ~ coiaf::optimize_coi(.x, data_type = \"real\")\n)\n\n# We can then combine the results into a data frame with the sample names\nres_df &lt;- data.frame(Patient = rownames(wsaf), COI = results)",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html#plotting-the-results",
    "href": "tutorials/coiaf/coiaf_analysis.html#plotting-the-results",
    "title": "Running coiaf",
    "section": "Plotting the results",
    "text": "Plotting the results\nNow that we have the estimated COI for each sample, we can plot the results.\n\nlibrary(ggplot2)\nggplot(res_df, aes(x = COI)) +\n  geom_histogram() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html#uncertainty-quantification",
    "href": "tutorials/coiaf/coiaf_analysis.html#uncertainty-quantification",
    "title": "Running coiaf",
    "section": "Uncertainty quantification",
    "text": "Uncertainty quantification\ncoiaf also provides a function to estimate the uncertainty in the COI estimate. This function uses a non-parametric bootstrap to estimate the uncertainty in the COI estimate. Note that this function can take a long time to run, so we will only run it on the first 10 samples.\n\n# We can use the same input data as before\nbootstrap_results &lt;- purrr::map(\n  input_data[1:10], ~ coiaf::bootstrap_ci(.x, solution_method = \"continuous\")\n)\n\n[1] \"All values of t are equal to  1 \\n Cannot calculate confidence intervals\"\n\n# We can then combine the results into a data frame with the sample names\nbootstrap_df &lt;- data.frame(\n  Patient = rownames(wsaf)[1:10], bootstrap_results |&gt; dplyr::bind_rows()\n)\n\nWe can then plot the results for each sample\n\nggplot(\n  bootstrap_df,\n  aes(x = Patient, y = coi, ymin = conf.low, ymax = conf.high)\n) +\n  geom_pointrange() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_analysis.html#summary",
    "href": "tutorials/coiaf/coiaf_analysis.html#summary",
    "title": "Running coiaf",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we have shown how to use coiaf to estimate the complexity of infection from SNP barcode data. For more information on the package, please see the package documentation.",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Estimating COI from SNP data"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_single_AF/background.html",
    "href": "tutorials/PGEcore/naive_single_AF/background.html",
    "title": "Estimate naive allele frequency",
    "section": "",
    "text": "This tool provides a naive implementation of estimating allele frequency from amino acid calls.\nAllele frequency is estimated in two ways: - Allele frequency by read count: within sample allele proportions are calculated using read counts and then the allele frequency is calculated as the average within sample allele frequency - Allele frequency by presence/absence: allele frequency is calculated as the count of a particular allele divided by the total number of alleles observed at a given position",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive single-locus allele freq",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_single_AF/background.html#tool-information",
    "href": "tutorials/PGEcore/naive_single_AF/background.html#tool-information",
    "title": "Estimate naive allele frequency",
    "section": "",
    "text": "This tool provides a naive implementation of estimating allele frequency from amino acid calls.\nAllele frequency is estimated in two ways: - Allele frequency by read count: within sample allele proportions are calculated using read counts and then the allele frequency is calculated as the average within sample allele frequency - Allele frequency by presence/absence: allele frequency is calculated as the count of a particular allele divided by the total number of alleles observed at a given position",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive single-locus allele freq",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_single_AF/background.html#script-usage",
    "href": "tutorials/PGEcore/naive_single_AF/background.html#script-usage",
    "title": "Estimate naive allele frequency",
    "section": "Script Usage",
    "text": "Script Usage\n\nRscript scripts/estimate_allele_frequency_naive/estimate_allele_frequency_naive.R \\\n  --aa_calls data/example_amino_acid_calls.tsv \\\n  --method read_count_prop \\\n  --output allele_freqs.tsv",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive single-locus allele freq",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_MLG_AF_AP/background.html",
    "href": "tutorials/PGEcore/naive_MLG_AF_AP/background.html",
    "title": "Estimate multi-locus prevalence and frequency via naive methods",
    "section": "",
    "text": "The prevalence of a multi-locus genotype is defined as the proportion of samples in which it is detected. In contrast, genotype frequency, while less straightforward to define, represents the probability that a new malaria infection carries this specific genotype. Prevalence and frequency are not equivalent because individuals can harbour multiple strains of malaria simultaneously. As a result, the sum of prevalence values across all genotypes may exceed 1, whereas genotype frequencies must always sum to exactly 1.\nCalculating genotype prevalence and frequency is more complex than it first appears, and depends on the number of heterozygous loci:\n\nIf there are zero heterozygous loci then we know exactly which genotype is present. The genotype is fully phased.\nIf there is a single heterozygous locus then we know which two phased genotypes must be present in the sample. From the relative read counts at this heterozygous locus we can obtain a rough estimate of the within-sample proportions of each genotype.\nIf there are two or more heterozygous loci then we cannot unambiguously state which genotypes are present. Doing so would require running more advanced methods that attempt to phase genotypes.\n\nWe can use these rules to obtain all known phased genotypes from the raw data. For example, imagine we have the following two samples, defined in variant string format:\ncrt:1_2:A_A/C\ncrt:1_2_3:A/C_A_A/C\nThe first sample we can unambiguously phase into the following two genotypes:\ncrt:1_2:A_A\ncrt:1_2:A_C\nThe second sample cannot be unambiguously phased due to the two heterozygous loci.\nNow imagine we are comparing both of these phased genotypes back against the original two samples. We would obtain the following matches:\ncrt:1_2:A_A --&gt; present unambiguously in sample1 and sample2\ncrt:1_2:A_C --&gt; present unambiguously in sample1 only\nNotice that for sample2, even though we were not able to extract its component genotypes, we were still able to identify unambiguous matches against this sample. This is because we were not looking over all three positions, we were only looking at two positions in which there was only one heterozygous locus.\nThis script performs these operations over an entire dataset. Briefly, it takes the following steps:\n\nConvert input data into variant string format\nExtract all unambiguous phased genotypes from the data\nCompare all phased genotypes back against the data to estimate prevalence and frequency\n\nGenotype frequency in this case is obtained as the average of the within-sample genotype frequencies (WSGF) over all samples. This has the advantage of being an unbiased estimator of the true population-level genotype frequency (PLGF) without requiring a known complexity of infection for each sample.",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive multi-locus genotype freq/prev",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_MLG_AF_AP/background.html#tool-information",
    "href": "tutorials/PGEcore/naive_MLG_AF_AP/background.html#tool-information",
    "title": "Estimate multi-locus prevalence and frequency via naive methods",
    "section": "",
    "text": "The prevalence of a multi-locus genotype is defined as the proportion of samples in which it is detected. In contrast, genotype frequency, while less straightforward to define, represents the probability that a new malaria infection carries this specific genotype. Prevalence and frequency are not equivalent because individuals can harbour multiple strains of malaria simultaneously. As a result, the sum of prevalence values across all genotypes may exceed 1, whereas genotype frequencies must always sum to exactly 1.\nCalculating genotype prevalence and frequency is more complex than it first appears, and depends on the number of heterozygous loci:\n\nIf there are zero heterozygous loci then we know exactly which genotype is present. The genotype is fully phased.\nIf there is a single heterozygous locus then we know which two phased genotypes must be present in the sample. From the relative read counts at this heterozygous locus we can obtain a rough estimate of the within-sample proportions of each genotype.\nIf there are two or more heterozygous loci then we cannot unambiguously state which genotypes are present. Doing so would require running more advanced methods that attempt to phase genotypes.\n\nWe can use these rules to obtain all known phased genotypes from the raw data. For example, imagine we have the following two samples, defined in variant string format:\ncrt:1_2:A_A/C\ncrt:1_2_3:A/C_A_A/C\nThe first sample we can unambiguously phase into the following two genotypes:\ncrt:1_2:A_A\ncrt:1_2:A_C\nThe second sample cannot be unambiguously phased due to the two heterozygous loci.\nNow imagine we are comparing both of these phased genotypes back against the original two samples. We would obtain the following matches:\ncrt:1_2:A_A --&gt; present unambiguously in sample1 and sample2\ncrt:1_2:A_C --&gt; present unambiguously in sample1 only\nNotice that for sample2, even though we were not able to extract its component genotypes, we were still able to identify unambiguous matches against this sample. This is because we were not looking over all three positions, we were only looking at two positions in which there was only one heterozygous locus.\nThis script performs these operations over an entire dataset. Briefly, it takes the following steps:\n\nConvert input data into variant string format\nExtract all unambiguous phased genotypes from the data\nCompare all phased genotypes back against the data to estimate prevalence and frequency\n\nGenotype frequency in this case is obtained as the average of the within-sample genotype frequencies (WSGF) over all samples. This has the advantage of being an unbiased estimator of the true population-level genotype frequency (PLGF) without requiring a known complexity of infection for each sample.",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive multi-locus genotype freq/prev",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_MLG_AF_AP/background.html#script-usage",
    "href": "tutorials/PGEcore/naive_MLG_AF_AP/background.html#script-usage",
    "title": "Estimate multi-locus prevalence and frequency via naive methods",
    "section": "Script Usage",
    "text": "Script Usage\nThe multilocus_prevfreq_naive.R script contains all the functions needed to read in the data, calculate prevalence and frequency, and write results to file.\nAn example of usage, executed from the root of this repo, would be:\n\nRscript scripts/multilocus_prevfreq_naive/multilocus_prevfreq_naive.R \\\n    --aa_table data/example_amino_acid_calls.tsv --loci_groups_input \\\n    data/example_loci_groups.tsv --output_path mlafp.tsv",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive multi-locus genotype freq/prev",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_COI/background.html",
    "href": "tutorials/PGEcore/naive_COI/background.html",
    "title": "Estimate naive COI",
    "section": "",
    "text": "There are several simple ways to estimate COI from raw data without relying on models. For example, if a sample contains any loci with 4 alleles then you could argue that COI is 4 (at least). However, sequencing errors can also create this false signal. Slightly more nuanced methods that make use of thresholds can be used to provide results that are robust to errors.\nThese module implements several basic COI estimation methods. It uses the following steps:\n\nReads in allele call data, for example produced from a PMO object.\nFor each sample, counts the number of alleles at each locus and sorts these into decreasing order. Either takes the \\(n^{th}\\) value as the COI estimate (the “integer method”), or takes the value at a stated quantile (the “quantile method”).\nReturns a data.frame of COI estimate for each sample.\nWrites this output data.frame to file.",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive COI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_COI/background.html#tool-information",
    "href": "tutorials/PGEcore/naive_COI/background.html#tool-information",
    "title": "Estimate naive COI",
    "section": "",
    "text": "There are several simple ways to estimate COI from raw data without relying on models. For example, if a sample contains any loci with 4 alleles then you could argue that COI is 4 (at least). However, sequencing errors can also create this false signal. Slightly more nuanced methods that make use of thresholds can be used to provide results that are robust to errors.\nThese module implements several basic COI estimation methods. It uses the following steps:\n\nReads in allele call data, for example produced from a PMO object.\nFor each sample, counts the number of alleles at each locus and sorts these into decreasing order. Either takes the \\(n^{th}\\) value as the COI estimate (the “integer method”), or takes the value at a stated quantile (the “quantile method”).\nReturns a data.frame of COI estimate for each sample.\nWrites this output data.frame to file.",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive COI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_COI/background.html#script-usage",
    "href": "tutorials/PGEcore/naive_COI/background.html#script-usage",
    "title": "Estimate naive COI",
    "section": "Script Usage",
    "text": "Script Usage\nThe estimate_coi_naive.R script contains all the requisite functions to read in the data, estimate COI, and write results to file.\nFor example:\n\nRscript estimate_coi_naive.R --input_path data/example2_allele_table.tsv \\\n    --output_path coi_table.tsv --method integer_method --integer_threshold 5\nRscript estimate_coi_naive.R --input_path data/example2_allele_table.tsv \\\n    --output_path coi_table.tsv --method quantile_method --quantile_threshold \\\n    0.05",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive COI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MALECOT/Template_background.html#summary-sheet",
    "href": "tutorials/MALECOT/Template_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nTODO\n\n\nAuthors\nTODO\n\n\nLatest version\nTODO\n\n\nLicense\nTODO\n\n\nWebsite\nhttps://www.google.com\n\n\nCode repository\nhttps://www.google.com\n\n\nPublication\nhttps://www.google.com\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO"
  },
  {
    "objectID": "tutorials/MALECOT/Template_background.html#purpose",
    "href": "tutorials/MALECOT/Template_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nA quick one paragraph description of what the tool does. For an example, see the DRpower page."
  },
  {
    "objectID": "tutorials/MALECOT/Template_background.html#existing-resources",
    "href": "tutorials/MALECOT/Template_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/MALECOT/Template_background.html#citation",
    "href": "tutorials/MALECOT/Template_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/DRpower/DRpower_design.html",
    "href": "tutorials/DRpower/DRpower_design.html",
    "title": "Designing a study with multiple end-points",
    "section": "",
    "text": "Here, we will use DRpower to help us design a prospective malaria molecular surveillance (MMS) study. We are interested in the following questions:\n\nWhat is the prevalence of sulfadoxine/pyrimethamine (SP) mutations?\nAre there any pfkelch13 mutations in our region?\nWhat is the prevalence of pfhrp2/3 gene deletions, and is this prevalence above 5% (the WHO threshold at which a nationwide change in RDTs is recommended)?\n\nBased on initial budget calculations, we are aiming to collect 100 samples from each of 3 sites in each of 10 provinces. We want to know what sort of power we can expect with these numbers for each of the end-points above.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Designing a study"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_design.html#premise",
    "href": "tutorials/DRpower/DRpower_design.html#premise",
    "title": "Designing a study with multiple end-points",
    "section": "",
    "text": "Here, we will use DRpower to help us design a prospective malaria molecular surveillance (MMS) study. We are interested in the following questions:\n\nWhat is the prevalence of sulfadoxine/pyrimethamine (SP) mutations?\nAre there any pfkelch13 mutations in our region?\nWhat is the prevalence of pfhrp2/3 gene deletions, and is this prevalence above 5% (the WHO threshold at which a nationwide change in RDTs is recommended)?\n\nBased on initial budget calculations, we are aiming to collect 100 samples from each of 3 sites in each of 10 provinces. We want to know what sort of power we can expect with these numbers for each of the end-points above.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Designing a study"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_design.html#sp-mutation-prevalence---margin-of-error-moe-calculation",
    "href": "tutorials/DRpower/DRpower_design.html#sp-mutation-prevalence---margin-of-error-moe-calculation",
    "title": "Designing a study with multiple end-points",
    "section": "SP mutation prevalence - margin of error (MoE) calculation",
    "text": "SP mutation prevalence - margin of error (MoE) calculation\nOur plan is to pool results over the 3 sites within each province to arrive at an estimate of the overall prevalence of SP mutations at the province level. When pooling results it is important to take account of intra-cluster correlation (ICC) in our confidence intervals (CIs). Failure to do so will lead to unrealistically tight intervals. DRpower provides several functions for calculating the expected margin of error (MoE) given assumptions about sample size, the ICC, and the known prevalence of mutations.\nFirst, let’s load the package and set the random seed so our results are reproducible:\n\nlibrary(DRpower)\nset.seed(1)\n\nWe will use the get_margin() function to tell us the MoE we can expect if the prevalence of a mutation is 10% at the province level. We set the parameters to define 3 clusters of 100 samples. We assume an ICC value of 0.05:\n\n# get MoE using simple calculation\nget_margin(N = 100, n_clust = 3, prevalence = 0.1, ICC = 0.05)\n\n    lower     upper \n 1.719297 18.280703 \n\n\nThis output tells us that we expect our CI to range from 1.7% at the lower end to 18.3% at the upper end when estimating a variant at 10% prevalence. In other words, we expect a MoE of 8.3%. This is precise enough for most purposes.\nOur worst case MoE will occur when the prevalence of the marker is 50%. We can explore this here:\n\n# get worst case MoE\nget_margin(N = 100, n_clust = 3, prevalence = 0.5, ICC = 0.05)\n\n   lower    upper \n36.19883 63.80117 \n\n\nIn the worst case our MoE goes up to \\(\\pm\\) 14%, which is still reasonably precise.\nHowever, this approach assumes that we know the level of intra-cluster correlation exactly. It also uses a normal approximation when producing CIs, which is a bad approximation when the prevalence is very low or very high. A more cautious approach is to estimate prevalence using the Bayesian model within DRpower. If we plan to use this approach, then we should use the get_margin_Bayesian() function to estimate the MoE:\n\n# get expected MoE via the Bayesian approach\nget_margin_Bayesian(N = rep(100, 3), prevalence = 0.1, reps = 1e2)\n\n      estimate    CI_2.5   CI_97.5\nlower   3.8705  3.391827  4.349173\nupper  25.1018 23.891353 26.312247\n\n\nWe should look at the first column of this data.frame when comparing to the previous values. We now find that our credible interval (CrI) is expected to range from 3.8% to 25.1%. This is quite different from what we found under the simpler get_margin() method, being tighter at low prevalence but wider at high prevalence. Note that unlike the previous function these are estimated values, hence they are presented along with a 95% CI. We can make estimate more precise if needed by increasing reps.\nWe can explore our worst case MoE using the same method:\n\n# get worst case MoE via the Bayesian approach\nget_margin_Bayesian(N = rep(100, 3), prevalence = 0.5, reps = 1e2)\n\n      estimate   CI_2.5  CI_97.5\nlower  33.4947 31.92120 35.06820\nupper  65.5736 64.12608 67.02112\n\n\nIn the worst case we can expect our CI to range from 33.5% to 66.6%, a MoE of around \\(\\pm\\) 16.5%. This is slightly higher than before, but still precise enough for most purposes.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Designing a study"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_design.html#pfkelch13-mutations---presenceabsence-analysis",
    "href": "tutorials/DRpower/DRpower_design.html#pfkelch13-mutations---presenceabsence-analysis",
    "title": "Designing a study with multiple end-points",
    "section": "pfkelch13 mutations - presence/absence analysis",
    "text": "pfkelch13 mutations - presence/absence analysis\nWhen it comes to pfkelch13, we are interested in knowing if there are any mutants in the province, rather than estimating the prevalence of mutants. A single positive sample would prove that there are mutants in the province, however, if the prevalence is very low we may get unlucky and miss them. We can use the get_power_presence() function to explore our probability of detecting at least one positive sample:\n\n# get power when prevalence is 1%\nget_power_presence(N = rep(100, 3), prevalence = 0.01, ICC = 0.05)\n\n[1] 65.38844\n\n\nWe can see that if pfkelch13 mutants were at 1% prevalence at the province level then we would have a 65% chance of detecting one in any of our 3 sites. This is quite low power - we normally aim for at least 80%. Compare that to the result if we assume mutants are at 2% in the province:\n\n# get power when prevalence is 2%\nget_power_presence(N = rep(100, 3), prevalence = 0.02, ICC = 0.05)\n\n[1] 88.08013\n\n\nWe now have close to 90% power. So, we can say we are powered to detect rare variants down to around 2% prevalence, but if they are less common than this we risk missing them. Whether or not this is an acceptable level of sensitivity is a programmatic decision.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Designing a study"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_design.html#pfhrp23-deletions---comparison-against-5-threshold",
    "href": "tutorials/DRpower/DRpower_design.html#pfhrp23-deletions---comparison-against-5-threshold",
    "title": "Designing a study with multiple end-points",
    "section": "pfhrp2/3 deletions - comparison against 5% threshold",
    "text": "pfhrp2/3 deletions - comparison against 5% threshold\nWe plan to follow the WHO Master Protocol, switching RDTs at a national level if any province has a prevalence of pfhrp2/3 deletions significantly over 5%. We can use the get_power_threshold() function to calculate our power under this model. We assume here that the prevalence of deletions at the province level is 10%:\n\n# estimate power under WHO approach\nget_power_threshold(N = rep(100, 3), prevalence = 0.1, prev_thresh = 0.05, ICC = 0.05, reps = 1e3)\n\n  prev_thresh power lower upper\n1        0.05  59.9 56.79 62.95\n\n\nWe only have around 60% power under this design. We would obtain a higher power if we assumed the prevalence of pfhrp2/3 deletions was higher than 10%, but we would need to be able to justify this assumption with evidence - for example results of a pilot study, or results from neighbouring countries.\nIn our case, we will instead abandon the threshold comparison approach in favour of a two-step design. Our new plan will be to look for the presence of any pfhrp2/3 deletions at the province level, and only if they are detected we will perform a follow-up study in that province. The nice thing about this is that we have already performed this power calculation, as it becomes a presence/absence analysis exactly as we did for pfkelch13. We therefore know that we have power to detect single deletions down to around 2% prevalence.\nFor our follow-up study, we can explore what would happen if we doubled the number of sites in a province:\n\n# estimate power with 6 sites of 100\nget_power_threshold(N = rep(100, 6), prevalence = 0.1, prev_thresh = 0.05, ICC = 0.05, reps = 1e3)\n\n  prev_thresh power lower upper\n1        0.05  78.4 75.72 80.91\n\n\nWe find that power is around 79%, which is acceptable. We will therefore go ahead with this two-stage design.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Designing a study"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_design.html#summary",
    "href": "tutorials/DRpower/DRpower_design.html#summary",
    "title": "Designing a study with multiple end-points",
    "section": "Summary",
    "text": "Summary\nWe have used DRpower to explore the precision and power we can expect for a study design consisting of 100 samples in each of 3 sites within a province. This has revealed that:\n\nWhen estimating SP mutations we will have good precision at the province level, probably with a MoE of around \\(\\pm\\) 15%.\nFor pfkelch13 mutations, we are powered to detect rare variants down to around 2%. Any lower than this and we run the risk of missing them.\nFor pfhrp2/3 deletions, we do not have power under the WHO Master Protocol approach. We have therefore modified our study design to a two-stage process that focuses first on detection, and then on prevalence estimation. In provinces where deletions are identified we will run a second phase with at least 6 sites of 100 samples.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Designing a study"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_background.html#summary-sheet",
    "href": "tutorials/DRpower/DRpower_background.html#summary-sheet",
    "title": "DRpower",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nStudy design, prevalence estimation\n\n\nAuthors\nBob Verity, Shazia Ruybal-Pesántez\n\n\nLatest version\nv1.0.2\n\n\nLicense\nMIT\n\n\nWebsite\nhttps://mrc-ide.github.io/Drpower\n\n\nCode repository\nhttps://github.com/mrc-ide/Drpower\n\n\nPublication\nUnpublished\n\n\nTutorial author\nBob Verity\n\n\nTutorial date\n10-Dec-23",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_background.html#purpose",
    "href": "tutorials/DRpower/DRpower_background.html#purpose",
    "title": "DRpower",
    "section": "Purpose",
    "text": "Purpose\nThe DRpower R package is primarily intended for pfhrp2/3 deletion studies, however, it can also be applied to drug resistance data - or indeed and data that takes the form of a prevalence estimate (numerator and denominator). It can be used in two main ways:\n\nIn the design phase of a study to perform power and sample size calculations. To tell us how many sites we need to recruit, and how many samples per site.\nIn the analysis phase of a study to estimate the prevalence of the marker of interest and optionally to compare this value against a set threshold.\n\nThe approach uses a Bayesian model to estimate the prevalence, which takes into account the potential of intra-cluster correlation in multi-site studies.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_background.html#existing-resources",
    "href": "tutorials/DRpower/DRpower_background.html#existing-resources",
    "title": "DRpower",
    "section": "Existing resources",
    "text": "Existing resources\n\nThe DRpower website gives details of the statistical arguments behind the tool as well as installation instructions and worked tutorials for both design and analysis phases.\nThe pfhrp2/3 Planner provides a web-based interface to the DRpower tool. It does not have all the functionality of the R package, but can be used to carry out simple design and analysis tasks in a user-friendly way.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_background.html#citation",
    "href": "tutorials/DRpower/DRpower_background.html#citation",
    "title": "DRpower",
    "section": "Citation",
    "text": "Citation\nPlease use the following citation:\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_background.html#summary-sheet",
    "href": "tutorials/isoRelate/isoRelate_background.html#summary-sheet",
    "title": "isoRelate",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nIdentity-by-decent (IBD), IBD multiple infections, IBD networks and significance plots\n\n\nAuthors\nLyndan Henden\n\n\nLatest version\nv0.1.0\n\n\nLicense\nYEAR:2018 COPYRIGHT HOLDER: Lyndal Henden\n\n\nWebsite\nhttps://github.com/bahlolab/isoRelate/\n\n\nCode repository\nhttps://github.com/bahlolab/isoRelate/\n\n\nPublication\nhttps://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007279\n\n\nTutorial authors\nKirsty McCann\n\n\nTutorial date\n12-Dec-23"
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_background.html#purpose",
    "href": "tutorials/isoRelate/isoRelate_background.html#purpose",
    "title": "isoRelate",
    "section": "Purpose",
    "text": "Purpose\nThe isoRelate R package has been developed with the intention to perform pairwise identity by decent analysis on haploid recombining organisms in the presence of multiclonal infections using SNP genotype data, but can also be applicable to whole genome sequencing data.\nisoRelate uses a novel statistic that has been developed by inferred IBD status at genomic locations. This statistic is used to identify loci under positive selection and illustrate visually appealling relatedness networks as a means of exploring shared haplotypes within populations.\nAppropriate uses for isoRelate 1. Estimate proportion of genome shared IBD between isolate pairs, 2. Detect genomic regions that are identicial by decent between isolate pairs, 3. IBD within multiple infections, 4. Identify loci under positive selection, 5. Develops networks and is visually appealing.\nisoRelate performs pairwise relatedness mapping on haploid isolates using a first order continuous time hidden Markov model (HMM). Those isolates with multiple infections where the infected individual is carrying multiple genetically distinct strains of the species may have multiplicity of infection (MOI) greater than 1, and isoRelate can analyze them using a diploid model, rather than a haploid model.\nThe IBD segments are calculated using the Viterbi algorithm, which finds the single most likely sequence of IBD states that could have generated the observed genotypic data. An alternative method to this is to calculate the posterior probability of IBD sharing, which calculates the probability of sharing 0, 1 or 2 alleles IBD at each SNP, given the genotypic data. Thus, in addition to the Viterbi algorithm, we provide a function to generate the average posterior probability of IBD sharing for each isolate pair, which is calculated as;\n\\[avePostPr = \\frac{PostPr(IBD = 1)}{2} + PostPr(IBD = 2).\\]"
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_background.html#data-formats",
    "href": "tutorials/isoRelate/isoRelate_background.html#data-formats",
    "title": "isoRelate",
    "section": "Data formats",
    "text": "Data formats\nTo use this tool, isoRelate requires PED and MAP formats that contains unphased genotype data for SNPs. Previously, generated of ped and map files has not been adequately documented. This tutorial attempts to add additional steps to process vcf data into pedmap format for use in isoRelate.\nPlease note: indels will cause problems when running isoRelate!\nThe typical pedmap format is as follows: 1. Family ID 2. Isolate ID 3. Paternal ID 4. Maternal ID 5. Multiplicity of infection (MOI) (1 = single infection or haploid, 2 = multiple infections or diploid) 6. Phenotype (1=unaffected, 2=affected, 0=unknown)\nThe IDs are alphanumeric: the combination of family and isolate ID should uniquely identify a sample. Columns 7 onwards (white-space delimited) are the isolate genotypes for biallelic SNPs where the A and B alleles are coded as 1 and 2 respectively and missing genotypes are coded as 0. All SNPs must have two alleles specified and each allele should be in a separate column. For single infections, genotypes should be specified as homozygous. Either Both alleles should be missing (i.e. 0) or neither. Column labels are not required.\nImportantly the paternal ID, maternal ID and phenotype columns are not used by isoRelate, however are required for completeness of the pedigree. For this tutorial, these columns have not been included.\nExamples of informative family IDs are the sample collection site or country, however family IDs can be the same as the isolate IDs.\nThe MAP file contains exactly 4 columns of information: 1. Chromosome 2. SNP identifier 3. Genetic map distance (centi-Morgans or Morgans) 4. Base-pair position\nwhere each row describes a single marker. Genetic map distances and base-pair positions are expected to be positive values. The MAP file must be ordered by increasing chromosome genetic map distance. SNP identifiers can contain any characters except spaces or tabs; also, you should avoid * symbols in names. The MAP file must contain as many markers as are in the PED file. Column labels are not required."
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_background.html#existing-resources",
    "href": "tutorials/isoRelate/isoRelate_background.html#existing-resources",
    "title": "isoRelate",
    "section": "Existing resources",
    "text": "Existing resources\n\nisoRelate vignettes - https://github.com/bahlolab/isoRelate/blob/master/vignettes/introduction.Rmd\nIdentity-by-descent analyses for measuring population dynamics and selection in recombining pathogens - https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007279"
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_background.html#citation",
    "href": "tutorials/isoRelate/isoRelate_background.html#citation",
    "title": "isoRelate",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"isoRelate\"):\n@Manual{,\n    title = {Identity-by-descent analyses for measuring population dynamics and selection in recombining pathogens},\n    author = {Lyndan Henden, Stuart Lee, Alyssa Barry, Melanie Bahlo},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_installation.html",
    "href": "tutorials/isoRelate/isoRelate_installation.html",
    "title": "Installing isoRelate",
    "section": "",
    "text": "plasmogenepi.r-universe plasmogenepi.r-universe, greatly simplifies package installation.\nTo install isoRelate, use the following:\n\n# Install isoRelate in R:\ninstall.packages('isoRelate', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\nAssuming all has been installed correctly with no errors, now load the package in RStudio:\n\nlibrary(isoRelate)"
  },
  {
    "objectID": "tutorials/isoRelate/isoRelate_installation.html#step-1-use-plasmogenepi.r-universe-installaion",
    "href": "tutorials/isoRelate/isoRelate_installation.html#step-1-use-plasmogenepi.r-universe-installaion",
    "title": "Installing isoRelate",
    "section": "",
    "text": "plasmogenepi.r-universe plasmogenepi.r-universe, greatly simplifies package installation.\nTo install isoRelate, use the following:\n\n# Install isoRelate in R:\ninstall.packages('isoRelate', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\nAssuming all has been installed correctly with no errors, now load the package in RStudio:\n\nlibrary(isoRelate)"
  },
  {
    "objectID": "tutorials/hierfstat/background.html#summary-sheet",
    "href": "tutorials/hierfstat/background.html#summary-sheet",
    "title": "Hierfstat",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nCalculate F statistics\n\n\n\n\n\n\nAuthors\nJerome Goudet\nThibaut Jombart\nZhian N. Kamvar\nEric Archer\nOlivier Hardy\n\n\nLatest version\nv0.5-11\n\n\n\n\n\n\nLicense\nGPL (&gt;=2)\n\n\n\n\n\n\nWebsite\nhttps://cran.r-project.org/web/packages/hierfstat/index.html\n\n\n\n\n\n\nCode repository\nhttps://github.com/jgx65/hierfstat\n\n\n\n\n\n\nPublication\nhttps://www.sciencedirect.com/science/article/pii/S1567134807001037\n\n\n\n\n\n\nTutorial authors\nJody Phelan\n\n\n\n\n\n\nTutorial date\n13-12-2023",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hierfstat/background.html#purpose",
    "href": "tutorials/hierfstat/background.html#purpose",
    "title": "Hierfstat",
    "section": "Purpose",
    "text": "Purpose\nThis tool allows you to calculate F statistics on genetic data. You can read in data from a variety of formats including VCF.",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hierfstat/background.html#existing-resources",
    "href": "tutorials/hierfstat/background.html#existing-resources",
    "title": "Hierfstat",
    "section": "Existing resources",
    "text": "Existing resources\n\nThe original paper with a samll tutorial is available here",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hierfstat/background.html#citation",
    "href": "tutorials/hierfstat/background.html#citation",
    "title": "Hierfstat",
    "section": "Citation",
    "text": "Citation\n@Manual{,\n  title = {hierfstat: Estimation and Tests of Hierarchical F-Statistics},\n  author = {Jerome Goudet and Thibaut Jombart},\n  year = {2022},\n  note = {https://www.r-project.org, https://github.com/jgx65/hierfstat},\n}",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/BRO/Template_installation.html",
    "href": "tutorials/BRO/Template_installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/BRO/Template_installation.html#step-n-what-to-do",
    "href": "tutorials/BRO/Template_installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/BRO/Template_analysis.html",
    "href": "tutorials/BRO/Template_analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/BRO/Template_analysis.html#the-data",
    "href": "tutorials/BRO/Template_analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/BRO/Template_analysis.html#first-analysis-section",
    "href": "tutorials/BRO/Template_analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages."
  },
  {
    "objectID": "tutorials/BRO/Template_analysis.html#summary",
    "href": "tutorials/BRO/Template_analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial."
  },
  {
    "objectID": "tutorials/SNP-slice/background.html#summary-sheet",
    "href": "tutorials/SNP-slice/background.html#summary-sheet",
    "title": "SNP-slice",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nMOI estimation, reconstruction of SNP haplotypes\n\n\nAuthors\nNianqiao Ju, Jiawei Liu, Qixin He\n\n\nLatest version\n2024-04-30\n\n\nLicense\nGPL-3.0\n\n\nWebsite\nhttps://www.google.com\n\n\nCode repository\nhttps://github.com/nianqiaoju/snp-slice\n\n\nPublication\nhttps://academic.oup.com/bioinformatics/article/40/6/btae344/7695237#467922339\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/background.html#purpose",
    "href": "tutorials/SNP-slice/background.html#purpose",
    "title": "SNP-slice",
    "section": "Purpose",
    "text": "Purpose\nThe SNP-slice algorithm enables reconstruction of SNP haplotypes from bi-allelic data, allowing for inference of haplotype/strain frequencies.",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/background.html#existing-resources",
    "href": "tutorials/SNP-slice/background.html#existing-resources",
    "title": "SNP-slice",
    "section": "Existing resources",
    "text": "Existing resources\nThe SNP-slice GitHub repository includes scripts for implementing the SNP-slice algorithm, as well as scripts for analyzing output and generating figures.",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/background.html#citation",
    "href": "tutorials/SNP-slice/background.html#citation",
    "title": "SNP-slice",
    "section": "Citation",
    "text": "Citation\nThe publication associated with the SNP-slice algorithm and analysis scripts can be found here (Ju et al 2024 Bioinformatics).",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/background.html",
    "href": "tutorials/DEploidIBD/background.html",
    "title": "PGEforge",
    "section": "",
    "text": "Feature\nDescription\n\n\n\n\nMain use\nInference of COI, proportions, haplotypes and within-sample IBD\n\n\nAuthors\nSha Joe Zhu, Jacob Almargo-Garcia, Gil McVean\n\n\nLatest version\nv0.6-beta\n\n\nWebsite\nhttps://deploid.readthedocs.io/en/latest/index.html\n\n\nCode repository\nhttps://github.com/DEploid-dev/DEploid\n\n\nPublication\nhttps://doi.org/10.7554/eLife.40845 (DEploidIBD), https://doi.org/10.1093/bioinformatics/btx530 (DEploid)\n\n\nTutorial Author\nJason A. Hendry\n\n\nTutorial Date\n11-Dec-2023",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/background.html#summary-sheet",
    "href": "tutorials/DEploidIBD/background.html#summary-sheet",
    "title": "PGEforge",
    "section": "",
    "text": "Feature\nDescription\n\n\n\n\nMain use\nInference of COI, proportions, haplotypes and within-sample IBD\n\n\nAuthors\nSha Joe Zhu, Jacob Almargo-Garcia, Gil McVean\n\n\nLatest version\nv0.6-beta\n\n\nWebsite\nhttps://deploid.readthedocs.io/en/latest/index.html\n\n\nCode repository\nhttps://github.com/DEploid-dev/DEploid\n\n\nPublication\nhttps://doi.org/10.7554/eLife.40845 (DEploidIBD), https://doi.org/10.1093/bioinformatics/btx530 (DEploid)\n\n\nTutorial Author\nJason A. Hendry\n\n\nTutorial Date\n11-Dec-2023",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/background.html#purpose",
    "href": "tutorials/DEploidIBD/background.html#purpose",
    "title": "PGEforge",
    "section": "Purpose",
    "text": "Purpose\nThe main purpose of DEploidIBD is to infer complexity of infection (COI), strain proportions, strain haplotypes, and within-sample IBD from P. falciparum whole genome sequencing (WGS) data.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/background.html#existing-resources",
    "href": "tutorials/DEploidIBD/background.html#existing-resources",
    "title": "PGEforge",
    "section": "Existing resources",
    "text": "Existing resources\n\nThere is a github repository containing the source code. Note that both DEploid and DEploidIBD run from the same codebase, with the later being invoked when the -ibd flag is used.\nThere is also a website with additional information about installaion and running the tools.\nFor DEploid, some examples of how to run the software are available in the supplementary data.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/background.html#citation",
    "href": "tutorials/DEploidIBD/background.html#citation",
    "title": "PGEforge",
    "section": "Citation",
    "text": "Citation\nIf you run DEploidIBD, please use the following citation:\n@article{RN96,\n   author = {Zhu, Sha Joe and Hendry, Jason A. and Almagro-Garcia, Jacob and Pearson, Richard D. and Amato, Roberto and Miles, Alistair and Weiss, Daniel J. and Lucas, Tim C. D. and Nguyen, Michele and Gething, Peter W. and Kwiatkowski, Dominic and McVean, Gil},\n   title = {The origins and relatedness structure of mixed infections vary with local prevalence of P. falciparum malaria},\n   journal = {eLife},\n   volume = {8},\n   pages = {e40845},\n   ISSN = {2050-084X},\n   DOI = {10.7554/eLife.40845},\n   url = {https://doi.org/10.7554/eLife.40845},\n   year = {2019},\n   type = {Journal Article}\n}\nOr if you run DEploid, use:\n@article{Zhu2018,\n   author = {Zhu, S. J. and Almagro-Garcia, J. and McVean, G.},\n   title = {Deconvolution of multiple infections in Plasmodium falciparum from high throughput sequencing data},\n   journal = {Bioinformatics},\n   volume = {34},\n   number = {1},\n   pages = {9-15},\n   ISSN = {1367-4803 (Print)\n1367-4803},\n   DOI = {10.1093/bioinformatics/btx530},\n   year = {2018},\n   type = {Journal Article}\n}",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/part1-preparing-data.html",
    "href": "tutorials/DEploidIBD/part1-preparing-data.html",
    "title": "PGEforge",
    "section": "",
    "text": "In this section of the tutorial we will prepare our input data to run DEploidIBD. In particular, we will need to: - Filter our input VCF file - Prepare a file containing population-level allele frequency (PLAF) estimates\nHere is a schematic of what we will do:\n\n\n\n\n\n\nBefore beginning this section, please make sure you have: - git cloned the PGEforge repository so that you can access some example data (TODO: improve) - completed the “Installation” section of the tutorial.\n\n\n\nWe are going to create some folders to organise the outputs of this tutorial and to make it easy to access the data. First, create a directory for the tutorial:\nmkdir DEploidIBD-tutorial\ncd DEploidIBD-tutorial\nNext, let’s create a softlink within the DEploidIBD-tutorial directory to the PGEforge data directory:\nln -s &lt;path/to/PGEforge/data&gt; .\nThis soft link (also known as a symbolic link) will allow us to access the contents of the PGEforge/data directory without using a long relative or absolute path. For example, you should be able to type:\nls data\n…and see folders containing example data made available in the PGEforge repository.\nFinally, we will make a few other directories that will be useful for organising our outputs:\nmkdir -p {data_filtered/intermediates,results,scripts}\nOur directory structure should now look like this:\n.\n├── data -&gt; /path/to/PGEforge/data\n├── data_filtered\n│   ├── intermediates\n├── results\n└── scripts\n\n\n\nThe primary input of DEploidIBD is a Variant Call Format (VCF) file containing information about the genotypes and allelic depths of samples in our population. Before running DEploidIBD, it is important that the variants within this VCF file are carefully filtered. There are three main objectives of this filtering: 1. Remove incompatible variants. - DEploidIBD works on biallelic SNPs; indels or multiallelic SNPs are not supported. 2. Remove problematic variants. - There can be artifacts in sequencing data, particularly during read mapping, that can negatively impact the inferences made by DEploidIBD. We will attempt to remove these beforehand. 3. Remove superfluous variants. - Depending on your application, you may not need all of the varaints in your VCF. Using less will reduce the runtime of DEploidIBD\nFilter to high-quality biallelic SNPs\nLet’s begin. We will use the VCF that was downloaded from the Pf3k project and filtered to samples from Democratic Republic of Congo. We can define a variable pointing to the file path:\nINPUT_VCF=data/wgs/pf3k/DRCongo/SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.DRCongo.vcf.gz\nThis VCF has already been filtered to high-quality biallelic SNPs. But for completeness, we will run the command that was used to perform this filtering here (TODO: include a quality filter):\necho \"Filtering to high-quality biallelic SNPs...\"\nFILT1_VCF=data_filtered/intermediates/pf3k.DRCongo.biallelic_snp.vcf.gz\nbcftools view --types snps --min-alleles 2 --max-alleles 2 $INPUT_VCF \\\n  | bcftools +fill-tags -Oz -o $FILT1_VCF -- -t All,F_MISSING\necho \"Done.\"\nWe have also uesd the bcftools +fill-tags command to recompute information about our variant sites after filtering.\nA useful subcommand in within bcftools is bcftools stats. This computes a variety of summary statistics and distributions for a given VCF file, and outputs the results to the terminal. Running the following command:\nbcftools stats $FILT1_VCF | grep ^SN\nwill extract only the key summary statistics. Here, your output should be:\nSN      0       number of samples:      113\nSN      0       number of records:      247496\nSN      0       number of no-ALTs:      0\nSN      0       number of SNPs: 247496\nSN      0       number of MNPs: 0\nSN      0       number of indels:       0\nSN      0       number of others:       0\nSN      0       number of multiallelic sites:   0\nSN      0       number of multiallelic SNP sites:       0\nYou can see that we have 113 samples and 247496 SNPs. Importantly, there are no indels or biallelic SNPs in the VCF file.\nFilter to common SNPs Next, we will filter to common SNPs. SNPs that are very rare will not be present in many samples and will be less informative for inferring COI than common SNPs. Moreover, as most Plasmodium falciparum SNPs are rare, this is a good way to remove a large number of SNPs and reduce the runtime of DEploidIBD. Here, we are going to only keep SNPs which have a population-level allele frequency (PLAF) of between [0.1, 0.9]. There is a significant disadvantage though, in that for the SNPs that are removed we will not get phasing information. If you are only interested in COI and IBD estimates, this will not be a problem. But if you are interested in phasing specific rare variants, you should consider either excluding this step, or a least making sure that your variants of interest are retained.\nAt the same time, we will also remove sites with more than 10% missingness.\necho \"Filtering to common SNPs...\"\nFILT2_VCF=data_filtered/intermediates/pf3k.DRCongo.biallelic_snp.af10-0.90.vcf.gz\nbcftools view $FILT1_VCF -e 'F_MISSING &gt;= 0.1' --min-af 0.1 --max-af 0.9 -Oz -o $FILT2_VCF\necho \"Done.\"\nLet’s check the number of SNPs left after this filtering:\nSN      0       number of samples:      113\nSN      0       number of records:      7869\nSN      0       number of no-ALTs:      0\nSN      0       number of SNPs: 7869\nSN      0       number of MNPs: 0\nSN      0       number of indels:       0\nSN      0       number of others:       0\nSN      0       number of multiallelic sites:   0\nSN      0       number of multiallelic SNP sites:       0\nSo by filtering to SNPs with a PLAF between 10 and 90%, we have reduced our VCF to 3.1% (7869/247496) of the SNPs.\nFilter mapping artifacts using depth Next, we will remove sites with very low or very high mean read depths. Sites with very low depth will produce noisy data and are worth removing. Sites with abnormally high depth may represent areas with issues during mapping. For the purpose of the tutorial, we will just perform a simple filtering using fixed thresholds:\necho \"Filtering based on depth...\"\nFINAL_VCF=data_filtered/pf3k.DRCongo.final.vcf.gz\nbcftools view $FILT2_VCF -i 'MEAN(FORMAT/DP) &gt;= 20 & MEAN(FORMAT/DP) &lt;= 100' -Oz -o $FINAL_VCF\necho \"Done.\"\nIf you are doing this on your own data, it would be better to filter depth based on the read depth distribution of your samples. For example, you could exclude sites with a mean depth below the 5th- or above the 95th-percentiles.\nNow our VCF file has been reduced to 6933 variants. Further filtering is possible, but for COI, proportion and IBD estimation this VCF should be adequate.\n\n\n\nDEploid and DEploidIBD require as input estimated allele frequencies for sites in the local population. In our case, the VCF we have been filtering represents all samples from Democratic Republic of the Congo in Pf3k, and we can treat this as our population. We will use bcftools query to extract the allele frequencies, and put them in an appropriate format for DEploidIBD.\nPLAF_TXT=data_filtered/pf3k.DRCongo.final.plaf.txt\necho -e 'POS\\tCHROM\\tPLAF' &gt; $PLAF_TXT \nbcftools query -f '%CHROM\\t%POS\\t%AF\\n' $FINAL_VCF &gt;&gt; $PLAF_TXT\nLet’s quickly look at the output:\nhead $PLAF_TXT\nWhich produces:\nPOS     CHROM   PLAF\nPf3D7_01_v3     95680   0.378378\nPf3D7_01_v3     95906   0.288288\nPf3D7_01_v3     98866   0.221239\nPf3D7_01_v3     100330  0.696429\nPf3D7_01_v3     107823  0.283186\nPf3D7_01_v3     108002  0.362832\nPf3D7_01_v3     114473  0.579646\nPf3D7_01_v3     125303  0.451327\nPf3D7_01_v3     130339  0.659292\nThe PLAF file has a row for each SNP and three columns: the chromosome, position, and an estimate of the PLAF.\n\n\n\nIn this section we have taken an input VCF representing our population of interest, and we have filtered it down to a smaller set of variants which will be sufficient for COI, proportion and within-sample IBD estimation. From this VCF, we have created a text file containing PLAFs for each SNP which will be used when we run DEploidIBD.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Part 1: Preparing your data"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/part1-preparing-data.html#part-1-preparing-data-for-deconvolution-with-deploidibd",
    "href": "tutorials/DEploidIBD/part1-preparing-data.html#part-1-preparing-data-for-deconvolution-with-deploidibd",
    "title": "PGEforge",
    "section": "",
    "text": "In this section of the tutorial we will prepare our input data to run DEploidIBD. In particular, we will need to: - Filter our input VCF file - Prepare a file containing population-level allele frequency (PLAF) estimates\nHere is a schematic of what we will do:\n\n\n\n\n\n\nBefore beginning this section, please make sure you have: - git cloned the PGEforge repository so that you can access some example data (TODO: improve) - completed the “Installation” section of the tutorial.\n\n\n\nWe are going to create some folders to organise the outputs of this tutorial and to make it easy to access the data. First, create a directory for the tutorial:\nmkdir DEploidIBD-tutorial\ncd DEploidIBD-tutorial\nNext, let’s create a softlink within the DEploidIBD-tutorial directory to the PGEforge data directory:\nln -s &lt;path/to/PGEforge/data&gt; .\nThis soft link (also known as a symbolic link) will allow us to access the contents of the PGEforge/data directory without using a long relative or absolute path. For example, you should be able to type:\nls data\n…and see folders containing example data made available in the PGEforge repository.\nFinally, we will make a few other directories that will be useful for organising our outputs:\nmkdir -p {data_filtered/intermediates,results,scripts}\nOur directory structure should now look like this:\n.\n├── data -&gt; /path/to/PGEforge/data\n├── data_filtered\n│   ├── intermediates\n├── results\n└── scripts\n\n\n\nThe primary input of DEploidIBD is a Variant Call Format (VCF) file containing information about the genotypes and allelic depths of samples in our population. Before running DEploidIBD, it is important that the variants within this VCF file are carefully filtered. There are three main objectives of this filtering: 1. Remove incompatible variants. - DEploidIBD works on biallelic SNPs; indels or multiallelic SNPs are not supported. 2. Remove problematic variants. - There can be artifacts in sequencing data, particularly during read mapping, that can negatively impact the inferences made by DEploidIBD. We will attempt to remove these beforehand. 3. Remove superfluous variants. - Depending on your application, you may not need all of the varaints in your VCF. Using less will reduce the runtime of DEploidIBD\nFilter to high-quality biallelic SNPs\nLet’s begin. We will use the VCF that was downloaded from the Pf3k project and filtered to samples from Democratic Republic of Congo. We can define a variable pointing to the file path:\nINPUT_VCF=data/wgs/pf3k/DRCongo/SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.DRCongo.vcf.gz\nThis VCF has already been filtered to high-quality biallelic SNPs. But for completeness, we will run the command that was used to perform this filtering here (TODO: include a quality filter):\necho \"Filtering to high-quality biallelic SNPs...\"\nFILT1_VCF=data_filtered/intermediates/pf3k.DRCongo.biallelic_snp.vcf.gz\nbcftools view --types snps --min-alleles 2 --max-alleles 2 $INPUT_VCF \\\n  | bcftools +fill-tags -Oz -o $FILT1_VCF -- -t All,F_MISSING\necho \"Done.\"\nWe have also uesd the bcftools +fill-tags command to recompute information about our variant sites after filtering.\nA useful subcommand in within bcftools is bcftools stats. This computes a variety of summary statistics and distributions for a given VCF file, and outputs the results to the terminal. Running the following command:\nbcftools stats $FILT1_VCF | grep ^SN\nwill extract only the key summary statistics. Here, your output should be:\nSN      0       number of samples:      113\nSN      0       number of records:      247496\nSN      0       number of no-ALTs:      0\nSN      0       number of SNPs: 247496\nSN      0       number of MNPs: 0\nSN      0       number of indels:       0\nSN      0       number of others:       0\nSN      0       number of multiallelic sites:   0\nSN      0       number of multiallelic SNP sites:       0\nYou can see that we have 113 samples and 247496 SNPs. Importantly, there are no indels or biallelic SNPs in the VCF file.\nFilter to common SNPs Next, we will filter to common SNPs. SNPs that are very rare will not be present in many samples and will be less informative for inferring COI than common SNPs. Moreover, as most Plasmodium falciparum SNPs are rare, this is a good way to remove a large number of SNPs and reduce the runtime of DEploidIBD. Here, we are going to only keep SNPs which have a population-level allele frequency (PLAF) of between [0.1, 0.9]. There is a significant disadvantage though, in that for the SNPs that are removed we will not get phasing information. If you are only interested in COI and IBD estimates, this will not be a problem. But if you are interested in phasing specific rare variants, you should consider either excluding this step, or a least making sure that your variants of interest are retained.\nAt the same time, we will also remove sites with more than 10% missingness.\necho \"Filtering to common SNPs...\"\nFILT2_VCF=data_filtered/intermediates/pf3k.DRCongo.biallelic_snp.af10-0.90.vcf.gz\nbcftools view $FILT1_VCF -e 'F_MISSING &gt;= 0.1' --min-af 0.1 --max-af 0.9 -Oz -o $FILT2_VCF\necho \"Done.\"\nLet’s check the number of SNPs left after this filtering:\nSN      0       number of samples:      113\nSN      0       number of records:      7869\nSN      0       number of no-ALTs:      0\nSN      0       number of SNPs: 7869\nSN      0       number of MNPs: 0\nSN      0       number of indels:       0\nSN      0       number of others:       0\nSN      0       number of multiallelic sites:   0\nSN      0       number of multiallelic SNP sites:       0\nSo by filtering to SNPs with a PLAF between 10 and 90%, we have reduced our VCF to 3.1% (7869/247496) of the SNPs.\nFilter mapping artifacts using depth Next, we will remove sites with very low or very high mean read depths. Sites with very low depth will produce noisy data and are worth removing. Sites with abnormally high depth may represent areas with issues during mapping. For the purpose of the tutorial, we will just perform a simple filtering using fixed thresholds:\necho \"Filtering based on depth...\"\nFINAL_VCF=data_filtered/pf3k.DRCongo.final.vcf.gz\nbcftools view $FILT2_VCF -i 'MEAN(FORMAT/DP) &gt;= 20 & MEAN(FORMAT/DP) &lt;= 100' -Oz -o $FINAL_VCF\necho \"Done.\"\nIf you are doing this on your own data, it would be better to filter depth based on the read depth distribution of your samples. For example, you could exclude sites with a mean depth below the 5th- or above the 95th-percentiles.\nNow our VCF file has been reduced to 6933 variants. Further filtering is possible, but for COI, proportion and IBD estimation this VCF should be adequate.\n\n\n\nDEploid and DEploidIBD require as input estimated allele frequencies for sites in the local population. In our case, the VCF we have been filtering represents all samples from Democratic Republic of the Congo in Pf3k, and we can treat this as our population. We will use bcftools query to extract the allele frequencies, and put them in an appropriate format for DEploidIBD.\nPLAF_TXT=data_filtered/pf3k.DRCongo.final.plaf.txt\necho -e 'POS\\tCHROM\\tPLAF' &gt; $PLAF_TXT \nbcftools query -f '%CHROM\\t%POS\\t%AF\\n' $FINAL_VCF &gt;&gt; $PLAF_TXT\nLet’s quickly look at the output:\nhead $PLAF_TXT\nWhich produces:\nPOS     CHROM   PLAF\nPf3D7_01_v3     95680   0.378378\nPf3D7_01_v3     95906   0.288288\nPf3D7_01_v3     98866   0.221239\nPf3D7_01_v3     100330  0.696429\nPf3D7_01_v3     107823  0.283186\nPf3D7_01_v3     108002  0.362832\nPf3D7_01_v3     114473  0.579646\nPf3D7_01_v3     125303  0.451327\nPf3D7_01_v3     130339  0.659292\nThe PLAF file has a row for each SNP and three columns: the chromosome, position, and an estimate of the PLAF.\n\n\n\nIn this section we have taken an input VCF representing our population of interest, and we have filtered it down to a smaller set of variants which will be sufficient for COI, proportion and within-sample IBD estimation. From this VCF, we have created a text file containing PLAFs for each SNP which will be used when we run DEploidIBD.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Part 1: Preparing your data"
    ]
  },
  {
    "objectID": "tutorials/THEREALMcCOIL/RMCL_background.html#summary-sheet",
    "href": "tutorials/THEREALMcCOIL/RMCL_background.html#summary-sheet",
    "title": "THE REAL McCOIL",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nEstimation of complexity of infection\n\n\nAuthors\nHsiao-Han Chang\n\n\nLatest version\nNA\n\n\nLicense\nNONE\n\n\nWebsite\nhttps://github.com/EPPIcenter/THEREALMcCOIL\n\n\nCode repository\nhttps://github.com/EPPIcenter/THEREALMcCOIL\n\n\nPublication\nhttps://pubmed.ncbi.nlm.nih.gov/28125584/\n\n\nTutorial authors\nNick Brazeau\n\n\nTutorial date\nDec 12 2023",
    "crumbs": [
      "Tool resources",
      "THEREALMcCOIL",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/THEREALMcCOIL/RMCL_background.html#purpose",
    "href": "tutorials/THEREALMcCOIL/RMCL_background.html#purpose",
    "title": "THE REAL McCOIL",
    "section": "Purpose",
    "text": "Purpose\nTHE REAL McCOIL is a method for estimating the complexity of infections within a malaria-infected host, where complexity of infection (COI) is defined as the number of distinct genotypic parasites within the host. This tutorial focuses on the categorical method for estimating COI.",
    "crumbs": [
      "Tool resources",
      "THEREALMcCOIL",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/THEREALMcCOIL/RMCL_background.html#existing-resources",
    "href": "tutorials/THEREALMcCOIL/RMCL_background.html#existing-resources",
    "title": "THE REAL McCOIL",
    "section": "Existing resources",
    "text": "Existing resources\nChang, HH et al. 2017, PLoS Comp Biol",
    "crumbs": [
      "Tool resources",
      "THEREALMcCOIL",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/THEREALMcCOIL/RMCL_background.html#citation",
    "href": "tutorials/THEREALMcCOIL/RMCL_background.html#citation",
    "title": "THE REAL McCOIL",
    "section": "Citation",
    "text": "Citation\nChang HH, Worby CJ, Yeka A, Nankabirwa J, Kamya MR, Staedke SG, Dorsey G, Murphy M, Neafsey DE, Jeffreys AE, Hubbart C, Rockett KA, Amato R, Kwiatkowski DP, Buckee CO, Greenhouse B. THE REAL McCOIL: A method for the concurrent estimation of the complexity of infection and SNP allele frequency for malaria parasites. PLoS Comput Biol. 2017 Jan 26;13(1):e1005348. doi: 10.1371/journal.pcbi.1005348. PMID: 28125584; PMCID: PMC5300274.",
    "crumbs": [
      "Tool resources",
      "THEREALMcCOIL",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html",
    "title": "Analysis with MIPAnalyzer",
    "section": "",
    "text": "MIPanalyzer is a tool suite for analyzing Molecular Inversion Probe (MIP) data with functionalitiies that could be extended to other platforms/outputs such as small variant call files (VCFs) or multi-loci amplicon data. However, for the purpose of this tutorial, we will focus on MIP data using the file structure and outputs from the Bailey-Lab and MIPTools.\n\nThe MIPanalyzer suite has four main arenas of analysis:\n\nMunging/wrangling MIP data\n\nCalculating genetic distance\nAnalyzing genetic structure\n\nSummarizing Drug Resistance Mutations (work in progress)\n\nThe tool is intended to make analysis of MIP data straightforward and standardized. Notably, many of the methods within the package rely on within-sample allele-frequencies to account for complexity of infection/polyclonality."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#overview",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#overview",
    "title": "Analysis with MIPAnalyzer",
    "section": "",
    "text": "MIPanalyzer is a tool suite for analyzing Molecular Inversion Probe (MIP) data with functionalitiies that could be extended to other platforms/outputs such as small variant call files (VCFs) or multi-loci amplicon data. However, for the purpose of this tutorial, we will focus on MIP data using the file structure and outputs from the Bailey-Lab and MIPTools.\n\nThe MIPanalyzer suite has four main arenas of analysis:\n\nMunging/wrangling MIP data\n\nCalculating genetic distance\nAnalyzing genetic structure\n\nSummarizing Drug Resistance Mutations (work in progress)\n\nThe tool is intended to make analysis of MIP data straightforward and standardized. Notably, many of the methods within the package rely on within-sample allele-frequencies to account for complexity of infection/polyclonality."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#data-input",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#data-input",
    "title": "Analysis with MIPAnalyzer",
    "section": "Data Input",
    "text": "Data Input\nWe assume that the user has two files: (1) a variant call file following VCF4 specifications and (2) an amino acid table (TBD)."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#munging-mip-data",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#munging-mip-data",
    "title": "Analysis with MIPAnalyzer",
    "section": "Munging MIP Data",
    "text": "Munging MIP Data\nAs input for MIPanalyzer, we will start with a variant call file, abbreviated as a VCF of the Sanger Barcode from Vietnam. The VCF is converted into a mipanalyzer class that is either biallelic or multiallelic depending on the user specification.\n\n#......................\n# read in the VCF from the main data page \n# and convert to  mipanalyzer_biallelic object\n#......................\ndat_biallelic &lt;- MIPanalyzer::vcf_to_mipanalyzer_biallelic(\"../../data/snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz\") \n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 136\n  header_line: 137\n  variant count: 91\n  column count: 106\n\nMeta line 136 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 91\n  Character matrix gt cols: 106\n  skip: 0\n  nrows: 91\n  row_num: 0\n\nProcessed variant: 91\nAll variants processed\n\n\nThe dat_biallelic object is a protected MIPanalyzer class (mipanalyzer_biallelic) that is tidy and fast container for genomic data. It has slots for sample name, loci (CHROM/POS/ID/REF/ALT/QUAL/FILTER/INFO), coverage (alleleic depth or dp), counts (allelic counts or ad), filter history (upstream manipulations performed on the object), and vcfmeta (the header of the original VCF file).\n\nFilter\nData can be quickly filtered using a series of commands. Moreover, MIPanalyzer incorporates several data visualization functions that extend the interactiveness of filtering process. Below, I explore how much data will be lost if I filter loci that have at least 10 reads for all samples (minimal coverage is 10 reads with a 0% tolerance for any missing data).\n\ndat_biallelic %&gt;% \n explore_filter_coverage_loci(min_coverage = 10, max_low_coverage = 0)\n\n\n\n\n\n\n\n\nI then apply this filter as so:\n\ndat_biallelic &lt;- dat_biallelic %&gt;%\n  filter_coverage_loci(min_coverage = 10, max_low_coverage = 0)\n\nThere are similar capabilities to filter by sample (filter_samples) and by loci (filter_loci). Within the filter_samples framework, users can exclude samples that have poor coverage cross the genome. Below, I should how these fucntions can be used: filtering samples based on coverage (25 depth for all sites is shown below).\n\ndat_biallelic %&gt;% \n explore_filter_coverage_samples(min_coverage = 25, max_low_coverage = 0)\n\n\n\n\n\n\n\ndat_biallelic &lt;- dat_biallelic %&gt;%\n  filter_coverage_samples(min_coverage = 10, max_low_coverage = 0)\n\nSimilarly, loci that have an unexpected amount of sequencing effort (“jackpotting”) that may be prone to sequencing error can also be excluded with filter_overcounts. Finally, users can remove sites that are uninformative (all variants are the same) with filter_loci_invariant. This is demonstrated below:\n\ndat_biallelic &lt;- dat_biallelic %&gt;%\n  filter_loci_invariant()"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#calculating-genetic-distances",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#calculating-genetic-distances",
    "title": "Analysis with MIPAnalyzer",
    "section": "Calculating Genetic Distances",
    "text": "Calculating Genetic Distances\nFor this section, we will explore pairwise relatedness through a variety of genetic distance measures. In the first schema, we can consider how allele frequencies are similar between samples using the \\(d_{ab}\\) metric proposed by the MalariaGEN Plasmodium falciparum Community Project in the article “Genomic epidemiology of artemisinin resistant malaria”, eLIFE (2016) (get_genomic_distance).\nIn a separate, schema, we can calculate the similarity between two samples based on whether the genetic sequence is identical at given genomic positions, or loci. We can either simply measure the number of sites with identical alleles between two individuals, termed identity by state (IBS), or, we can use statistical models to determine if identical alleles and “blocks” of the genome were likely to be inherited from a common ancestor, termed identity by descent (IBD). Separately, we can calculate if within-sample allele frequencies (+/- some degree of tolerance) are identical between two individuals - which is essentially a continuous extension of IBS. The IBS calculations are straightforward and just represent comparisons between samples. IBD calculations require parametric assumptions in order to account for the heritability factor (as part of its definition). The IBD calculator incorporated in MIPanalyzer is based off the classic Malécot definition of IBD and is considered in a maximum-likelihood framework.\n\n# AF based methods \ndab_est &lt;- get_genomic_distance(dat_biallelic, report_progress = FALSE)\n# identity based methods\nmix_est &lt;- get_IB_mixture(dat_biallelic, report_progress = FALSE)\nibd_est &lt;- inbreeding_mle(dat_biallelic, \n                          f = seq(0, 1, l = 5),\n                          report_progress = FALSE)\nibs_est &lt;- get_IBS_distance(dat_biallelic, report_progress = FALSE)\n\nThe output for each distance calculator is a distance matrix with the upper triangle filled in, with the exception of the IBD estimator that also includes the log-likelihood of each inbreeding level, \\(f\\), considered by the model (specified by user).\n\n# plot all pairwise distances (make a distance instead of similarity score with 1-x)\np1 &lt;- plot_distance(1 - ibd_est$mle, col_pal = \"magma\") + ggtitle(\"IBD Dist\")\np2 &lt;- plot_distance(1 - mix_est, col_pal = \"inferno\") + ggtitle(\"Mixture Dist\")\np3 &lt;- plot_distance(1 - ibs_est, col_pal = \"plasma\") + ggtitle(\"IBS Dist\")\np4 &lt;- plot_distance(dab_est, col_pal = \"viridis\") + ggtitle(\"Dab Dist\")\n\ncowplot::plot_grid(p1,p2,p3,p4, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\nAs expected, our samples have much less IBD than IBS. Similarly, the \\(D_{ab}\\) has a wide range of values. However, across all four genomic distances, sample pairs that are highly related are"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#analyzing-genetic-structure",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#analyzing-genetic-structure",
    "title": "Analysis with MIPAnalyzer",
    "section": "Analyzing Genetic Structure",
    "text": "Analyzing Genetic Structure\nIn this section, we will perform a principal component analysis and plot the results to assess for structure (as part of an exploratory data analysis exercise).\n\n# calculate within-sample allele frequencies\nwsaf &lt;- get_wsaf(dat_biallelic, )\n# produce pca \npca &lt;- pca_wsaf(wsaf)\n\n\nplot_pca(pca, num_components = 3)\n\n\nWe can also look at the amount of variance captured in our principal components as well as which loci are contributing most to the variation. Understanding the amount of variance that is captured in the principal components conveys how much structure is in the data, while understanding which loci are contributing the most variation conveys signals of selection, drift, etc.\n\n# plot percentage variance explained\nplot_pca_variance(pca)\n\n\n\n# get CHROM in numeric format for each locus\nchrom_numeric &lt;- mapply(function(x) as.numeric(strsplit(x, \"_\")[[1]][2]), dat_biallelic$loci$CHROM)\n\n# plot loading values\nplot_pca_contribution(pca, component = 1, chrom = chrom_numeric, pos = dat_biallelic$loci$POS)\n\n\n\n\n\n\n\n\nAnother approach to analyzing structure is to assess principal coordinate analysis. While principal component analysis (PCA) is based on linear combinations, principal coordinate analysis (PCoA) is based on minimizing distance based on an internal loss function (also called classical scaling, function back-ended by ape::pcoa). The utility of PCoA versus PCA, is that PCoA can take any distance (matrix) that the user specifies and thus can capture genetic structures that may be nonlinear.\n\n# genomic distance from malariagen manuscript as above\ngdist &lt;- get_genomic_distance(dat_biallelic, report_progress = FALSE)\n# perform PCoA\npcoa &lt;- pcoa_genomic_distance(gdist)\n# scatterplot\nplot_pcoa(pcoa, num_components = 3)"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#summarizing-drug-resistance-mutations-work-in-progress",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#summarizing-drug-resistance-mutations-work-in-progress",
    "title": "Analysis with MIPAnalyzer",
    "section": "Summarizing Drug Resistance Mutations (work in progress)",
    "text": "Summarizing Drug Resistance Mutations (work in progress)\nTo Do (pending file format from Bailey group)"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#summary",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_analysis.html#summary",
    "title": "Analysis with MIPAnalyzer",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we explored how to use the MIPanalyzer to munge/wrangle MIP data, filter samples, and prepare our MIP data for analysis. We then performed genetic analysis by calculating genetic distances as well as analyzing our data for genetic structure."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_installation.html",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_installation.html",
    "title": "Installing MIPanalyzer",
    "section": "",
    "text": "You can directly install MIPanalyzer from plasmogenepi.r-universe, which greatly simplifies installation."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_installation.html#installing-from-r-universe",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_installation.html#installing-from-r-universe",
    "title": "Installing MIPanalyzer",
    "section": "",
    "text": "You can directly install MIPanalyzer from plasmogenepi.r-universe, which greatly simplifies installation."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_installation.html#installing-from-github",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_installation.html#installing-from-github",
    "title": "Installing MIPanalyzer",
    "section": "Installing From Github",
    "text": "Installing From Github\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"mrc-ide/MIPanalyzer\")\n\n\nDependencies\nMIPanalyzer relies on the Rcpp package, which requires certain OS-specific dependencies:\n\nWindows\n\nDownload and install the appropriate version of Rtools for your version of R. On installation, ensure you check the box to arrange your system PATH as recommended by Rtools\n\nMac OS X\n\nDownload and install XCode\nWithin XCode go to Preferences : Downloads and install the Command Line Tools\n\nLinux (Debian/Ubuntu)\n\nInstall the core software development utilities required for R package development as well as LaTeX by executing\n\nsudo apt-get install r-base-dev texlive-full\n\nAssuming everything installed correctly, you can now load the package:\n\nlibrary(MIPanalyzer)"
  },
  {
    "objectID": "tutorials/rehh/Template_background.html#summary-sheet",
    "href": "tutorials/rehh/Template_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nextended haplotype heterozygosity estimation and visualization\n\n\n\n\nAuthors\nAlexander Klassmann\nMathieu Gautier\nRenaud Vitalis\n\n\nLatest version\n3.2.2\n\n\n\n\nLicense\nGPL-2 | GPL-3 [expanded from: GPL (≥ 2)]\n\n\n\n\nWebsite\nhttps://www.google.com\n\n\n\n\nCode repository\nhttps://gitlab.com/oneoverx/rehh\n\n\n\n\nPublication\nhttps://doi.org/10.1111/1755-0998.12634\n\n\n\n\nTutorial authors\nKaramoko Niaré\n\n\n\n\nTutorial date\n12/11/2023"
  },
  {
    "objectID": "tutorials/rehh/Template_background.html#purpose",
    "href": "tutorials/rehh/Template_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nThis tool computes several metrics of positive selection signatures along the genome of an organism. These metrics include the Extended Haplotype Homozygosity (EHH), integrated EHH (IHH), site-specific EHH (EHHS), integrated EHHS (iES), integrated haplotype homozygosity score (iHS), ratio of EHHS (Rsb) and cross-population EHH (XP-EHH). EHH is defined as the probability that two randomly chosen chromosomes, carrying the core allele, are homozygous over a given surrounding chromosomal region. More details on the EHH can be found in the paper published by Sabeti et al. 2002. This istechnically a useful gemomic metrics in malaria, as the nature and length of shared haplotypes and EHH scores flanking an allele, such as a drug resistance mutation, show the ancestral relationship between mutants and how selective sweep survived recombinations over time."
  },
  {
    "objectID": "tutorials/rehh/Template_background.html#existing-resources",
    "href": "tutorials/rehh/Template_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nThis tutorial give details on how to run the functions"
  },
  {
    "objectID": "tutorials/rehh/Template_background.html#citation",
    "href": "tutorials/rehh/Template_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\n @Article{,\n    author = {Mathieu Gautier and Renaud Vitalis},\n    title = {rehh: An R package to detect footprints of selection in\n      genome-wide SNP data from haplotype structure},\n    journal = {Bioinformatics},\n    year = {2012},\n    volume = {28},\n    number = {8},\n    pages = {1176-1177},\n  }\n  @Article{,\n    author = {Mathieu Gautier and Alexander Klassmann and Renaud\n      Vitalis},\n    title = {rehh 2.0: a reimplementation of the R package rehh to\n      detect positive selection from haplotype structure},\n    journal = {Molecular Ecology Resources},\n    year = {2017},\n    volume = {17},\n    number = {1},\n    pages = {78-90},\n    doi = {10.1111/1755-0998.12634},\n  }"
  },
  {
    "objectID": "tutorials/moire/moire_installation.html",
    "href": "tutorials/moire/moire_installation.html",
    "title": "Installing moire",
    "section": "",
    "text": "Install moire in R:\n\ninstall.packages('moire', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tool resources",
      "moire",
      "Installation"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "PGEforge",
    "section": "Welcome",
    "text": "Welcome\nPGEforge is a community-driven platform designed to simplify Plasmodium genomic data analysis. The process of analyzing genomic data often involves various software tools with different formats, user interfaces, and levels of accessibility. These differences can create barriers, especially for those without strong computational skills.\nAlthough there are numerous existing software tools available to analyze data for malaria genomic surveillance, there is little guidance outlining how to choose, use, or assemble these tools to translate genetic data into interpretable and actionable results. For example, estimating the extent of antimalarial resistance from sequence data requires several different steps depending on the number and type of relevant polymorphisms and outcomes of interest. To improve the accessibility, accuracy, and reproducibility of genomic surveillance, a roadmap is needed to guide the specific analysis functionalities needed for any given end result, as well as knowledge of which tools are available to perform those functionalities.\nAn adjacent but extremely important challenge is the large variation in software standards of currently available tools for analyzing Plasmodium genomic data, often limiting the accessibility and utility of these tools. Many tools are challenging to use due to inadequate documentation, difficulty in installation due to operating system incompatibility or poorly documented dependencies, and requirements for data to be input in specific, nonstandard formats.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#our-motivation",
    "href": "index.html#our-motivation",
    "title": "PGEforge",
    "section": "Our motivation",
    "text": "Our motivation\nPGEforge aims to overcome these challenges by providing clear tutorials, streamlined workflows, and comprehensive resources to help researchers at all levels understand and analyze genomic data. By prioritizing the end-user, we strive to encourage better software development and foster an inclusive research community. Our goal is to make advanced genomic analysis accessible to everyone, promoting collaboration and accelerating progress in malaria research.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-site",
    "href": "index.html#how-to-use-this-site",
    "title": "PGEforge",
    "section": "How to use this site",
    "text": "How to use this site\nThe site is organized to help you efficiently access and utilize our resources. Here’s an overview of what you’ll find in each section:\n\nData: Access a curated selection of datasets commonly used in genomic analysis. These datasets help you familiarize yourself with standard data formats and are used consistently across our tutorials, ensuring easy comparison of tools.\nTool Landscaping: Discover a wide range of analysis tools, complete with basic information on their functions, where to find them, and whether we offer tutorials for them.\nSoftware Standards: Learn about our objective software standards aimed at guiding developers towards best practices and creating user-friendly tools.\nTool Resources: Explore comprehensive guides covering the entire process of installing software, formatting data, running basic functions, and interpreting outputs. These resources are designed to make complex tasks straightforward and accessible. We also introduce the “PlasmoGenEpi R-universe,” a website that simplifies tool installation. Systematic benchmarking on simulated data to assess performance and accuracy for various metrics is available for some tools (watch this space).\nAnalysis Workflows: Understand how different tools can be integrated to address common questions in malaria genomic epidemiology. This section outlines eight defined use cases and analysis functionalities (desired outputs), typical analysis workflows and maps tools to various analytical steps.\nHow to Contribute: Find out how you can contribute to this community-driven resource. We welcome input from all areas of the research community to continuously improve and expand our platform.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_analysis.html",
    "href": "tutorials/moire/moire_analysis.html",
    "title": "Running Moire Example",
    "section": "",
    "text": "In this analysis we will be using the SNP barcode data from the sanger 100 SNP Plasmodium falciparum barcode (Chang et al. 2019). This was generated by subsetting the WGS data (also described within the Data section of this website) and we will be using the data from the DRC. See the description in the Data section for more details on this dataset.\nIn this tutorial we will use PGEhammer to convert data from the VCF format to the format required by MOIRE. To install the package run the following command\n\n# Install PGEhammer in R:\ninstall.packages('PGEhammer', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\nNow we can import libraries that we will need.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(vcfR)\nlibrary(PGEhammer)\n\nMOIRE requires input data in either long or wide format, which can be loaded using the load_long_form_data() or load_delimited_data() functions, respectively. The long format represents data with each observed allele for each sample on a separate row, while the wide format uses a row for each sample, and a separate column for each genomic target, with each cell containing a string indicating the collection of alleles that are observed. In the following steps, we will convert the Variant Call Format (VCF) data to the required long format using the function vcf2long from PGEhammer.\n\n# Load the vcf  \nvcf &lt;- read.vcfR('../../data/snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.DRCongo.vcf.gz')\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 136\n  header_line: 137\n  variant count: 91\n  column count: 122\n\nMeta line 136 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 91\n  Character matrix gt cols: 122\n  skip: 0\n  nrows: 91\n  row_num: 0\n\nProcessed variant: 91\nAll variants processed\n\n# Convert to long format \ndf &lt;- vcf2long(vcf)\n\nConverting from vcf to long format...\n\n\nReformatting complete.\n\nhead(df) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nsample_id\nlocus\nallele\nread_count\n\n\n\n\nQG0182-C\nPf3D7_01_v3_145515\nallele-1\n36\n\n\nQG0182-C\nPf3D7_01_v3_145515\nallele-2\n0\n\n\nQG0182-C\nPf3D7_01_v3_179347\nallele-1\n0\n\n\nQG0182-C\nPf3D7_01_v3_179347\nallele-2\n188\n\n\nQG0182-C\nPf3D7_01_v3_180554\nallele-1\n0\n\n\nQG0182-C\nPf3D7_01_v3_180554\nallele-2\n150\n\n\n\n\n\nPrior to executing MOIRE, it is advisable to remove alleles that are likely to be erroneous. In the following step, we apply a filter to remove alleles with a read count below 10, however, how you filter your data should be informed by prior knowledge and experience with your data.\n\ndf &lt;- df |&gt;\n  dplyr::filter(read_count &gt;= 10)\n\nFor the purpose of this tutorial we will subset the data to just include chromosome 1\n\nsubset_df &lt;- df[grepl('^Pf3D7_01', df$locus), ]",
    "crumbs": [
      "Tool resources",
      "moire",
      "Running moire example"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_analysis.html#the-data",
    "href": "tutorials/moire/moire_analysis.html#the-data",
    "title": "Running Moire Example",
    "section": "",
    "text": "In this analysis we will be using the SNP barcode data from the sanger 100 SNP Plasmodium falciparum barcode (Chang et al. 2019). This was generated by subsetting the WGS data (also described within the Data section of this website) and we will be using the data from the DRC. See the description in the Data section for more details on this dataset.\nIn this tutorial we will use PGEhammer to convert data from the VCF format to the format required by MOIRE. To install the package run the following command\n\n# Install PGEhammer in R:\ninstall.packages('PGEhammer', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\nNow we can import libraries that we will need.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(vcfR)\nlibrary(PGEhammer)\n\nMOIRE requires input data in either long or wide format, which can be loaded using the load_long_form_data() or load_delimited_data() functions, respectively. The long format represents data with each observed allele for each sample on a separate row, while the wide format uses a row for each sample, and a separate column for each genomic target, with each cell containing a string indicating the collection of alleles that are observed. In the following steps, we will convert the Variant Call Format (VCF) data to the required long format using the function vcf2long from PGEhammer.\n\n# Load the vcf  \nvcf &lt;- read.vcfR('../../data/snp_barcode/sangerBarcode_SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.DRCongo.vcf.gz')\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 136\n  header_line: 137\n  variant count: 91\n  column count: 122\n\nMeta line 136 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 91\n  Character matrix gt cols: 122\n  skip: 0\n  nrows: 91\n  row_num: 0\n\nProcessed variant: 91\nAll variants processed\n\n# Convert to long format \ndf &lt;- vcf2long(vcf)\n\nConverting from vcf to long format...\n\n\nReformatting complete.\n\nhead(df) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nsample_id\nlocus\nallele\nread_count\n\n\n\n\nQG0182-C\nPf3D7_01_v3_145515\nallele-1\n36\n\n\nQG0182-C\nPf3D7_01_v3_145515\nallele-2\n0\n\n\nQG0182-C\nPf3D7_01_v3_179347\nallele-1\n0\n\n\nQG0182-C\nPf3D7_01_v3_179347\nallele-2\n188\n\n\nQG0182-C\nPf3D7_01_v3_180554\nallele-1\n0\n\n\nQG0182-C\nPf3D7_01_v3_180554\nallele-2\n150\n\n\n\n\n\nPrior to executing MOIRE, it is advisable to remove alleles that are likely to be erroneous. In the following step, we apply a filter to remove alleles with a read count below 10, however, how you filter your data should be informed by prior knowledge and experience with your data.\n\ndf &lt;- df |&gt;\n  dplyr::filter(read_count &gt;= 10)\n\nFor the purpose of this tutorial we will subset the data to just include chromosome 1\n\nsubset_df &lt;- df[grepl('^Pf3D7_01', df$locus), ]",
    "crumbs": [
      "Tool resources",
      "moire",
      "Running moire example"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_analysis.html#running-the-mcmc",
    "href": "tutorials/moire/moire_analysis.html#running-the-mcmc",
    "title": "Running Moire Example",
    "section": "Running the MCMC",
    "text": "Running the MCMC\nNow that we have the data in the correct format we will run the Markov chain Monte Carlo (MCMC) with the default parameters.\n\ndata &lt;- moire::load_long_form_data(subset_df)\nmcmc_results &lt;- moire::run_mcmc(data, is_missing = data$is_missing, verbose = FALSE)",
    "crumbs": [
      "Tool resources",
      "moire",
      "Running moire example"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_analysis.html#summarising-the-results",
    "href": "tutorials/moire/moire_analysis.html#summarising-the-results",
    "title": "Running Moire Example",
    "section": "Summarising the results",
    "text": "Summarising the results\nFrom the MCMC results we can produce estimations for each sample and summarise the results.\nFirst we will look at COI using summarize_coi.\n\n# Estimate the COI for each sample\ncoi_summary &lt;- moire::summarize_coi(mcmc_results)\n\nhead(coi_summary) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nsample_id\npost_coi_lower\npost_coi_med\npost_coi_upper\npost_coi_mean\nnaive_coi\noffset_naive_coi\nprob_polyclonal\n\n\n\n\nQG0182-C\n1\n1\n1\n1.000\n1\n1\n0.000\n\n\nQG0183-C\n1\n1\n2\n1.108\n2\n1\n0.096\n\n\nQG0184-C\n1\n1\n1\n1.018\n1\n1\n0.018\n\n\nQG0185-C\n1\n1\n1\n1.007\n1\n1\n0.007\n\n\nQG0186-C\n1\n1\n2\n1.039\n1\n1\n0.036\n\n\nQG0187-C\n1\n1\n1\n1.000\n1\n1\n0.000\n\n\n\n\n\nThis dataframe includes summaries of both the posterior distribution of COI for each biological sample and the naive estimates.\nBelow we use the summarize_he function to summarise locus heterozygosity from the posterior distribution of sampled allele frequencies.\n\nhe_summary &lt;- moire::summarize_he(mcmc_results)\n\nhead(he_summary) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nlocus\npost_stat_lower\npost_stat_med\npost_stat_upper\npost_stat_mean\n\n\n\n\nPf3D7_01_v3_145515\n0.2670405\n0.3736329\n0.4515084\n0.3683778\n\n\nPf3D7_01_v3_179347\n0.0164229\n0.0630886\n0.1608248\n0.0688653\n\n\nPf3D7_01_v3_180554\n0.1696347\n0.2652525\n0.3638944\n0.2646813\n\n\nPf3D7_01_v3_283144\n0.2752269\n0.3691431\n0.4527259\n0.3680179\n\n\nPf3D7_01_v3_535211\n0.1172749\n0.2383835\n0.3372387\n0.2354225\n\n\n\n\n\nUsing summarize_allele_freqs we can look at individual allele frequencies\n\nallele_freq_summary &lt;- moire::summarize_allele_freqs(mcmc_results)\n\nhead(allele_freq_summary) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\npost_allele_freqs_lower\npost_allele_freqs_med\npost_allele_freqs_upper\npost_allele_freqs_mean\nlocus\nallele\n\n\n\n\n0.6557107\n0.7513633\n0.8412912\n0.7520307\nPf3D7_01_v3_145515\nallele-1\n\n\n0.1587088\n0.2486367\n0.3442893\n0.2479693\nPf3D7_01_v3_145515\nallele-2\n\n\n0.0082800\n0.0326075\n0.0881904\n0.0361381\nPf3D7_01_v3_179347\nallele-1\n\n\n0.9118095\n0.9673925\n0.9917200\n0.9638619\nPf3D7_01_v3_179347\nallele-2\n\n\n0.7608693\n0.8425985\n0.9064267\n0.8409991\nPf3D7_01_v3_180554\nallele-1\n\n\n0.0935733\n0.1574015\n0.2391307\n0.1590009\nPf3D7_01_v3_180554\nallele-2\n\n\n\n\n\nThe summarize_relatedness function provides a dataframe of within-host relatedness\n\nrelatedness_summary &lt;- moire::summarize_relatedness(mcmc_results)\n\nhead(relatedness_summary) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nsample_id\npost_relatedness_lower\npost_relatedness_med\npost_relatedness_upper\npost_relatedness_mean\n\n\n\n\nQG0182-C\nNA\nNA\nNA\nNaN\n\n\nQG0183-C\n0.0070207\n0.5699971\n0.8141899\n0.4491186\n\n\nQG0184-C\n0.9993327\n0.9993360\n0.9993471\n0.9993373\n\n\nQG0185-C\n0.9954492\n0.9954609\n0.9954958\n0.9954689\n\n\nQG0186-C\n0.2001541\n0.8753348\n0.9416712\n0.7888673\n\n\nQG0187-C\nNA\nNA\nNA\nNaN\n\n\n\n\n\nThe moire tool introduces a new metric called effective MOI, which adjusts for within-host relatedness. A detailed description of this metric and how to interpret it can be found here.\n\neffective_coi_summary &lt;- moire::summarize_effective_coi(mcmc_results)\n\nhead(effective_coi_summary) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nsample_id\npost_effective_coi_lower\npost_effective_coi_med\npost_effective_coi_upper\npost_effective_coi_mean\n\n\n\n\nQG0182-C\n1\n1\n1.000000\n1.000000\n\n\nQG0183-C\n1\n1\n1.796385\n1.057058\n\n\nQG0184-C\n1\n1\n1.000000\n1.000012\n\n\nQG0185-C\n1\n1\n1.000000\n1.000032\n\n\nQG0186-C\n1\n1\n1.078041\n1.008387\n\n\nQG0187-C\n1\n1\n1.000000\n1.000000",
    "crumbs": [
      "Tool resources",
      "moire",
      "Running moire example"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_analysis.html#summary",
    "href": "tutorials/moire/moire_analysis.html#summary",
    "title": "Running Moire Example",
    "section": "Summary",
    "text": "Summary\nIn summary, moire estimates allele frequencies, MOI, and within-host relatedness. We have shown how to generate basic results from a VCF. MOIRE has extensive documentation, including more details on other functionality available within the tool and a tutorial validating the outputs using simulated data.",
    "crumbs": [
      "Tool resources",
      "moire",
      "Running moire example"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_background.html#summary-sheet",
    "href": "tutorials/moire/moire_background.html#summary-sheet",
    "title": "moire",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nEstimating multiplicity of infection (MOI), population allele frequencies, and within-host relatedness from polyallelic genomics data.\n\n\nAuthors\nMaxwell Murphy, Bryan Greenhouse\n\n\nLatest version\nv3.5.0\n\n\nLicense\nGNU General Public License v3.0\n\n\nWebsite\nhttps://eppicenter.github.io/moire\n\n\nCode repository\nhttps://github.com/eppicenter/moire\n\n\nPublication\nhttps://doi.org/10.1093/bioinformatics/btad245\n\n\nTutorial authors\nKathryn Murie, Maxwell Murphy\n\n\nTutorial date\n7-Jan-25",
    "crumbs": [
      "Tool resources",
      "moire",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_background.html#purpose",
    "href": "tutorials/moire/moire_background.html#purpose",
    "title": "moire",
    "section": "Purpose",
    "text": "Purpose\nThe moire (Multiplicity Of Infection and allele frequency REcovery) tool can be used to estimate allele frequencies, MOI, and within-host relatedness from genetic data subject to experimental error. It utilises a Markov Chain Monte Carlo (MCMC) based approach to Bayesian estimation and can take both polyallelic and SNP data as inputs. This tool also introduces a new metric called effective MOI (eMOI), which combines MOI and within-host relatedness into a unified and comparable measure of genetic diversity.",
    "crumbs": [
      "Tool resources",
      "moire",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_background.html#existing-resources",
    "href": "tutorials/moire/moire_background.html#existing-resources",
    "title": "moire",
    "section": "Existing resources",
    "text": "Existing resources\n\nThe moire website provides basic usage instructions\nWithin the moire website there is a more in depth tutorial using simulated genotyping data.",
    "crumbs": [
      "Tool resources",
      "moire",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/moire/moire_background.html#citation",
    "href": "tutorials/moire/moire_background.html#citation",
    "title": "moire",
    "section": "Citation",
    "text": "Citation\n\ncitation('moire')\n\nTo cite package 'moire' in publications use:\n\n  Murphy M, Greenhouse B (2024). \"MOIRE: A software package for the\n  estimation of allele frequencies and effective multiplicity of\n  infection from polyallelic data.\" _Bioinformatics_.\n  doi:10.1093/bioinformatics/btae619\n  &lt;https://doi.org/10.1093/bioinformatics/btae619&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {MOIRE: A software package for the estimation of allele frequencies and effective multiplicity of infection from polyallelic data},\n    author = {Maxwell Murphy and Bryan Greenhouse},\n    journal = {Bioinformatics},\n    year = {2024},\n    month = {oct},\n    doi = {10.1093/bioinformatics/btae619},\n  }",
    "crumbs": [
      "Tool resources",
      "moire",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/rehh/Template_installation.html",
    "href": "tutorials/rehh/Template_installation.html",
    "title": "Installing (rehh)",
    "section": "",
    "text": "Released packages can be installed from CRAN using install.packages(“rehh”) The current version of the repository can be installed with help of the R-package devtools by install.packages(“devtools”, repos = ‘http://cran.us.r-project.org’) devtools::install_gitlab(“oneoverx/rehh”)"
  },
  {
    "objectID": "tutorials/rehh/Template_installation.html#step-1-install-the-rehh-r-package",
    "href": "tutorials/rehh/Template_installation.html#step-1-install-the-rehh-r-package",
    "title": "Installing (rehh)",
    "section": "",
    "text": "Released packages can be installed from CRAN using install.packages(“rehh”) The current version of the repository can be installed with help of the R-package devtools by install.packages(“devtools”, repos = ‘http://cran.us.r-project.org’) devtools::install_gitlab(“oneoverx/rehh”)"
  },
  {
    "objectID": "tutorials/rehh/Template_installation.html#step-2-install-and-install-the-dependencies-r.utils-vcfr-and-data.table",
    "href": "tutorials/rehh/Template_installation.html#step-2-install-and-install-the-dependencies-r.utils-vcfr-and-data.table",
    "title": "Installing (rehh)",
    "section": "Step 2: install and install the dependencies (R.utils, vcfR and data.table)",
    "text": "Step 2: install and install the dependencies (R.utils, vcfR and data.table)\nR package R.utils is available on CRAN and can be installed in R as install.packages(“R.utils”) To install the pre-release version of R.utils that is available in Git branch develop on GitHub, use: remotes::install_github(“HenrikBengtsson/R.utils”, ref=“develop”) vcfR is available at CRAN. To install use: install.packages(“vcfR”) The development version of vcfR can be installed through github: devtools::install_github(repo=“knausb/vcfR”) data.table R package can be installed from CRAN using install.packages(“data.table”) latest development version (only if newer available) data.table::update_dev_pkg() latest development version (force install) install.packages(“data.table”, repos=“https://rdatatable.gitlab.io/data.table”)"
  },
  {
    "objectID": "tutorials/rehh/Template_installation.html#step-3-install-bcftools",
    "href": "tutorials/rehh/Template_installation.html#step-3-install-bcftools",
    "title": "Installing (rehh)",
    "section": "Step 3: install bcftools",
    "text": "Step 3: install bcftools\ngit clone –recurse-submodules https://github.com/samtools/htslib.git git clone https://github.com/samtools/bcftools.git cd bcftools # The following is optional: # autoheader && autoconf && ./configure –enable-libgsl –enable-perl-filters make"
  },
  {
    "objectID": "tutorials/rehh/Template_installation.html#step-4-load-rehh-and-dependent-packages-for-the-analysis",
    "href": "tutorials/rehh/Template_installation.html#step-4-load-rehh-and-dependent-packages-for-the-analysis",
    "title": "Installing (rehh)",
    "section": "Step 4: load rehh and dependent packages for the analysis",
    "text": "Step 4: load rehh and dependent packages for the analysis\nlibrary(rehh) library(R.utils) library(vcfR) library(data.table)"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_background.html#summary-sheet",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nMIP data analysis\n\n\nAuthors\nRobert Verity; OJ Watson; Nick Brazeau\n\n\nLatest version\nv1.1.0\n\n\nLicense\nMIT\n\n\nWebsite\nhttps://mrc-ide.github.io/MIPanalyzer/\n\n\nCode repository\nhttps://github.com/mrc-ide/MIPanalyzer\n\n\nPublication\nhttps://pubmed.ncbi.nlm.nih.gov/32355199/\n\n\nTutorial authors\nNick Brazeau\n\n\nTutorial date\nDec 13 2023"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_background.html#purpose",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nA quick one paragraph description of what the tool does. For an example, see the DRpower page."
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_background.html#existing-resources",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/MIPanalyzer/MIPanalyzer_background.html#citation",
    "href": "tutorials/MIPanalyzer/MIPanalyzer_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/THEREALMcCOIL/RMCL_installation.html",
    "href": "tutorials/THEREALMcCOIL/RMCL_installation.html",
    "title": "THE REAL McCOIL",
    "section": "",
    "text": "THE REAL McCOIL, can be installed from the EPPIcenter GitHub and built/“made” for your machine as below:\n\nClone the code into the directory using the command git clone https://github.com/EPPIcenter/THEREALMcCOIL.git.\nNavigate to the categorical method directory: cd THEREALMcCOIL/categorical_method\nComplete the make/installation: R CMD SHLIB McCOIL_categorical_code.c llfunction_het.c (Note, prior to running this make step, you may need to erase prior .so and .o files).\n\n\n\nTHE REAL McCOIL relies on the Rcpp package, which requires certain OS-specific dependencies:\n\nWindows\n\nDownload and install the appropriate version of Rtools for your version of R. On installation, ensure you check the box to arrange your system PATH as recommended by Rtools\n\nMac OS X\n\nDownload and install XCode\nWithin XCode go to Preferences : Downloads and install the Command Line Tools\n\nLinux (Debian/Ubuntu)\n\nInstall the core software development utilities required for R package development as well as LaTeX by executing\n\nsudo apt-get install r-base-dev texlive-full",
    "crumbs": [
      "Tool resources",
      "THEREALMcCOIL",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/THEREALMcCOIL/RMCL_installation.html#installation-from-github",
    "href": "tutorials/THEREALMcCOIL/RMCL_installation.html#installation-from-github",
    "title": "THE REAL McCOIL",
    "section": "",
    "text": "THE REAL McCOIL, can be installed from the EPPIcenter GitHub and built/“made” for your machine as below:\n\nClone the code into the directory using the command git clone https://github.com/EPPIcenter/THEREALMcCOIL.git.\nNavigate to the categorical method directory: cd THEREALMcCOIL/categorical_method\nComplete the make/installation: R CMD SHLIB McCOIL_categorical_code.c llfunction_het.c (Note, prior to running this make step, you may need to erase prior .so and .o files).\n\n\n\nTHE REAL McCOIL relies on the Rcpp package, which requires certain OS-specific dependencies:\n\nWindows\n\nDownload and install the appropriate version of Rtools for your version of R. On installation, ensure you check the box to arrange your system PATH as recommended by Rtools\n\nMac OS X\n\nDownload and install XCode\nWithin XCode go to Preferences : Downloads and install the Command Line Tools\n\nLinux (Debian/Ubuntu)\n\nInstall the core software development utilities required for R package development as well as LaTeX by executing\n\nsudo apt-get install r-base-dev texlive-full",
    "crumbs": [
      "Tool resources",
      "THEREALMcCOIL",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/installation.html",
    "href": "tutorials/DEploidIBD/installation.html",
    "title": "PGEforge",
    "section": "",
    "text": "Please note that the following installation instructions were derived from those described in the github repository.\n\n\nDEploidIBD is implemented in C++ and the code needs to be compiled to your local computer hardware before it can run.\nMacOS Use the package manager Homebrew to install autoconf and other tools necessary for compilation:\nbrew install pkg-config automake autoconf autoconf-archive cppunit\nUbuntu/Debian For Ubuntu or Debian operaing systems, use the package manager apt-get to install the compilation tools:\napt-get install build-essential autoconf autoconf-archive libcppunit-dev zlib1g-dev\n\n\n\nUse git to clone the repository:\ngit clone https://github.com/DEploid-dev/DEploid.git\ncd DEploid\ngit submodule update --init --recursive --remote\n\n\n\nFinally, you can compile DEploidIBD with the following commands:\n./bootstrap\nmake\nIf the compilation has run successfully, you should be able to run the DEploidIBD executable with:\n./dEploid\nThe github repository also includes some example data and commands. For example, you can run:\n./dEploid \\\n -vcf data/testData/PG0390-C.test.vcf \\\n -plaf data/testData/labStrains.test.PLAF.txt \n -o PG0390-CNopanel \\\n -noPanel\nwhich will produce a set of example output files.\nRecommended: Adding DEploidIBD to your bash PATH If we want to run the dEploid executable from anywhere, we need to add it to our PATH variable in bash. To do this, first make sure you are in the root directory of the DEploidIBD github repository. Then, run:\nexport PATH=$PATH:`pwd`\nNow, you should be able to run dEploid from any folder.\nAdditional resources: The DEploid manual page There are some additional instructions on how to run DEploid and the meaning of individual flags available in the github repository. First, navigate into the cloned DEploid directory on your local machine, e.g. cd DEploid. Then, run:\nman docs/_build/man/dEploid.1\n\n\n\nIn the upcoming sections of the tutorial we will make use of a command-line VCF manipulation tool called bcftools. bcftools can be installed in a variety of ways, but I would recommend doing so using conda.\nIf you have not already, install conda following the appropriate instructions here.\nNext, we will create a virtual environment for the tutorial that contains bcftools with the following command:\nconda create -n deploid -c bioconda bcftools\nFor Mac you can also install bcftools by running:\nbrew install bcftools\nOnce this is done, you should be able to activate your environment with:\nconda activate deploid\nRunning bcftools should display the help menu which includes a list of available subcommands. We are now ready to proceed with the first part of the tutorial.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/installation.html#installing-deploidibd",
    "href": "tutorials/DEploidIBD/installation.html#installing-deploidibd",
    "title": "PGEforge",
    "section": "",
    "text": "Please note that the following installation instructions were derived from those described in the github repository.\n\n\nDEploidIBD is implemented in C++ and the code needs to be compiled to your local computer hardware before it can run.\nMacOS Use the package manager Homebrew to install autoconf and other tools necessary for compilation:\nbrew install pkg-config automake autoconf autoconf-archive cppunit\nUbuntu/Debian For Ubuntu or Debian operaing systems, use the package manager apt-get to install the compilation tools:\napt-get install build-essential autoconf autoconf-archive libcppunit-dev zlib1g-dev\n\n\n\nUse git to clone the repository:\ngit clone https://github.com/DEploid-dev/DEploid.git\ncd DEploid\ngit submodule update --init --recursive --remote\n\n\n\nFinally, you can compile DEploidIBD with the following commands:\n./bootstrap\nmake\nIf the compilation has run successfully, you should be able to run the DEploidIBD executable with:\n./dEploid\nThe github repository also includes some example data and commands. For example, you can run:\n./dEploid \\\n -vcf data/testData/PG0390-C.test.vcf \\\n -plaf data/testData/labStrains.test.PLAF.txt \n -o PG0390-CNopanel \\\n -noPanel\nwhich will produce a set of example output files.\nRecommended: Adding DEploidIBD to your bash PATH If we want to run the dEploid executable from anywhere, we need to add it to our PATH variable in bash. To do this, first make sure you are in the root directory of the DEploidIBD github repository. Then, run:\nexport PATH=$PATH:`pwd`\nNow, you should be able to run dEploid from any folder.\nAdditional resources: The DEploid manual page There are some additional instructions on how to run DEploid and the meaning of individual flags available in the github repository. First, navigate into the cloned DEploid directory on your local machine, e.g. cd DEploid. Then, run:\nman docs/_build/man/dEploid.1\n\n\n\nIn the upcoming sections of the tutorial we will make use of a command-line VCF manipulation tool called bcftools. bcftools can be installed in a variety of ways, but I would recommend doing so using conda.\nIf you have not already, install conda following the appropriate instructions here.\nNext, we will create a virtual environment for the tutorial that contains bcftools with the following command:\nconda create -n deploid -c bioconda bcftools\nFor Mac you can also install bcftools by running:\nbrew install bcftools\nOnce this is done, you should be able to activate your environment with:\nconda activate deploid\nRunning bcftools should display the help menu which includes a list of available subcommands. We are now ready to proceed with the first part of the tutorial.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/part2-running-deploidibd.html",
    "href": "tutorials/DEploidIBD/part2-running-deploidibd.html",
    "title": "PGEforge",
    "section": "",
    "text": "In this section, we are going to run DEploidIBD on a single sample without using a reference panel. After this is completed, DEploidIBD will have produced an estimate of the complexity of infection (COI), the proportions of each strain, and IBD profiles for every pair of strains within the sample. Finally, we will go over the output files and what they contain.\nHere is a schematic of what we will do:\n\n\n\n\n\n\nPlease ensure you have completed both ‘Installation’ and ‘Part 1’ of the tutorial. At this point, you should have: - Installed DEploidIBD - Installed bcftools - Filtered your input VCF - Created a text file containing population level allele frequency (PLAF) estimates for each SNP from your filtered VCF\nYour directory structure should look like:\n.\n├── data -&gt; /path/to/PGEforge/data\n├── data_filtered\n│   ├── intermediates\n│   ├── pf3k.DRCongo.final.plaf.txt\n│   └── pf3k.DRCongo.final.vcf.gz\n├── results\n\n\n\nSelecting a sample\nDEploidIBD runs on only one sample from a population at a time. This means we need to subset our input VCF down to an individual sample of interest. Let’s first create a folder where we will store per-sample VCFs:\nmkdir -p data_filtered/by_sample\nNext, we will use bcftools to filter our VCF to an individual sample. Here, I have chosen the sample QG0182-C, which is simply the first sample in the metadata file located at PGEforge/data/wgs/pf3k/DRCongo/pf3k.metadata.DRCongo.csv. Let’s define some commands in bash to make our code a bit cleaner:\nTARGET_SAMPLE=QG0182-C\nSAMPLE_VCF=data_filtered/by_sample/pf3k.DRCongo.$TARGET_SAMPLE.vcf.gz\nOur input VCF should be the filtered VCF from the previous part of the tutorial:\nINPUT_VCF=data_filtered/pf3k.DRCongo.final.vcf.gz\nAnd now we can filter:\nbcftools view $INPUT_VCF -s $TARGET_SAMPLE -Oz -o $SAMPLE_VCF\nIf we run:\nbcftools stats $SAMPLE_VCF | grep ^SN\nWe should see that our VCF contains only a single sample, and the same number of records as our filtered VCF:\nSN      0       number of samples:      1\nSN      0       number of records:      6933\nSN      0       number of no-ALTs:      0\nSN      0       number of SNPs: 6933\nSN      0       number of MNPs: 0\nSN      0       number of indels:       0\nSN      0       number of others:       0\nSN      0       number of multiallelic sites:   0\nSN      0       number of multiallelic SNP sites:       0\nRunning DEploidIBD\nLet’s create an output folder to store the results from DEploidIBD:\nRESULT_DIR=results/$TARGET_SAMPLE\nmkdir -p $RESULT_DIR\nFinally, it is time to run DEploidIBD\ndEploid -vcf $SAMPLE_VCF -plaf $PLAF_TXT -o $RESULT_DIR/$TARGET_SAMPLE -k 4 -noPanel -ibd\nHere is a table explaining the flags we’ve used:\n\n\n\n\n\n\n\nFlag\nMeaning\n\n\n\n\n-vcf\nPath to input VCF file.\n\n\n-plaf\nPath to PLAF text file. See Part 1 of the tutorial for details\n\n\n-k\nThe maximum COI that will be inferred. The default value is 5, but in practice 4 is probably the limit for WGS data.\n\n\n-noPanel\nDo not use a reference panel for haplotype inference.\n\n\n-ibd\nRun the IBD model, e.g. DEploidIBD. In this model we handle within-sample IBD\n\n\n\nWith these flags, DEploidIBD should take a few minutes to run on a laptop and print some basic information to the screen. Let’s have a look:\n#########################################\n#        dEploid v0.7.1-beta log        #\n#########################################\nProgram was compiled on: Mon-11-Dec-2023-17:23:28-UTC\ndEploid version: 47533f7526ed8f7b615750969e9b008054bdc7fc\nlasso version: \n\nInput data: \n      PLAF: data_filtered/pf3k.DRCongo.final.plaf.txt\n       VCF: data_filtered/by_sample/pf3k.DRCongo.QG0182-C.vcf.gz\n\nMCMC parameters: \n        MCMC burn: 0.5\n      MCMC sample: 800\n MCMC sample rate: 5\n      Random seed: 0\n  IBD Method used: YES\n      Update Prop: NO\n    Update Single: YES\n      Update Pair: YES\n\nOther parameters:\n    Miss copy prob: 0.01\n  Avrg Cent Morgan: 15000\n                 G: 20\n         IBD sigma: 20\n     ScalingFactor: 100\n     VQSLOD:        8\n      Initial prob: 0.00194298 0.895357 7.5923e-16 0.1027\n\nMCMC diagnostic:\n     Accept_ratio: 0\n         Max_llks: -11603.3\n Final_theta_llks: -8827.36\n        Mean_llks: -8967.05\n        Stdv_llks: 25.5135\n    DIC_by_Dtheta: 18213.5\n      DIC_by_varD: 19236\n\nRun time:\n    Start at: Wed Dec 13 16:02:39 2023\n      End at: Wed Dec 13 16:08:18 2023\n\nOutput saved to:\n IBD method output saved to:\n  Likelihood: results/QG0182-C/QG0182-C.*.llk\n Proportions: results/QG0182-C/QG0182-C.*.prop\n  Haplotypes: results/QG0182-C/QG0182-C.*.hap\n   IBD probs: results/QG0182-C/QG0182-C.ibd.probs\n\n IBD probabilities:\n           0-1: 0.740756\n           1-1: 0.259244\n\n IBD best path llk: 8192.76\n\n         Effective_K: 1.2312\n          Inferred_K: 2\nAdjusted_effective_K: 1.2312\n\nProportions:\n0.00194298    0.895357  7.5923e-16      0.1027\nThe most critical estimates are at the bottom. We can see that DEploidIBD has inferred a COI, here denoted \\(K\\), of two:\n          Inferred_K: 2\nIt also reports another statistic, called the “Effective COI” (\\(K_{eff}\\)), with this sample having \\(K_{eff}=1.231\\).\n         Effective_K: 1.2312\n          Inferred_K: 2\nEffective COI has the following definition:\n\\[\nK_{eff} = \\frac{1}{\\sum_{j=1}^{K} p_{j}^2}\n\\]\nAs you can see, this statistic takes into account the proportions of the strains in the infection and produces a continuous-valued statistic rather than an interger, as is the case with COI. In particular, \\(K_{eff}\\) will be in the range of \\([1, K]\\). If the strains in the infection have balanced proportions, the \\(K_{eff}\\) value will be close to \\(K\\). On the other hand, if strains within the infection have very low proportions, these will contribute only marginally to the \\(K_{eff}\\), and as a result it can be considerably lower than the COI (\\(K\\)). In our case we can see the inferred proportions are:\nProportions:\n0.00194298    0.895357  7.5923e-16      0.1027\nThe two proportions below 0.01 are discard, so the proportions of our strains of interest are 0.89 and 0.1. This examples why our \\(K_{eff}=1.231\\) value is quite low: these proportions are relatively unbalanced.\nIf we look in the metadata file at data/wgs/pf3k/DRCongo/pf3k.metadata.DRCongo.csv we can see that sample QG0182-C has a \\(F_{ws}\\) value of 0.843. Canonically, as sample with \\(F_{ws} &lt; 0.95\\) is classified as polyclonal. So the inference from DEploidIBD of a \\(K=2\\) is consistent with the \\(F_{ws}\\) statistic for this sample.\nFinally, let’s quickly look at the IBD outputs. The average proportion of the genome in each possible IBD configuration is given here:\n IBD probabilities:\n           0-1: 0.740756\n           1-1: 0.259244\nIn this case, “1-1: 0.259” indicates that the two strains have an average relatedness of 26% across the genome. The full IBD profile is stored in the file results/QG0182-C/QG0182-C.ibd.probs. This gives, for every SNP posiiton, the probability that the sample is in each IBD configuraion. We can quickly make a plot using R:\nibd_path &lt;- \"results/QG0182-C/QG0182-C.ibd.probs\"\nibd_df &lt;- read.csv(ibd_path, sep=\"\\t\", header=F)\n\n# Unfortunately, due to a trailing tab, we have to munge a bit\ncolumns &lt;- ibd_df[1,1:4]\nibd_df &lt;- ibd_df[2:nrow(ibd_df), 1:4]\ncolnames(ibd_df) &lt;- columns\n\n# We will plot the IBD status of chromosome 1\nchrom14_df &lt;- subset(ibd_df, CHROM == 'Pf3D7_14_v3')\n\n# Plot\nplot(\n    x=chrom14_df[,\"POS\"], \n    y=chrom14_df[,\"1-1\"],\n    main=\"\",\n    xlab=\"Position (Chromosome 14)\",\n    ylab=\"IBD Probability\",\n    type='l',\n    lwd=1.5,\n    col=\"firebrick\",\n    ylim=c(0, 1)\n)\nWhich should produce the following plot:\n\n\n\nWe can see three high-probability IBD blocks within this figure, and several smaller IBD blocks with a higher degree of uncertainty.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Part 2: Running DEploidIBD"
    ]
  },
  {
    "objectID": "tutorials/DEploidIBD/part2-running-deploidibd.html#part-2-reference-panel-free-deconvolution-with-deploidibd",
    "href": "tutorials/DEploidIBD/part2-running-deploidibd.html#part-2-reference-panel-free-deconvolution-with-deploidibd",
    "title": "PGEforge",
    "section": "",
    "text": "In this section, we are going to run DEploidIBD on a single sample without using a reference panel. After this is completed, DEploidIBD will have produced an estimate of the complexity of infection (COI), the proportions of each strain, and IBD profiles for every pair of strains within the sample. Finally, we will go over the output files and what they contain.\nHere is a schematic of what we will do:\n\n\n\n\n\n\nPlease ensure you have completed both ‘Installation’ and ‘Part 1’ of the tutorial. At this point, you should have: - Installed DEploidIBD - Installed bcftools - Filtered your input VCF - Created a text file containing population level allele frequency (PLAF) estimates for each SNP from your filtered VCF\nYour directory structure should look like:\n.\n├── data -&gt; /path/to/PGEforge/data\n├── data_filtered\n│   ├── intermediates\n│   ├── pf3k.DRCongo.final.plaf.txt\n│   └── pf3k.DRCongo.final.vcf.gz\n├── results\n\n\n\nSelecting a sample\nDEploidIBD runs on only one sample from a population at a time. This means we need to subset our input VCF down to an individual sample of interest. Let’s first create a folder where we will store per-sample VCFs:\nmkdir -p data_filtered/by_sample\nNext, we will use bcftools to filter our VCF to an individual sample. Here, I have chosen the sample QG0182-C, which is simply the first sample in the metadata file located at PGEforge/data/wgs/pf3k/DRCongo/pf3k.metadata.DRCongo.csv. Let’s define some commands in bash to make our code a bit cleaner:\nTARGET_SAMPLE=QG0182-C\nSAMPLE_VCF=data_filtered/by_sample/pf3k.DRCongo.$TARGET_SAMPLE.vcf.gz\nOur input VCF should be the filtered VCF from the previous part of the tutorial:\nINPUT_VCF=data_filtered/pf3k.DRCongo.final.vcf.gz\nAnd now we can filter:\nbcftools view $INPUT_VCF -s $TARGET_SAMPLE -Oz -o $SAMPLE_VCF\nIf we run:\nbcftools stats $SAMPLE_VCF | grep ^SN\nWe should see that our VCF contains only a single sample, and the same number of records as our filtered VCF:\nSN      0       number of samples:      1\nSN      0       number of records:      6933\nSN      0       number of no-ALTs:      0\nSN      0       number of SNPs: 6933\nSN      0       number of MNPs: 0\nSN      0       number of indels:       0\nSN      0       number of others:       0\nSN      0       number of multiallelic sites:   0\nSN      0       number of multiallelic SNP sites:       0\nRunning DEploidIBD\nLet’s create an output folder to store the results from DEploidIBD:\nRESULT_DIR=results/$TARGET_SAMPLE\nmkdir -p $RESULT_DIR\nFinally, it is time to run DEploidIBD\ndEploid -vcf $SAMPLE_VCF -plaf $PLAF_TXT -o $RESULT_DIR/$TARGET_SAMPLE -k 4 -noPanel -ibd\nHere is a table explaining the flags we’ve used:\n\n\n\n\n\n\n\nFlag\nMeaning\n\n\n\n\n-vcf\nPath to input VCF file.\n\n\n-plaf\nPath to PLAF text file. See Part 1 of the tutorial for details\n\n\n-k\nThe maximum COI that will be inferred. The default value is 5, but in practice 4 is probably the limit for WGS data.\n\n\n-noPanel\nDo not use a reference panel for haplotype inference.\n\n\n-ibd\nRun the IBD model, e.g. DEploidIBD. In this model we handle within-sample IBD\n\n\n\nWith these flags, DEploidIBD should take a few minutes to run on a laptop and print some basic information to the screen. Let’s have a look:\n#########################################\n#        dEploid v0.7.1-beta log        #\n#########################################\nProgram was compiled on: Mon-11-Dec-2023-17:23:28-UTC\ndEploid version: 47533f7526ed8f7b615750969e9b008054bdc7fc\nlasso version: \n\nInput data: \n      PLAF: data_filtered/pf3k.DRCongo.final.plaf.txt\n       VCF: data_filtered/by_sample/pf3k.DRCongo.QG0182-C.vcf.gz\n\nMCMC parameters: \n        MCMC burn: 0.5\n      MCMC sample: 800\n MCMC sample rate: 5\n      Random seed: 0\n  IBD Method used: YES\n      Update Prop: NO\n    Update Single: YES\n      Update Pair: YES\n\nOther parameters:\n    Miss copy prob: 0.01\n  Avrg Cent Morgan: 15000\n                 G: 20\n         IBD sigma: 20\n     ScalingFactor: 100\n     VQSLOD:        8\n      Initial prob: 0.00194298 0.895357 7.5923e-16 0.1027\n\nMCMC diagnostic:\n     Accept_ratio: 0\n         Max_llks: -11603.3\n Final_theta_llks: -8827.36\n        Mean_llks: -8967.05\n        Stdv_llks: 25.5135\n    DIC_by_Dtheta: 18213.5\n      DIC_by_varD: 19236\n\nRun time:\n    Start at: Wed Dec 13 16:02:39 2023\n      End at: Wed Dec 13 16:08:18 2023\n\nOutput saved to:\n IBD method output saved to:\n  Likelihood: results/QG0182-C/QG0182-C.*.llk\n Proportions: results/QG0182-C/QG0182-C.*.prop\n  Haplotypes: results/QG0182-C/QG0182-C.*.hap\n   IBD probs: results/QG0182-C/QG0182-C.ibd.probs\n\n IBD probabilities:\n           0-1: 0.740756\n           1-1: 0.259244\n\n IBD best path llk: 8192.76\n\n         Effective_K: 1.2312\n          Inferred_K: 2\nAdjusted_effective_K: 1.2312\n\nProportions:\n0.00194298    0.895357  7.5923e-16      0.1027\nThe most critical estimates are at the bottom. We can see that DEploidIBD has inferred a COI, here denoted \\(K\\), of two:\n          Inferred_K: 2\nIt also reports another statistic, called the “Effective COI” (\\(K_{eff}\\)), with this sample having \\(K_{eff}=1.231\\).\n         Effective_K: 1.2312\n          Inferred_K: 2\nEffective COI has the following definition:\n\\[\nK_{eff} = \\frac{1}{\\sum_{j=1}^{K} p_{j}^2}\n\\]\nAs you can see, this statistic takes into account the proportions of the strains in the infection and produces a continuous-valued statistic rather than an interger, as is the case with COI. In particular, \\(K_{eff}\\) will be in the range of \\([1, K]\\). If the strains in the infection have balanced proportions, the \\(K_{eff}\\) value will be close to \\(K\\). On the other hand, if strains within the infection have very low proportions, these will contribute only marginally to the \\(K_{eff}\\), and as a result it can be considerably lower than the COI (\\(K\\)). In our case we can see the inferred proportions are:\nProportions:\n0.00194298    0.895357  7.5923e-16      0.1027\nThe two proportions below 0.01 are discard, so the proportions of our strains of interest are 0.89 and 0.1. This examples why our \\(K_{eff}=1.231\\) value is quite low: these proportions are relatively unbalanced.\nIf we look in the metadata file at data/wgs/pf3k/DRCongo/pf3k.metadata.DRCongo.csv we can see that sample QG0182-C has a \\(F_{ws}\\) value of 0.843. Canonically, as sample with \\(F_{ws} &lt; 0.95\\) is classified as polyclonal. So the inference from DEploidIBD of a \\(K=2\\) is consistent with the \\(F_{ws}\\) statistic for this sample.\nFinally, let’s quickly look at the IBD outputs. The average proportion of the genome in each possible IBD configuration is given here:\n IBD probabilities:\n           0-1: 0.740756\n           1-1: 0.259244\nIn this case, “1-1: 0.259” indicates that the two strains have an average relatedness of 26% across the genome. The full IBD profile is stored in the file results/QG0182-C/QG0182-C.ibd.probs. This gives, for every SNP posiiton, the probability that the sample is in each IBD configuraion. We can quickly make a plot using R:\nibd_path &lt;- \"results/QG0182-C/QG0182-C.ibd.probs\"\nibd_df &lt;- read.csv(ibd_path, sep=\"\\t\", header=F)\n\n# Unfortunately, due to a trailing tab, we have to munge a bit\ncolumns &lt;- ibd_df[1,1:4]\nibd_df &lt;- ibd_df[2:nrow(ibd_df), 1:4]\ncolnames(ibd_df) &lt;- columns\n\n# We will plot the IBD status of chromosome 1\nchrom14_df &lt;- subset(ibd_df, CHROM == 'Pf3D7_14_v3')\n\n# Plot\nplot(\n    x=chrom14_df[,\"POS\"], \n    y=chrom14_df[,\"1-1\"],\n    main=\"\",\n    xlab=\"Position (Chromosome 14)\",\n    ylab=\"IBD Probability\",\n    type='l',\n    lwd=1.5,\n    col=\"firebrick\",\n    ylim=c(0, 1)\n)\nWhich should produce the following plot:\n\n\n\nWe can see three high-probability IBD blocks within this figure, and several smaller IBD blocks with a higher degree of uncertainty.",
    "crumbs": [
      "Tool resources",
      "DEploidIBD",
      "Part 2: Running DEploidIBD"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/analysis.html",
    "href": "tutorials/SNP-slice/analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/analysis.html#the-data",
    "href": "tutorials/SNP-slice/analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/analysis.html#first-analysis-section",
    "href": "tutorials/SNP-slice/analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages.",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/analysis.html#summary",
    "href": "tutorials/SNP-slice/analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial.",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/installation.html",
    "href": "tutorials/SNP-slice/installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/SNP-slice/installation.html#step-n-what-to-do",
    "href": "tutorials/SNP-slice/installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "SNP-slice",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/BRO/Template_background.html#summary-sheet",
    "href": "tutorials/BRO/Template_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nTODO\n\n\nAuthors\nTODO\n\n\nLatest version\nTODO\n\n\nLicense\nTODO\n\n\nWebsite\nhttps://www.google.com\n\n\nCode repository\nhttps://www.google.com\n\n\nPublication\nhttps://www.google.com\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO"
  },
  {
    "objectID": "tutorials/BRO/Template_background.html#purpose",
    "href": "tutorials/BRO/Template_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nA quick one paragraph description of what the tool does. For an example, see the DRpower page."
  },
  {
    "objectID": "tutorials/BRO/Template_background.html#existing-resources",
    "href": "tutorials/BRO/Template_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/BRO/Template_background.html#citation",
    "href": "tutorials/BRO/Template_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/hierfstat/analysis.html",
    "href": "tutorials/hierfstat/analysis.html",
    "title": "Analysis tutorial",
    "section": "",
    "text": "First we will load the libraries we will need:\nlibrary(hierfstat)\nlibrary(vcfR)\nWe’re going to be using WGS data from Vietnam which was generated for the Pf3k Project. First load in the metadata containing the sample names and information on where the samples were collected.\nmeta&lt;-read.csv(\"../../data/wgs/pf3k/Vietnam/pf3k.metadata.Vietnam.csv\")\nThen we can load in the genetic data from VCF format.\nvcf&lt;-read.vcfR(\"../../data/wgs/pf3k/Vietnam/SNP_INDEL_Pf3D7_ALL_v3.combined.filtered.vqslod6.biallelic_snp.Vietnam.vcf.gz\")\nNext we extract the genotypes and convert them into a dosage format.\nvcf_gts&lt;-extract.gt(vcf)\n\nrecode_gt&lt;-function(x){\n  x&lt;-gsub(\"[|]\",\"/\",x)\n  x&lt;-gsub(\"0/0\",0,x)\n  x&lt;-gsub(\"0/1\",1,x)\n  x&lt;-gsub(\"1/1\",2,x)\n  as.numeric(x)\n}\n\nraw_gts&lt;-apply(vcf_gts,MARGIN = 2,function(x){recode_gt(x)})\nNow we can remove loci which are monomorphic.\ncalculate_af&lt;-function(x){\n  sum(x,na.rm = T)/length(x)*2\n}\naf &lt;- apply(raw_gts,MARGIN = 1,calculate_af)\ntransposed_gts&lt;-as.data.frame(t(raw_gts[which(af&gt;0 & af&lt;1),]))\nWe now have a matrix where the samples are rows and the columns are loci. Hierfstat requires you to also provide population assignments for each sample by adding it to the data as the first column.\ndat&lt;-cbind(meta$site,transposed_gts)\nNow we are ready to calculate some statistics!\nresults&lt;-basic.stats(dat)\nresults$overall\n\n     Ho      Hs      Ht     Dst     Htp    Dstp     Fst    Fstp     Fis    Dest \n 0.0674  0.0635  0.0639  0.0003  0.0640  0.0005  0.0052  0.0078 -0.0601  0.0005\nThe results$overall table contains basic statistics averaged over loci. The statistics presented are defined in eq.7.38– 7.43 pp.164–5 of Nei (1987).",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Analyze data with `hierfstat`"
    ]
  },
  {
    "objectID": "tutorials/hierfstat/analysis.html#summary",
    "href": "tutorials/hierfstat/analysis.html#summary",
    "title": "Analysis tutorial",
    "section": "Summary",
    "text": "Summary\nWe loaded in data from VCF format, converted this to dosage and finally calculated basic statistics.",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Analyze data with `hierfstat`"
    ]
  },
  {
    "objectID": "tutorials/hierfstat/installation.html",
    "href": "tutorials/hierfstat/installation.html",
    "title": "Installing hierfstat",
    "section": "",
    "text": "We can easily install hierfstat from the plasmogenepi repository\n\nif(!require(\"hierfstat\"))  install.packages('hierfstat', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\nWe will also need vcfR to load some data. Install this with:\n\nif(!require(\"vcfR\"))  install.packages(\"vcfR\")\n\nOnce this finishes, check that the library loads ok\n\nlibrary(hierfstat)\n\n\n\n\n Back to top",
    "crumbs": [
      "Tool resources",
      "hierfstat",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/isoRelate/Template_background.html#summary-sheet",
    "href": "tutorials/isoRelate/Template_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nTODO\n\n\nAuthors\nTODO\n\n\nLatest version\nTODO\n\n\nLicense\nTODO\n\n\nWebsite\nhttps://www.google.com\n\n\nCode repository\nhttps://www.google.com\n\n\nPublication\nhttps://www.google.com\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO"
  },
  {
    "objectID": "tutorials/isoRelate/Template_background.html#purpose",
    "href": "tutorials/isoRelate/Template_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nA quick one paragraph description of what the tool does. For an example, see the DRpower page."
  },
  {
    "objectID": "tutorials/isoRelate/Template_background.html#existing-resources",
    "href": "tutorials/isoRelate/Template_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/isoRelate/Template_background.html#citation",
    "href": "tutorials/isoRelate/Template_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/DRpower/DRpower_installation.html",
    "href": "tutorials/DRpower/DRpower_installation.html",
    "title": "Installing DRpower",
    "section": "",
    "text": "DRpower is an R package, but it uses C++ code under the hood via the Rcpp package to make calculations faster. When you load the package for the first time, these C++ files must be compiled on your local machine. This requires you to have a C++ compiler installed.\nExpand the arrow to see instructions for your operating system:\n\n\nWindows\n\n\nInstall Rtools, which contains a series of tools for building R packages.\nOnce you have Rtools installed you should be able to compile.\n\n\n\n\nMac\n\n\nOpen a Terminal window. You can do this by searching (keyboard shortcut command+space) for “Terminal”.\nInside the terminal window, type the command xcode-select --install. This will prompt you to install Xcode command line tools.\nOnce you have Xcode command line tools installed you should be able to compile.\n\n\n\n\nLinux (Ubuntu)\n\n\nInstalling on Linux is a bit more tricky. Start by installing gcc.\nOpen a Terminal window.\nExecute the command sudo apt-get install r-base-dev.\nYou should now be able to compile.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_installation.html#step-1-install-a-c-compiler",
    "href": "tutorials/DRpower/DRpower_installation.html#step-1-install-a-c-compiler",
    "title": "Installing DRpower",
    "section": "",
    "text": "DRpower is an R package, but it uses C++ code under the hood via the Rcpp package to make calculations faster. When you load the package for the first time, these C++ files must be compiled on your local machine. This requires you to have a C++ compiler installed.\nExpand the arrow to see instructions for your operating system:\n\n\nWindows\n\n\nInstall Rtools, which contains a series of tools for building R packages.\nOnce you have Rtools installed you should be able to compile.\n\n\n\n\nMac\n\n\nOpen a Terminal window. You can do this by searching (keyboard shortcut command+space) for “Terminal”.\nInside the terminal window, type the command xcode-select --install. This will prompt you to install Xcode command line tools.\nOnce you have Xcode command line tools installed you should be able to compile.\n\n\n\n\nLinux (Ubuntu)\n\n\nInstalling on Linux is a bit more tricky. Start by installing gcc.\nOpen a Terminal window.\nExecute the command sudo apt-get install r-base-dev.\nYou should now be able to compile.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_installation.html#step-2-install-and-load-the-r-package",
    "href": "tutorials/DRpower/DRpower_installation.html#step-2-install-and-load-the-r-package",
    "title": "Installing DRpower",
    "section": "Step 2: install and load the R package",
    "text": "Step 2: install and load the R package\nIn R, ensure that you have the devtools package installed by running\n\ninstall.packages(\"devtools\", repos = 'http://cran.us.r-project.org')\n\nYou can then install the DRpower package directly from GitHub by running\n\ndevtools::install_github(\"mrc-ide/DRpower\", ref = \"v1.0.2\")\n\nAssuming everything installed correctly, we need to load the package:\n\nlibrary(DRpower)\n\nYou can test that the package is loaded and working by running the following command, which should produce this output:\n\ncheck_DRpower_loaded()\n\nDRpower version 1.0.2 loaded successfully!",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_analysis_pfhrp2.html",
    "href": "tutorials/DRpower/DRpower_analysis_pfhrp2.html",
    "title": "Analysis of pfhrp2/3 data",
    "section": "",
    "text": "The data for this analysis come from a study by Feleke et al. (2021). See the description in the Data section for more details on this dataset. In short, the data come from a prospective, cross-sectional survey conducted in northern and western Ethiopia between 2017-2018. False-negative samples by HRP2-based RDT were sent for genomic sequencing to confirm pfhrp2/3 deletions. We are interested in the prevalence of deletions at the province level, and in particular comparing this prevalence against the WHO Master Protocol threshold of 5%.\nWe start by loading packages:\n\nlibrary(tidyverse)\nlibrary(DRpower)\nlibrary(here)\n\nAfter loading the data from the PGEforge data folder, we will focus our attention on the Tigray province:\n\n# import data from PGEforge and filter to Tigray region\ndf_Tigray &lt;- read.csv(here(\"data/pfhrp2-3_counts\", \"Feleke_pfhrp2.csv\")) |&gt;\n  filter(ADMIN1 == \"Tigray\")\n\n# display the table\ndf_Tigray |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nCountry\nADMIN1\nSite\nLongitude\nLatitude\nYear_start\nYear_end\nNum_pfhrp2_tested\nNum_pfhrp2_deleted\n\n\n\n\nEthiopia\nTigray\nAhferom\nNA\nNA\n2017\n2018\n117\n18\n\n\nEthiopia\nTigray\nAtseged Tsimbila\nNA\nNA\n2017\n2018\n160\n48\n\n\nEthiopia\nTigray\nGulomekeda\nNA\nNA\n2017\n2018\n21\n5\n\n\nEthiopia\nTigray\nK. Humera\nNA\nNA\n2017\n2018\n176\n39\n\n\nEthiopia\nTigray\nL. Adiabo\nNA\nNA\n2017\n2018\n145\n22\n\n\nEthiopia\nTigray\nT. Adiabo\nNA\nNA\n2017\n2018\n69\n10\n\n\n\n\n\nAlthough we have a lot of information here, we only really need the final two columns of this data.frame - the number tested and the number of pfhrp2 deletions found.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Analysis of *pfhrp2/3* data"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#the-data",
    "href": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#the-data",
    "title": "Analysis of pfhrp2/3 data",
    "section": "",
    "text": "The data for this analysis come from a study by Feleke et al. (2021). See the description in the Data section for more details on this dataset. In short, the data come from a prospective, cross-sectional survey conducted in northern and western Ethiopia between 2017-2018. False-negative samples by HRP2-based RDT were sent for genomic sequencing to confirm pfhrp2/3 deletions. We are interested in the prevalence of deletions at the province level, and in particular comparing this prevalence against the WHO Master Protocol threshold of 5%.\nWe start by loading packages:\n\nlibrary(tidyverse)\nlibrary(DRpower)\nlibrary(here)\n\nAfter loading the data from the PGEforge data folder, we will focus our attention on the Tigray province:\n\n# import data from PGEforge and filter to Tigray region\ndf_Tigray &lt;- read.csv(here(\"data/pfhrp2-3_counts\", \"Feleke_pfhrp2.csv\")) |&gt;\n  filter(ADMIN1 == \"Tigray\")\n\n# display the table\ndf_Tigray |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nCountry\nADMIN1\nSite\nLongitude\nLatitude\nYear_start\nYear_end\nNum_pfhrp2_tested\nNum_pfhrp2_deleted\n\n\n\n\nEthiopia\nTigray\nAhferom\nNA\nNA\n2017\n2018\n117\n18\n\n\nEthiopia\nTigray\nAtseged Tsimbila\nNA\nNA\n2017\n2018\n160\n48\n\n\nEthiopia\nTigray\nGulomekeda\nNA\nNA\n2017\n2018\n21\n5\n\n\nEthiopia\nTigray\nK. Humera\nNA\nNA\n2017\n2018\n176\n39\n\n\nEthiopia\nTigray\nL. Adiabo\nNA\nNA\n2017\n2018\n145\n22\n\n\nEthiopia\nTigray\nT. Adiabo\nNA\nNA\n2017\n2018\n69\n10\n\n\n\n\n\nAlthough we have a lot of information here, we only really need the final two columns of this data.frame - the number tested and the number of pfhrp2 deletions found.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Analysis of *pfhrp2/3* data"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#estimating-the-prevalence",
    "href": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#estimating-the-prevalence",
    "title": "Analysis of pfhrp2/3 data",
    "section": "Estimating the prevalence",
    "text": "Estimating the prevalence\nNine times out of ten, the thing we’re interested in is the prevalence of pfhrp2/3 deletions. We can estimate this using the get_prevalence() function, using values from the data.frame above:\n\n# estimate the prevalence of deletions\nget_prevalence(n = df_Tigray$Num_pfhrp2_deleted,\n               N = df_Tigray$Num_pfhrp2_tested)\n\n   MAP CrI_lower CrI_upper prob_above_threshold\n1 20.4     14.11     28.93                    1\n\n\nOur point estimate of the prevalence is 20.4%, and our Bayesian 95% credible interval (CrI) is from 14.11% to 28.93%. Here, we have used the maximum a posteriori (MAP) estimate as out point estimate, but other options are available (see ?get_prevalence).\nWe can also produce a simple plot of the posterior distribution of the prevalence:\n\n# plot the posterior distribution of the prevalence\nplot_prevalence(n = df_Tigray$Num_pfhrp2_deleted,\n                N = df_Tigray$Num_pfhrp2_tested)\n\n\n\n\n\n\n\n\nNotice that both the table output and the plot give a probability close to 100% of being above the 5% threshold. Based on these results we would be justified in switching RDTs.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Analysis of *pfhrp2/3* data"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#estimating-the-intra-cluster-correlation",
    "href": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#estimating-the-intra-cluster-correlation",
    "title": "Analysis of pfhrp2/3 data",
    "section": "Estimating the intra-cluster correlation",
    "text": "Estimating the intra-cluster correlation\nA second quantity that we might be interested in is the level of intra-cluster correlation. This tells us whether the prevalence is consistent between sites (low ICC) or highly variable between sites (high ICC). A high ICC can be caused by many factors, including a patchy geographic distribution in the prevalence of deletions within the province. Not only is this interesting information in itself, it also provides extremely useful context for future studies in the same region or neighbouring regions.\nWe can estimate the ICC using the get_ICC() function:\n\n# estimate the ICC\nget_ICC(n = df_Tigray$Num_pfhrp2_deleted,\n        N = df_Tigray$Num_pfhrp2_tested)\n\n     MAP CrI_lower CrI_upper\n1 0.0151         0    0.1031\n\n\nOur point estimate of the ICC is around 0.015. This is below the 0.05 value that we typically assume based on analysis of historical data, which may imply that the prevalence of pfhrp2 deletions is relatively flat in the Tigray province. However, we can also see that the 95% CrI ranges from 0 to 0.103, and so we do not have very strong information here.\nAs with prevalence, we can produce a simple plot of the posterior distribution of ICC:\n\n# plot the posterior distribution of the ICC\nplot_ICC(n = df_Tigray$Num_pfhrp2_deleted,\n         N = df_Tigray$Num_pfhrp2_tested)\n\n\n\n\n\n\n\n\nFinally, if we really want to understand how prevalence and ICC relate to each other, we can plot the joint distribution of both parameters. Here, we have set the breaks in both the x- and y-dimensions to focus in on the region of interest:\n\n# joint plot of prevalence and ICC\nplot_joint(n = df_Tigray$Num_pfhrp2_deleted,\n           N = df_Tigray$Num_pfhrp2_tested,\n           prev_breaks = seq(0.1, 0.3, l = 101),\n           ICC_breaks = seq(0, 0.1, l = 101))\n\n\n\n\n\n\n\n\nThis shows that there are no strong correlations between parameters, but honestly this is probably more information than we need most of the time.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Analysis of *pfhrp2/3* data"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#retrospective-power-analysis",
    "href": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#retrospective-power-analysis",
    "title": "Analysis of pfhrp2/3 data",
    "section": "Retrospective power analysis",
    "text": "Retrospective power analysis\nAn interesting question is: given the final sample sizes in the Tigray region, what was our power? This sort of retrospective power analysis can be useful to evaluate the strength of evidence of a study. However, it is obviously far better to perform a power analysis before conducting the study!\nHere, we take the standard approach of assuming the prevalence of deletions is 10% in the region:\n\n# estimate power using the actual sample sizes\nget_power_threshold(N = df_Tigray$Num_pfhrp2_tested, prevalence = 0.1, reps = 1e3)\n\n  prev_thresh power lower upper\n1        0.05  79.2 76.55 81.68\n\n\nWe find that power was around 79%, which is definitely adequate.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Analysis of *pfhrp2/3* data"
    ]
  },
  {
    "objectID": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#summary",
    "href": "tutorials/DRpower/DRpower_analysis_pfhrp2.html#summary",
    "title": "Analysis of pfhrp2/3 data",
    "section": "Summary",
    "text": "Summary\nIn summary, we estimate from the Feleke et al. (2021) data that the prevalence of pfhrp2 deletions in the Tigray region of Ethiopia was 20.4% (CrI 14.11%-28.93%) at the time of the study (2017-2018). We are highly confident that this prevalence is above the WHO threshold of 5%. We find evidence that the intra-cluster correlation may be low in this region (0.015, CrI 0.00-0.103), implying that the prevalence of pfhrp2 deletions may be relatively flat. Finally, in a retrospective power analysis we identify that this was a well-powered study, giving us increased confidence in the conclusions.",
    "crumbs": [
      "Tool resources",
      "DRpower",
      "Analysis of *pfhrp2/3* data"
    ]
  },
  {
    "objectID": "tutorials/MALECOT/Template_analysis.html",
    "href": "tutorials/MALECOT/Template_analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/MALECOT/Template_analysis.html#the-data",
    "href": "tutorials/MALECOT/Template_analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/MALECOT/Template_analysis.html#first-analysis-section",
    "href": "tutorials/MALECOT/Template_analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages."
  },
  {
    "objectID": "tutorials/MALECOT/Template_analysis.html#summary",
    "href": "tutorials/MALECOT/Template_analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial."
  },
  {
    "objectID": "tutorials/MALECOT/Template_installation.html",
    "href": "tutorials/MALECOT/Template_installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/MALECOT/Template_installation.html#step-n-what-to-do",
    "href": "tutorials/MALECOT/Template_installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/PGEcore/naive_single_AP/background.html",
    "href": "tutorials/PGEcore/naive_single_AP/background.html",
    "title": "Estimate naive allele prevalence",
    "section": "",
    "text": "This tool provides a naive implementation of estimating allele prevalence from amino acid calls. Prevalence is estimated by calculating the proportion of samples (with at least one call) that have a given variant at a given position.",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive single-locus allele prev",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_single_AP/background.html#tool-information",
    "href": "tutorials/PGEcore/naive_single_AP/background.html#tool-information",
    "title": "Estimate naive allele prevalence",
    "section": "",
    "text": "This tool provides a naive implementation of estimating allele prevalence from amino acid calls. Prevalence is estimated by calculating the proportion of samples (with at least one call) that have a given variant at a given position.",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive single-locus allele prev",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/naive_single_AP/background.html#script-usage",
    "href": "tutorials/PGEcore/naive_single_AP/background.html#script-usage",
    "title": "Estimate naive allele prevalence",
    "section": "Script Usage",
    "text": "Script Usage\n\nRscript estimate_allele_prevalence_naive.R \\\n  --aa_calls data/example_amino_acid_calls.tsv \\\n  --output prevalence.tsv",
    "crumbs": [
      "Tool resources",
      "PGEcore",
      "Naive single-locus allele prev",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/background.html#summary-sheet",
    "href": "tutorials/PGEcore/background.html#summary-sheet",
    "title": "PGEcore",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nBespoke code to perform analysis functionalities not currently implemented in available analysis tools\n\n\nAuthors\nPlasmoGenEpi\n\n\nLatest version\n\n\n\nLicense\n\n\n\nWebsite\nhttps://github.com/PlasmoGenEpi/PGEcore/\n\n\nCode repository\nhttps://github.com/PlasmoGenEpi/PGEcore\n\n\nPublication\n\n\n\nTutorial authors\n\n\n\nTutorial date",
    "crumbs": [
      "Tool resources",
      "PGEcore"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/background.html#purpose",
    "href": "tutorials/PGEcore/background.html#purpose",
    "title": "PGEcore",
    "section": "Purpose",
    "text": "Purpose\nThe PGEcore bespoke scripts perform functionalities not currently fulfilled by any available tool.",
    "crumbs": [
      "Tool resources",
      "PGEcore"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/background.html#existing-resources",
    "href": "tutorials/PGEcore/background.html#existing-resources",
    "title": "PGEcore",
    "section": "Existing resources",
    "text": "Existing resources\nThe PGEcore GitHub repository includes more details on each bespoke script, including usage.\nSee:\n\nEstimate naive COI\nEstimate naive single-locus allele frequency\nEstimate naive single-locus allele prevalence\nEstimate naive multi-locus genotype frequency and prevalence\n\n(note to self: these links will be broken until PGEcore merged to main, but should be ready to go once PGEcore is live)",
    "crumbs": [
      "Tool resources",
      "PGEcore"
    ]
  },
  {
    "objectID": "tutorials/PGEcore/background.html#citation",
    "href": "tutorials/PGEcore/background.html#citation",
    "title": "PGEcore",
    "section": "Citation",
    "text": "Citation",
    "crumbs": [
      "Tool resources",
      "PGEcore"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_installation.html",
    "href": "tutorials/coiaf/coiaf_installation.html",
    "title": "Installing coiaf",
    "section": "",
    "text": "To install the package, run the following code:\n\ninstall.packages('coiaf', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_installation.html#installation",
    "href": "tutorials/coiaf/coiaf_installation.html#installation",
    "title": "Installing coiaf",
    "section": "",
    "text": "To install the package, run the following code:\n\ninstall.packages('coiaf', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_background.html#summary-sheet",
    "href": "tutorials/coiaf/coiaf_background.html#summary-sheet",
    "title": "coiaf",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nMOI estimation\n\n\nAuthors\nAris Paschalidis, OJ Watson\n\n\nLatest version\n0.1.2\n\n\nLicense\nMIT\n\n\nWebsite\nbailey-lab.github.io/coiaf\n\n\nCode repository\nhttps://github.com/bailey-lab/coiaf/\n\n\nPublication\nhttps://doi.org/10.1371/journal.pcbi.1010247\n\n\nTutorial authors\nMaxwell Murphy\n\n\nTutorial date\n11-Dec-2023",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_background.html#purpose",
    "href": "tutorials/coiaf/coiaf_background.html#purpose",
    "title": "coiaf",
    "section": "Purpose",
    "text": "Purpose\ncoiaf is a package for estimating complexity of infection (COI) from bi-allelic SNP data. In comparison to other available methods, coiaf is fast, accurate, and can be used to estimate COI for a large number of samples.",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_background.html#existing-resources",
    "href": "tutorials/coiaf/coiaf_background.html#existing-resources",
    "title": "coiaf",
    "section": "Existing resources",
    "text": "Existing resources\nThe coiaf website contains a detailed tutorial on how to use the package, as well as a vignette with worked examples.\nThe paper describing the package is available here.",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/coiaf/coiaf_background.html#citation",
    "href": "tutorials/coiaf/coiaf_background.html#citation",
    "title": "coiaf",
    "section": "Citation",
    "text": "Citation\n\n @article{\n  Paschalidis_Watson_Aydemir_Verity_Bailey_2023, \n  title={coiaf: Directly estimating complexity of infection with allele frequencies}, \n  volume={19},\n  ISSN={1553-7358},\n  DOI={10.1371/journal.pcbi.1010247},\n  number={6},\n  journal={PLOS Computational Biology},\n  publisher={Public Library of Science},\n  author={Paschalidis, Aris and Watson, Oliver J. and Aydemir, Ozkan and Verity, Robert and Bailey, Jeffrey A.},\n  year={2023},\n  month=jun,\n  pages={e1010247},\n  language={en} \n}",
    "crumbs": [
      "Tool resources",
      "coiaf",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/moimix/moimix_background.html#summary-sheet",
    "href": "tutorials/moimix/moimix_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nWithin host allele frequency, identify polyclonal infections\n\n\nAuthors\nStuart Lee\n\n\nLatest version\n0.0.2.9001\n\n\nWebsite\nhttps://bahlolab.github.io/moimix/\n\n\nCode repository\nhttps://github.com/bahlolab/moimix\n\n\nPublication\nNone\n\n\nTutorial authors\nSophie Bérubé\n\n\nTutorial date\n13/12/2023"
  },
  {
    "objectID": "tutorials/moimix/moimix_background.html#purpose",
    "href": "tutorials/moimix/moimix_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nThe moimix package uses biallelic SNP data output in the form of a VCF file from massively parallel sequencing platforms to estimate the following:\n\nwithin host allele frequencies (including minor allele frequencies)\nwhether an infection is monclonal or not"
  },
  {
    "objectID": "tutorials/moimix/moimix_background.html#existing-resources",
    "href": "tutorials/moimix/moimix_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nThe moimix website gives general information about data inputs, features of the package, and installation. -The moimix vignette describes individual functions in the package, and shows a step-by-step tutorial.\nThe moimix package uses functions from the flexmix and SeqArray packages.\n\nLee S, Harrison A, Tessier N, Tavul L, Miotto O, Siba P, Kwiatkowski D, Müller I, Barry AE and Bahlo M, Assessing clonality in malaria parasites using massively parallel sequencing data, 2016, in preparation."
  },
  {
    "objectID": "tutorials/moimix/moimix_background.html#citation",
    "href": "tutorials/moimix/moimix_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\n@Manual{,\n    title = {moimix: Estimating mulitplicity of infection from high-throughput\nsequencing data},\n    author = {\"Stuart Lee\"},\n    year = {2023},\n    note = {R package version 0.0.2.9001},\n    url = {https://github.com/bahlolab/moimix},\n  }"
  },
  {
    "objectID": "tutorials/FreqEstimationModel/background.html#summary-sheet",
    "href": "tutorials/FreqEstimationModel/background.html#summary-sheet",
    "title": "FreqEstimationModel",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nFrequency/prevalence estimation of phased genotypes\n\n\nAuthors\nAimee Taylor\n\n\nLatest version\n\n\n\nLicense\nMIT\n\n\nWebsite\nNA\n\n\nCode repository\nhttps://github.com/aimeertaylor/FreqEstimationModel\n\n\nPublication\nhttps://doi.org/10.1186/1475-2875-13-102\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/background.html#purpose",
    "href": "tutorials/FreqEstimationModel/background.html#purpose",
    "title": "FreqEstimationModel",
    "section": "Purpose",
    "text": "Purpose\nThe FreqEstimationModel R package estimates population-level frequencies of genetic markers of antimalarial resistance where\n\ngenetic markers of antimalarial resistance are either alleles at single biallelic SNPs or sequences of alleles (haplotypes) over a small number of biallelic SNPs (at most seven) in the malaria parasite genome;\npopulation-level parasite frequencies (vs within-infection parasite frequencies) are estimated by jointly modelling parasite genetic data on many infections, including those that contain genetically distinct parasites (i.e., those that are polyclonal);\nparasite genetic data are prevalence data (i.e., they describe the presence of alleles). A non-default version of the model does exploit information on read-depths (see note on chapter 6 below), but it has not been peer-reviewed.",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/background.html#existing-resources",
    "href": "tutorials/FreqEstimationModel/background.html#existing-resources",
    "title": "FreqEstimationModel",
    "section": "Existing resources",
    "text": "Existing resources\nThe FreqEstimationModel GitHub repository includes scripts for implementing FreqEstimationModel as well as vignettes demonstrating usage.",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/FreqEstimationModel/background.html#citation",
    "href": "tutorials/FreqEstimationModel/background.html#citation",
    "title": "FreqEstimationModel",
    "section": "Citation",
    "text": "Citation\nThe publications associated with the FreqEstimationModel can be found at:\n\nTaylor et al 2014 Malaria Journal\nTaylor et al 2016 Open Forum Infectious Diseases",
    "crumbs": [
      "Tool resources",
      "FreqEstimationModel",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/IDM/analysis.html",
    "href": "tutorials/IDM/analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/IDM/analysis.html#the-data",
    "href": "tutorials/IDM/analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/IDM/analysis.html#first-analysis-section",
    "href": "tutorials/IDM/analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages.",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/IDM/analysis.html#summary",
    "href": "tutorials/IDM/analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial.",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Analyze data with `SNP-slice`"
    ]
  },
  {
    "objectID": "tutorials/IDM/installation.html",
    "href": "tutorials/IDM/installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/IDM/installation.html#step-n-what-to-do",
    "href": "tutorials/IDM/installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "Incomplete data model (IDM)",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/pixelate/Template_background.html#summary-sheet",
    "href": "tutorials/pixelate/Template_background.html#summary-sheet",
    "title": "Insert tool title",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nTODO\n\n\nAuthors\nTODO\n\n\nLatest version\nTODO\n\n\nLicense\nTODO\n\n\nWebsite\nhttps://www.google.com\n\n\nCode repository\nhttps://www.google.com\n\n\nPublication\nhttps://www.google.com\n\n\nTutorial authors\nTODO\n\n\nTutorial date\nTODO"
  },
  {
    "objectID": "tutorials/pixelate/Template_background.html#purpose",
    "href": "tutorials/pixelate/Template_background.html#purpose",
    "title": "Insert tool title",
    "section": "Purpose",
    "text": "Purpose\nA quick one paragraph description of what the tool does. For an example, see the DRpower page."
  },
  {
    "objectID": "tutorials/pixelate/Template_background.html#existing-resources",
    "href": "tutorials/pixelate/Template_background.html#existing-resources",
    "title": "Insert tool title",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/pixelate/Template_background.html#citation",
    "href": "tutorials/pixelate/Template_background.html#citation",
    "title": "Insert tool title",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/hmmibdr/hmmibdr_background.html#summary-sheet",
    "href": "tutorials/hmmibdr/hmmibdr_background.html#summary-sheet",
    "title": "hmmibdr",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nRcpp wrapper for hmmIBD\n\n\nAuthors\nOJ Watson\n\n\nLatest version\n0.2.0\n\n\nLicense\nOJ Watson 2019\n\n\nWebsite\nNA\n\n\nCode repository\nhttps://github.com/OJWatson/hmmibdr/tree/main\n\n\nPublication\nNA\n\n\nTutorial authors\nSophie Berube\n\n\nTutorial date\n13-12-2023"
  },
  {
    "objectID": "tutorials/hmmibdr/hmmibdr_background.html#purpose",
    "href": "tutorials/hmmibdr/hmmibdr_background.html#purpose",
    "title": "hmmibdr",
    "section": "Purpose",
    "text": "Purpose\nThis package is an Rcpp wrapper for the hmmibd software. Currently, there are several c and python scripts (within the hmmibd software) required to convert files from VCF format to a text format that can then be used by either hmmibd or hmmibdr to perform further analysis. However, these scripts require the user to run c and python code; these are the same requirements as those for performing analysis using only the hmmibd software . Therefore, we recommend the use of the hmmibd software directly for the entire analysis.\nPlease refer to the hmmibd tutorial for a complete demonstration of the tool."
  },
  {
    "objectID": "tutorials/hmmibdr/hmmibdr_background.html#existing-resources",
    "href": "tutorials/hmmibdr/hmmibdr_background.html#existing-resources",
    "title": "hmmibdr",
    "section": "Existing resources",
    "text": "Existing resources\n\nA short tutorial for hmmibdr is located on the Github Page\nSee the hmmibd for further information."
  },
  {
    "objectID": "tutorials/hmmibdr/hmmibdr_background.html#citation",
    "href": "tutorials/hmmibdr/hmmibdr_background.html#citation",
    "title": "hmmibdr",
    "section": "Citation",
    "text": "Citation\n@Manual{,\n    title = {hmmibdr: HMM Identity by Descent},\n    author = {OJ Watson},\n    year = {2023},\n    note = {R package version 0.2.0},\n  }"
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "",
    "text": "Data is generated by simulating sanger 100 barcode data for 100 samples with simulated COIs. main Data tab for further details.",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#the-data",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#the-data",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "",
    "text": "Data is generated by simulating sanger 100 barcode data for 100 samples with simulated COIs. main Data tab for further details.",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#r-packages",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#r-packages",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "R packages",
    "text": "R packages\nInstall R package vcfR for reading in vcf data.\n\ninstall.package(\"vcfR\")\n\n\nlibrary(tidyverse)\nlibrary(vcfR)",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#converting-from-vcf-to-input-data",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#converting-from-vcf-to-input-data",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "Converting from VCF to input data",
    "text": "Converting from VCF to input data\nThe input data to MultiLociBiallelicModel requires a table with sample ID in first column and then a column for each marker present with a 0 == homozygous reference, 1 == homozygous alt, and 2 for heterozygote. This only works on biallelic SNPS and therefore should be limited to only these SNPs.\n\n# read in vcf \nSpotMalariapfPanel_simData_sanger100 = read.vcfR(\"../../data/snp_barcode/SpotMalariapfPanel_simData_sanger100.vcf.gz\")\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 76\n  header_line: 77\n  variant count: 100\n  column count: 109\n\nMeta line 76 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 100\n  Character matrix gt cols: 109\n  skip: 0\n  nrows: 100\n  row_num: 0\n\nProcessed variant: 100\nAll variants processed\n\n# get the genotyping info \nSpotMalariapfPanel_simData_sanger100_gt_tibble = SpotMalariapfPanel_simData_sanger100@gt %&gt;%\n              as_tibble()\n\n# get the genomic location info \nSpotMalariapfPanel_simData_sanger100_fixDf = as_tibble(getFIX(SpotMalariapfPanel_simData_sanger100))\n\n# get a vector of whether or not the SNPs are biallelic \nsnpIsBiallelic = is.biallelic(SpotMalariapfPanel_simData_sanger100)\n\n# filter and combine the genotyping data \nSpotMalariapfPanel_simData_sanger100_fixDf_filt = SpotMalariapfPanel_simData_sanger100_fixDf[snpIsBiallelic, ]\nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt = SpotMalariapfPanel_simData_sanger100_fixDf_filt %&gt;%\n  bind_cols( SpotMalariapfPanel_simData_sanger100_gt_tibble[is.biallelic(SpotMalariapfPanel_simData_sanger100),]) %&gt;% \n  gather(sample, call, 9:ncol(.)) %&gt;% \n  mutate(FORMAT = strsplit(FORMAT, split = \":\"), \n         call = strsplit(call, split = \":\")) %&gt;% \n  unnest(cols = c(FORMAT, call))\n\n# get the genotyping calls \nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt %&gt;% \n  filter(FORMAT == \"GT\") %&gt;% \n  mutate(varID = paste0(CHROM, \"-\", POS)) %&gt;% \n  mutate(genotypeCall = case_when(\n    \"0/0\" == call ~ \"0\", \n    \"1/1\" == call ~ \"1\", \n    \"0/1\" == call ~ \"2\"\n  )) %&gt;% \n  mutate(genotypeCall = as.numeric(genotypeCall))\n\n\n# create the matrix for the input into the mle data \n\nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_sp = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt %&gt;% \n  select(sample, varID, genotypeCall) %&gt;% \n  rename(ID = sample) %&gt;% \n  spread(varID, genotypeCall)\n\nThe mle function is found within the R script supplied by the publication[@Tsoungui_Obama2022-gz]. The number of loci used in the publication was limited to about 10 and the function is unable to do large number of loci due to the size of the matrix that would have to be created so would advise limiting number of loci used, here doing the 6 most diverse loci by he.\n\n# calculating he \n\nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt %&gt;% \n  mutate(genotypeCallExpand = case_when(\n    2 == genotypeCall ~ \"ref/alt\", \n    1 == genotypeCall ~ \"alt\", \n    0 == genotypeCall ~ \"ref\"\n  ))\n\nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod_sel = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod %&gt;% \n  select(varID, genotypeCallExpand) %&gt;% \n  mutate(genotypeCallExpand = strsplit(genotypeCallExpand, split = \"/\")) %&gt;% \n  unnest(genotypeCallExpand) %&gt;% \n  group_by(varID, genotypeCallExpand) %&gt;% \n  count() %&gt;% \n  group_by(varID) %&gt;% \n  mutate(total = sum(n)) %&gt;% \n  mutate(freq = n/total)\n\nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod_sel_he = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod_sel %&gt;% \n  group_by(varID) %&gt;% \n  summarise(he = 1 - sum(freq^2)) %&gt;% \n  arrange(desc(he))",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#mle",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#mle",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "MLE",
    "text": "MLE\n\n# select just the top 6 loci \n\nSpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod_sel_he_top = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod_sel_he %&gt;% \n  head(n = 6)\n\n# select out only the sample id and the varID \nselCols = c(\"ID\", SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_mod_sel_he_top$varID)\n\ninputToMLE = SpotMalariapfPanel_simData_sanger100_fixDf_filt_allgt_gt_sp[, selCols]\n\n# load mle functions \nsource(\"SNPModel.R\")\n\n# Find the MLEs\nest &lt;- mle(inputToMLE, id=TRUE)",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#prevalence",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#prevalence",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "prevalence",
    "text": "prevalence\nSeveral prevalences are called from the mle estimation results. These are the definition provided by the paper[@Tsoungui_Obama2022-gz].\n\nUnobservable prevalence\n\n\nSince haplotype information is typically unavailable from molecular assays, haplotypes are per se not observable in molecular samples. To emphasize this fact we call the probability that a haplotype occurs in an infection “unobservable prevalence.”\n\n\nConditional prevalence\n\n\nBecause of ambiguity of haplotype information in multiple infections, it is impossible to identify the number of samples containing haplotype h in a dataset. In practice, often only unambiguous samples are considered, to determine prevalence. Here, we derive the corresponding quantity in the underlying framework, i.e., the prevalence of haplotype h, conditioned on observing only unambiguous data. The quantity is referred to as “conditional prevalence.”\n\n\nRelative prevalence\n\n\nDue to unobservable information, a statistical model is required to obtain estimates for frequencies. However, in practice, “ad-hoc” estimates are popular if statistical methods are not available. Frequency estimates can be obtained, by first disregarding all ambiguous observations, calculate the empirically observed unambiguous prevalence of all haplotypes, and finally normalizing them - here we refer to this as the “relative unambiguous prevalence.”\n\n\n# Estimate prevalence\n## Unobservable prevalence\nunobsprev &lt;- estunobsprev(est)\nunobsprev_df = tibble(\n  haplotype = dimnames(unobsprev)[[2]], \n  unobsprevalence = c(unobsprev)\n)\n\n## Conditional prevalence\ncondprev &lt;- estcondprev(est)\ncondprev_df = tibble(\n  haplotype = dimnames(condprev)[[2]], \n  condprevalence = c(condprev)\n)\n\n## Relative prevalence\nrelprev &lt;- estrelprev(inputToMLE, id=TRUE)\nrelprev_df = tibble(\n  haplotype = dimnames(relprev)[[2]], \n  relprevalence = c(relprev)\n)\n\nprevalences = full_join(\n  unobsprev_df, \n  condprev_df\n) %&gt;% \n  full_join(relprev_df) %&gt;% \n  arrange(desc(unobsprevalence))\n\nJoining with `by = join_by(haplotype)`\nJoining with `by = join_by(haplotype)`\n\n\n\nDT::datatable(prevalences)",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#sample-moi-calls",
    "href": "tutorials/MultiLociBiallelicModel/MultiLociBiallelicModel_analysis.html#sample-moi-calls",
    "title": "Running MultiLociBiallelicModel on bialleic SNP data",
    "section": "Sample MOI calls",
    "text": "Sample MOI calls\nUsing the mle estimation results combined with the haplotype calls for the sample there are functions provided for making MOI estimates based on what’s the most probable MOI. The function samplwiseMOI gives you a list of probabilities for a range of MOIs and returns the most the probable MOI.\n\n# convert to matrix to make it easier to provide the haplotype call to the function \ninputToMLE_mat = as.matrix(inputToMLE[, 2:ncol(inputToMLE)])\nrownames(inputToMLE_mat) = inputToMLE$ID\n\nallSampleMOIs = tibble()\n# iterate over each sample by row and get an estimate of the MOI for each sample by providing the haplotype call for that sample \n\nfor (row in 1:nrow(inputToMLE_mat)) {\n  # estimate MOI probabilites \n  currentSampleMOI = samplwiseMOI(inputToMLE_mat[row, ], est)\n  # gathering \n  allSampleMOIs = bind_rows(allSampleMOIs,\n                            tibble(sample = rownames(inputToMLE_mat)[row],\n                                   MOI = currentSampleMOI$MOI))\n}\n\nDT::datatable(allSampleMOIs)\n\n\n\n\n\nSince these samples are simualted, the known COI for the sample is known so can compare simulated MOI(COI) vs determined MOI by MultiLociBiallelicModel.\n\nsimCOI = readr::read_tsv(\"../../data/snp_barcode/SpotMalariapfPanel_simData_sanger100_simCOIs.tab.txt\") %&gt;% \n  rename(sample = Patient) %&gt;% \n  left_join(allSampleMOIs)\n\nRows: 100 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Patient\ndbl (1): initialCOI\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nJoining with `by = join_by(sample)`\n\nggplot(simCOI) +\n  geom_count(aes(x = initialCOI,\n                 y = MOI)) +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\n\nsimCOI_sum = simCOI %&gt;%\n  group_by(initialCOI, MOI) %&gt;%\n  count()\n\nggplot(simCOI_sum) +\n  geom_col(aes(x = initialCOI, y = n, fill = factor(MOI)), color = \"black\", position = \"dodge\") +\n  scale_fill_brewer(\"Observed MOI\", palette = \"Dark2\") + \n  theme_minimal() + \n  labs(x = \"Simualted COI\")",
    "crumbs": [
      "Tool resources",
      "MultiLociBiallelicModel",
      "Running `MultiLociBiallelicModel` on bialleic SNP data"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_background.html#summary-sheet",
    "href": "tutorials/paneljudge/paneljudge_background.html#summary-sheet",
    "title": "paneljudge",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases,barcode/panel assessment\n\n\nAuthors,Aimee Taylor & Pierre Jacob\n\n\nLatest version,0.0.0.9000\n\n\nLicense,MIT\n\n\nWebsite,https://github.com/aimeertaylor/paneljudge\n\n\nCode repository,https://github.com/aimeertaylor/paneljudge\n\n\nPublication,https://pubmed.ncbi.nlm.nih.gov/35437908/\n\n\nTutorial authors,Nick Brazeau\n\n\nTutorial date,Dec 12 2023",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_background.html#purpose",
    "href": "tutorials/paneljudge/paneljudge_background.html#purpose",
    "title": "paneljudge",
    "section": "Purpose",
    "text": "Purpose\nAn R package that judges the “informativeness” of a genetic panel (or barcode) in its ability to infer relatedness. The panel is assessed under simulation with an identity-by-descent hidden-markov model backend. Precision in relatedness estimates indicate the superior panel.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_background.html#existing-resources",
    "href": "tutorials/paneljudge/paneljudge_background.html#existing-resources",
    "title": "paneljudge",
    "section": "Existing resources",
    "text": "Existing resources\nTaylor et al. Genetics 212.4 (2019): 1337-1351.\nSchaffner et al. Malaria journal 17.1 (2018): 196.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_background.html#citation",
    "href": "tutorials/paneljudge/paneljudge_background.html#citation",
    "title": "paneljudge",
    "section": "Citation",
    "text": "Citation\nLaVerriere et al., Molecular Ecology Resources 22.6 (2022): 2285-2303\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"paneljudge\"):\n@Manual{,\n    title = {paneljudge: Judge the performance of a panel of genetic markers using\nsimulated data},\n    author = {Aimee Taylor and Pierre Jacob},\n    year = {2023},\n    note = {R package version 0.0.0.9000},\n  }",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_installation.html",
    "href": "tutorials/paneljudge/paneljudge_installation.html",
    "title": "Installing paneljudge",
    "section": "",
    "text": "The paneljudge R package can be directly installed from plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_installation.html#installation-from-r-universe",
    "href": "tutorials/paneljudge/paneljudge_installation.html#installation-from-r-universe",
    "title": "Installing paneljudge",
    "section": "",
    "text": "The paneljudge R package can be directly installed from plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed.",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/paneljudge/paneljudge_installation.html#installation-from-github",
    "href": "tutorials/paneljudge/paneljudge_installation.html#installation-from-github",
    "title": "Installing paneljudge",
    "section": "Installation from Github",
    "text": "Installation from Github\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"aimeertaylor/paneljudge\")\n\n\nDependencies\npaneljudge relies on the Rcpp package, which requires certain OS-specific dependencies:\n\nWindows\n\nDownload and install the appropriate version of Rtools for your version of R. On installation, ensure you check the box to arrange your system PATH as recommended by Rtools\n\nMac OS X\n\nDownload and install XCode\nWithin XCode go to Preferences : Downloads and install the Command Line Tools\n\nLinux (Debian/Ubuntu)\n\nInstall the core software development utilities required for R package development as well as LaTeX by executing\n\nsudo apt-get install r-base-dev texlive-full\n\nAssuming everything installed correctly, you can now load the package:\n\nlibrary(paneljudge)",
    "crumbs": [
      "Tool resources",
      "paneljudge",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/MLMOI/Template_background.html#summary-sheet",
    "href": "tutorials/MLMOI/Template_background.html#summary-sheet",
    "title": "MLMOI",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nMOI estimation\n\n\nAuthors\nMeraj Hashemi, Kristan Shneider\n\n\nLatest version\n70fd9fd\n\n\nLicense\nnone\n\n\nWebsite\nhttps://github.com/Maths-against-Malaria/MOI-Bias-correction\n\n\nCode repository\nhttps://github.com/Maths-against-Malaria/MOI-Bias-correction\n\n\nPublication\nhttps://doi.org/10.1371/journal.pone.0261889\n\n\nTutorial authors\nAlfred Simkin\n\n\nTutorial date\n11-Dec-23"
  },
  {
    "objectID": "tutorials/MLMOI/Template_background.html#purpose",
    "href": "tutorials/MLMOI/Template_background.html#purpose",
    "title": "MLMOI",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of MLMOI is to estimate complexity of infection with various corrections for unobservable states. The program also generates simulated datasets. The approach seems to involve simulating datasets and checking to see if the observed data correlates well with the simulated datasets derived by a given model. We were unable to install the package as it has been removed from the R-CRAN library, and had difficulty following the vignettes in the archived package. More specifically, We had difficulty interpreting unitless numbers associated with a given locus for a given sample, and understanding why some missing samples (no sample name) had filled in values and other named samples had missing values). Manual installation (with no package available) was also difficult to follow."
  },
  {
    "objectID": "tutorials/MLMOI/Template_background.html#existing-resources",
    "href": "tutorials/MLMOI/Template_background.html#existing-resources",
    "title": "MLMOI",
    "section": "Existing resources",
    "text": "Existing resources\n\nAny existing online tutorials?\nAny important papers?"
  },
  {
    "objectID": "tutorials/MLMOI/Template_background.html#citation",
    "href": "tutorials/MLMOI/Template_background.html#citation",
    "title": "MLMOI",
    "section": "Citation",
    "text": "Citation\nBibTeX style citation. For an R package, you can get this using citation(package = \"name\"):\nHere is an example for DRpower, using citation(package = \"DRpower\"):\n@Manual{,\n    title = {DRpower: Study design and analysis for pfhrp2/3 deletion prevalence studies},\n    author = {Bob Verity and Shazia Ruybal},\n    note = {R package version 1.0.2},\n  }"
  },
  {
    "objectID": "tutorials/dcifer/dcifer_installation.html",
    "href": "tutorials/dcifer/dcifer_installation.html",
    "title": "Installing dcifer",
    "section": "",
    "text": "The dcifer R package is both on CRAN and part of the plasmogenepi.r-universe.\nYou can install it by running the following code:\n\n# Install dcifer in R:\ninstall.packages('dcifer', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\n\n\n\n Back to top",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_analysis.html",
    "href": "tutorials/dcifer/dcifer_analysis.html",
    "title": "Using dcifer to estimate relatedness from biallelic SNP data",
    "section": "",
    "text": "Below we will demonstrate how to use dcifer using biallelic Sanger 100-SNP barcode data in .vcf format. We will use data created by simulating 100 polyclonal infections from Bangladesh (n=50) and Ghana (n=50). See the PGEforge website for further details.\nIn this tutorial we will use PGEhammer to convert data from the VCF format to the format required by Dcifer. To install the package run the following command\n\n# Install PGEhammer in R:\ninstall.packages('PGEhammer', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\n\nThe downloaded binary packages are in\n    /var/folders/wx/rr171mzs0lj0mtflng6dwl7h0000gp/T//Rtmp8DqrQV/downloaded_packages\n\n\nNow we can load the packages we will need.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(dcifer)\nlibrary(vcfR)\nlibrary(kableExtra)\nlibrary(PGEhammer)",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_analysis.html#the-data",
    "href": "tutorials/dcifer/dcifer_analysis.html#the-data",
    "title": "Using dcifer to estimate relatedness from biallelic SNP data",
    "section": "",
    "text": "Below we will demonstrate how to use dcifer using biallelic Sanger 100-SNP barcode data in .vcf format. We will use data created by simulating 100 polyclonal infections from Bangladesh (n=50) and Ghana (n=50). See the PGEforge website for further details.\nIn this tutorial we will use PGEhammer to convert data from the VCF format to the format required by Dcifer. To install the package run the following command\n\n# Install PGEhammer in R:\ninstall.packages('PGEhammer', repos = c('https://plasmogenepi.r-universe.dev', 'https://cloud.r-project.org'))\n\n\nThe downloaded binary packages are in\n    /var/folders/wx/rr171mzs0lj0mtflng6dwl7h0000gp/T//Rtmp8DqrQV/downloaded_packages\n\n\nNow we can load the packages we will need.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(dcifer)\nlibrary(vcfR)\nlibrary(kableExtra)\nlibrary(PGEhammer)",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_analysis.html#wrangling-the-data",
    "href": "tutorials/dcifer/dcifer_analysis.html#wrangling-the-data",
    "title": "Using dcifer to estimate relatedness from biallelic SNP data",
    "section": "Wrangling the data",
    "text": "Wrangling the data\nDcifer requires input data in long format, the long format represents data with each observation on a separate row. This data can be biallelic or multiallelic. In the following steps, we will convert the Variant Call Format (VCF) data to the required long format using the function vcf2long from PGEhammer.\n\n# Read vcf\nvcf &lt;- read.vcfR(here('data/snp_barcode/SpotMalariapfPanel_simData_sanger100.vcf.gz'))\n\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 76\n  header_line: 77\n  variant count: 100\n  column count: 109\n\nMeta line 76 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 100\n  Character matrix gt cols: 109\n  skip: 0\n  nrows: 100\n  row_num: 0\n\nProcessed variant: 100\nAll variants processed\n\n# Convert to long format \ndf &lt;- vcf2long(vcf)\n\nConverting from vcf to long format...\n\n\nReformatting complete.\n\n\nWarning in vcf2long(vcf): Your vcf is not all bi-allelic. Make sure to double\ncheck if this is not expected.\n\nhead(df) |&gt;\n  kable() |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nsample_id\nlocus\nallele\nread_count\n\n\n\n\nGhana-45\nPf3D7_01_v3_145515\nallele-1\n889\n\n\nGhana-45\nPf3D7_01_v3_145515\nallele-2\n7354\n\n\nGhana-45\nPf3D7_01_v3_179347\nallele-1\n1061\n\n\nGhana-45\nPf3D7_01_v3_179347\nallele-2\n9661\n\n\nGhana-45\nPf3D7_01_v3_180554\nallele-1\n9564\n\n\nGhana-45\nPf3D7_01_v3_180554\nallele-2\n5",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_analysis.html#calculate-coi-and-allele-frequencies",
    "href": "tutorials/dcifer/dcifer_analysis.html#calculate-coi-and-allele-frequencies",
    "title": "Using dcifer to estimate relatedness from biallelic SNP data",
    "section": "Calculate COI and allele frequencies",
    "text": "Calculate COI and allele frequencies\nBefore we calculate IBD we first need to calculate COI. Below we use the function getCOI that Dcifer provides which uses naive estimation, but you could use another tool for this.\n\nlrank &lt;- 2\ncoi   &lt;- getCOI(dsmp, lrank = lrank)\n\nThe last thing we need to do before calculating IBD is to add in allele frequencies. Again we use a function within Dcifer for this, calcAfreq.\n\nafreq &lt;- calcAfreq(dsmp, coi, tol = 1e-5) \nstr(afreq, list.len = 2)\n\nList of 87\n $ t1  : Named num [1:5] 0.4239 0.2808 0.1116 0.0422 0.1415\n  ..- attr(*, \"names\")= chr [1:5] \"D10--D6--FCR3--V1-S.0\" \"HB3.0\" \"t1.0\" \"t1.1\" ...\n $ t10 : Named num [1:4] 0.8539 0.00942 0.00951 0.12717\n  ..- attr(*, \"names\")= chr [1:4] \"D10--D6--HB3.0\" \"t10.0\" \"t10.2\" \"U659.0\"\n  [list output truncated]\n\n\n\ndres0 &lt;- ibdDat(dsmp, coi, afreq, pval = TRUE, confint = TRUE, rnull = 0, \n                alpha = 0.05, nr = 1e3)",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorials/dcifer/dcifer_analysis.html#summary",
    "href": "tutorials/dcifer/dcifer_analysis.html#summary",
    "title": "Using dcifer to estimate relatedness from biallelic SNP data",
    "section": "Summary",
    "text": "Summary\nIn summary, we have used Dcifer to estimate COI and allele frequencies before estimating IBD. Dcifer has extensive documentation, including more details on other functionality available within the tool and a tutorial using microhaplotype data.",
    "crumbs": [
      "Tool resources",
      "dcifer",
      "Tutorial"
    ]
  },
  {
    "objectID": "tutorials/Template/Template_analysis.html",
    "href": "tutorials/Template/Template_analysis.html",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/Template/Template_analysis.html#the-data",
    "href": "tutorials/Template/Template_analysis.html#the-data",
    "title": "Give this whatever name you want",
    "section": "",
    "text": "These headings are placeholders, but you may want to start by giving a brief description of which data you will use. Remember you can point to the main Data tab for further details.\nRemember to load any packages, including the tool that you are working on:\n\nlibrary(tidyverse)\nlibrary(here)"
  },
  {
    "objectID": "tutorials/Template/Template_analysis.html#first-analysis-section",
    "href": "tutorials/Template/Template_analysis.html#first-analysis-section",
    "title": "Give this whatever name you want",
    "section": "First analysis section",
    "text": "First analysis section\nMake as many sections as you need. Try to break long documents out over multiple pages."
  },
  {
    "objectID": "tutorials/Template/Template_analysis.html#summary",
    "href": "tutorials/Template/Template_analysis.html#summary",
    "title": "Give this whatever name you want",
    "section": "Summary",
    "text": "Summary\nIt’s a good idea to round off with a summary of what we did.\nPS, note that the name of the tutorial on the PGEforge website can be whatever you want - it does not have to match the name of this tutorial."
  },
  {
    "objectID": "tutorials/Template/Template_installation.html",
    "href": "tutorials/Template/Template_installation.html",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/Template/Template_installation.html#step-n-what-to-do",
    "href": "tutorials/Template/Template_installation.html#step-n-what-to-do",
    "title": "Installing (tool name)",
    "section": "",
    "text": "Short explanation. For most tools we will use Max’s plasmogenepi.r-universe, which greatly simplifies installation. For other tools, please list the steps as needed."
  },
  {
    "objectID": "tutorials/estMOI/background.html#summary-sheet",
    "href": "tutorials/estMOI/background.html#summary-sheet",
    "title": "estMOI",
    "section": "Summary sheet",
    "text": "Summary sheet\n\n\n\n\n\nMain use-cases\nMultiplicity of Infection estimation\n\n\nAuthors\nSamuel Assefa\n\n\nLatest version\nv1.03\n\n\nLicense\nNot stated\n\n\nWebsite\nhttps://github.com/sammy-assefa/estMOI\n\n\nCode repository\nhttps://github.com/sammy-assefa/estMOI\n\n\nPublication\nhttps://academic.oup.com/bioinformatics/article/30/9/1292/235531\n\n\nTutorial authors\nJody Phelan\n\n\nTutorial date\n11-Dec-23",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/estMOI/background.html#purpose",
    "href": "tutorials/estMOI/background.html#purpose",
    "title": "estMOI",
    "section": "Purpose",
    "text": "Purpose\nestMOI is a Perl script that estimates the MOI locally in the genome and then overall to obtain a global estimate. The inputs are alignment (BAM files), variant regions in the Variant Call Format/VCF and an optional file of regions to exclude from analysis. Minimal multiplicity is inferred by considering the maximum number of distinct haplotypes formed by combinations of a user-specified number of single nucleotide polymorphisms (SNPs) on single or paired reads.",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/estMOI/background.html#existing-resources",
    "href": "tutorials/estMOI/background.html#existing-resources",
    "title": "estMOI",
    "section": "Existing resources",
    "text": "Existing resources\n\nA manual can be found here",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/estMOI/background.html#citation",
    "href": "tutorials/estMOI/background.html#citation",
    "title": "estMOI",
    "section": "Citation",
    "text": "Citation\nYou can cite the tool using the following publication:\n\nAssefa, Samuel A et al. “estMOI: estimating multiplicity of infection using parasite deep sequencing data.” Bioinformatics (Oxford, England) vol. 30,9 (2014): 1292-4. doi:10.1093/bioinformatics/btu005",
    "crumbs": [
      "Tool resources",
      "estMOI",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_overview.html",
    "href": "tutorials/hmmIBD/hmmIBD_overview.html",
    "title": "PGEforge",
    "section": "",
    "text": "The program takes as input a file of genotype calls for a set of samples and outputs estimates of the fraction of their genomes that are identical by descent (IBD), based on the observed genotypes and the estimated allele frequencies in the population. Optionally, a second file of genotype calls can be supplied, which will be treated as coming from a different population with different allele frequencies. For a single population, all pairwise comparisons are made between the samples (unless otherwise specified with a -b or -g flag.) For two populations, all comparisons are made between samples from the two different populations.\nUnder the HMM, each variant site is assumed to be in one of two hidden states, IBD or not-IBD. To calculate the probability of each state, estimates of the allele frequencies for every variant are required. By default, they are calculated from the input data, but a separate file of allele frequencies can be supplied by the user (preferable if analyzing a subset of the data).\nThe model has two free parameters, (1) the fraction of the genome that is IBD, and (2) the number of generations during which recombination has been breaking down IBD blocks. (Note: the former is generally estimated more accurately than the latter, and is pretty robust to the latter.) The program fits for optimal values of these parameters by an iterative estimation-maximization procedure. Iterations of the fit are capped at a user-settable maximum (default = 5). To accurately determine the IBD fraction for large shared chromosome segments, only a few iterations are needed, while for smaller, older blocks of IBD, the fit may continue to improve for 15 or more iterations.\nThe Viterbi algorithm calculates the best single set of state assignments given data under the HMM and outputs that set. The forward-backward algorithm sums the fraction of the genome that is IBD over all possible state assignments given data under the HMM, weighting each by the probability of that set of states. If you are interested in the IBD fraction, rather than precisely which parts of the genome are IBD, this is probably the output you want (see fract_sites_IBD in the Output section).\nIt may be a good idea to thin the variant sites being supplied to hmmIBD for several reasons. First, variants with very low or zero minor allele frequency contribute little to IBD determination but do waste compute time. Second, very closely spaced sites will (be default) be automatically thinned by hmmIBD and not in an intelligent way, in an effort to avoid mutations (or sequencing errors) that extend beyond a single base pair. Third, if the density of sites varies a lot across the genome, hmmIBD’s power to detect short IBD segments will be higher where site density is higher. The repository includes a Python script (thin_sites.py) that implements a simple scheme for thinning sites.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_overview.html#overview",
    "href": "tutorials/hmmIBD/hmmIBD_overview.html#overview",
    "title": "PGEforge",
    "section": "",
    "text": "The program takes as input a file of genotype calls for a set of samples and outputs estimates of the fraction of their genomes that are identical by descent (IBD), based on the observed genotypes and the estimated allele frequencies in the population. Optionally, a second file of genotype calls can be supplied, which will be treated as coming from a different population with different allele frequencies. For a single population, all pairwise comparisons are made between the samples (unless otherwise specified with a -b or -g flag.) For two populations, all comparisons are made between samples from the two different populations.\nUnder the HMM, each variant site is assumed to be in one of two hidden states, IBD or not-IBD. To calculate the probability of each state, estimates of the allele frequencies for every variant are required. By default, they are calculated from the input data, but a separate file of allele frequencies can be supplied by the user (preferable if analyzing a subset of the data).\nThe model has two free parameters, (1) the fraction of the genome that is IBD, and (2) the number of generations during which recombination has been breaking down IBD blocks. (Note: the former is generally estimated more accurately than the latter, and is pretty robust to the latter.) The program fits for optimal values of these parameters by an iterative estimation-maximization procedure. Iterations of the fit are capped at a user-settable maximum (default = 5). To accurately determine the IBD fraction for large shared chromosome segments, only a few iterations are needed, while for smaller, older blocks of IBD, the fit may continue to improve for 15 or more iterations.\nThe Viterbi algorithm calculates the best single set of state assignments given data under the HMM and outputs that set. The forward-backward algorithm sums the fraction of the genome that is IBD over all possible state assignments given data under the HMM, weighting each by the probability of that set of states. If you are interested in the IBD fraction, rather than precisely which parts of the genome are IBD, this is probably the output you want (see fract_sites_IBD in the Output section).\nIt may be a good idea to thin the variant sites being supplied to hmmIBD for several reasons. First, variants with very low or zero minor allele frequency contribute little to IBD determination but do waste compute time. Second, very closely spaced sites will (be default) be automatically thinned by hmmIBD and not in an intelligent way, in an effort to avoid mutations (or sequencing errors) that extend beyond a single base pair. Third, if the density of sites varies a lot across the genome, hmmIBD’s power to detect short IBD segments will be higher where site density is higher. The repository includes a Python script (thin_sites.py) that implements a simple scheme for thinning sites.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_overview.html#execution",
    "href": "tutorials/hmmIBD/hmmIBD_overview.html#execution",
    "title": "PGEforge",
    "section": "Execution",
    "text": "Execution\nhmmIBD is run from the command line. It requires the user to supply two options when invoked; it can also take seven optional arguments:\nhmmIBD -i &lt;input filename (for pop1, if using 2 pops)&gt; -o &lt;output filename&gt; \n     [-I &lt;input file, pop2&gt;] [-f &lt;allele frequency file (pop1)&gt;] [-F &lt;allele freq file (pop2)&gt;]\n     [-b &lt;file with samples to skip&gt;] [-m &lt;max fit iteration&gt;] [-n &lt;max N generation&gt;] [-g &lt;file with sample pairs to use&gt;] [-r &lt;fixed IBD prior&gt;]\nRequired options: - -i: File of genotype data. See below for format. - -o: Output file name. Two output files will be produced, with “.hmm.txt” and “.hmm_fract.txt” appended to the supplied name. See below for details\nOptional options: - -f: File of allele frequencies for the sample population. Format: tab-delimited, no header, one variant per row. Line format: &lt;chromosome (int)&gt; &lt;position (bp, int)&gt; &lt;allele 1 freq&gt; &lt;all 2 freq&gt; [&lt;all 3 freq&gt;] ... The genotype and frequency files must contain exactly the same variants, in the same order. If no file is supplied, allele frequencies are calculated from the input data file. - -I: File of genotype data from a second population; same format as for -i. (added in 2.0.0) - -F: File of allele frequencies for the second population; same format as for -f. (added in 2.0.0) - -m: Maximum number of fit iterations (defaults to 5). - -b: File of sample ids to exclude from all analysis. Format: no header, one id (string) per row. (Note: b stands for “bad samples”.) - -g: File of sample pairs to analyze; all others are not processed by the HMM (but are still used to calculate allele frequencies). Format: no header, tab-delimited, two sample ids (strings) per row. (Note: “g” stands for “good pairs”.) - - -n: Cap on the number of generations (floating point). Sets the maximum value for that parameter in the fit. This is useful if you are interested in recent IBD and are working with a population with substantial linkage disequilbrium. Specifying a small value will force the program to assume little recombination and thus a low transition rate; otherwise it will identify the small blocks of LD as ancient IBD, and will force the number of generations to be large. - -r: Supplies a fixed value of the IBD fraction (fract_sites_IBD) used to determine IBD segments. This is useful when using hmmIBD to detect or characterize selective sweeps. Without this, IBD segments are more likely to be detected when comparing relatives, since the overall relatedness biases the probability of detecting any one segment.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_overview.html#input-file-formats",
    "href": "tutorials/hmmIBD/hmmIBD_overview.html#input-file-formats",
    "title": "PGEforge",
    "section": "Input file formats",
    "text": "Input file formats\nFormat for genotype file: tab-delimited text file, with one single nucleotide polymorphism (SNP) per line. The first two columns are the chromosome and position, followed by one sample per column. A header line, giving the sample names, is required. Genotypes are coded by number: -1 for missing data, 0 for the first allele, 1 for the second, etc. SNPs and indels (if you trust them) can thus be treated on an equal footing. The variants must be in chromosome and position order, and can have between two and eight alleles (more, if you feel like changing max_allele in the code). The package includes a Python script, vcf2hmm.py, that extracts genotypes from VCF files into the appropriate format. It takes a VCF file name and an output filename (the latter without extensions) as input, along with optional file names that give lists of samples and sites to include. It outputs the genotypes into filename_seq.txt, and also outputs the alleles for each site (filename_allele.txt) and the allele frequencies (filename_freq.txt).",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_overview.html#output-files",
    "href": "tutorials/hmmIBD/hmmIBD_overview.html#output-files",
    "title": "PGEforge",
    "section": "Output files",
    "text": "Output files\nTwo output files are produced. The file &lt;filename&gt;.hmm.txt contains a list of all segments (where a segment is one or more contiguous variant sites in the same state) for each sample pair, the assigned state (IBD or not-IBD) for each, and the number of variants covered by the segment; note that an assigned state of 0 means IBD, while 1 means not-IBD. These segments represent the most probable state assignments.\nThe file &lt;filename&gt;.hmm_fract.txt summarizes results for each sample pair (including some information that may be of interest only to me). If you are only interested in the fraction of the genome that is IBD between a pair of samples, look at the last column (fract_sites_IBD). Columns:\n\nN_informative_sites: Number of sites with data for both samples, and with at least one copy of the minor allele.\ndiscordance: Fraction of informative sites that have different alleles in the two samples.\nlog_p: natural logarithm of the probability of the final set of state assignments and the set of observations (calculated with the Viterbi algorithm).\nN_fit_iteration: Number of iterations carried out in EM fitting.\nN_generation: Estimated number of generations of recombination (1 of 2 free parameters in fit).\nN_state_transition: Number of transitions between IBD and not-IBD states across entire genome.\nseq_shared_best_traj: Fraction of sequence IBD based on the best state assignment, calculated as (seq in IBD segments) / (seq in IBD segments + seq in not-IBD segments). Segments in which there is a state transition between IBD and not-IBD are ignored.\nfract_sites_IBD: Fraction of variant sites called IBD calculated for all possible state assignments, weighted by their probability (equal to the marginal posterior probability of the IBD state calculated with the forward-backward algorithm).\nfract_vit_sites_IBD: Fraction of variant sites called IBD calculated for the best state assignment (i.e. the result of the Viterbi algorithm, as in seq_shared_best_traj).",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_overview.html#some-variables-you-might-want-to-change",
    "href": "tutorials/hmmIBD/hmmIBD_overview.html#some-variables-you-might-want-to-change",
    "title": "PGEforge",
    "section": "Some variables you might want to change",
    "text": "Some variables you might want to change\nThe following variables control program execution in various ways, and can be easily changed in the C code prior to compilation.\n\neps – assumed genotype error rate (rate of calling allele i as allele j) (default = 0.1%).\nmin_inform – minimum number of informative sites (those with at least one copy of the minor allele in this pair) required for processing sample pair.\nmax_discord – maximum fraction of informative sites with discordant genotypes between the two samples; used to skip unrelated pairs .\nmin_discord – minimum discordance, useful for skipping identical pairs.\nnchrom – number of chromosomes in genome (14 for P. falciparum and P. vivax)\nmin_snp_sep – number of bp separation required between SNPs, to avoid correlated mutations spanning &gt;1 bp (default = 10).",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_installation.html",
    "href": "tutorials/hmmIBD/hmmIBD_installation.html",
    "title": "PGEforge",
    "section": "",
    "text": "The C source code must be compiled before use, and requires no special libraries. If you do not already have a C compiler installed, you will have to install one. (Note if you are looking for a compiler in your environment: common names you might see for a compiler include CC and GCC.) The installation procedure varies with operation system:\n\n\nMac\n\n\nOpen a Terminal window. You can do this by searching (keyboard shortcut command+space) for “Terminal”.\nInside the terminal window, type the command xcode-select --install. This will prompt you to install Xcode command line tools.\nOnce you have Xcode command line tools installed you should be able to compile.\n\n\n\n\nLinux\n\nFollow these directions: https://phoenixnap.com/kb/install-gcc-ubuntu.\n\n\n\nWindows\n\nInstallation is complicated on Windows. These two guides may help: https://www.guru99.com/c-gcc-install.html and https://dev.to/gamegods3/how-to-install-gcc-in-windows-10-the-easier-way-422j.\n\nIf your compiler is named ‘CC’, the following command\ncc -o hmmIBD -O3 -lm -Wall hmmIBD.c\nshould yield the executable file hmmIBD.\nIf you wish to use them, the Python scripts for extracting VCF data and thinning sites require that Python be installed, version 3.6 or later. Mac and most versions of Linux already have Python3 installed. For Windows, directions can be found here.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/hmmIBD/hmmIBD_installation.html#installation",
    "href": "tutorials/hmmIBD/hmmIBD_installation.html#installation",
    "title": "PGEforge",
    "section": "",
    "text": "The C source code must be compiled before use, and requires no special libraries. If you do not already have a C compiler installed, you will have to install one. (Note if you are looking for a compiler in your environment: common names you might see for a compiler include CC and GCC.) The installation procedure varies with operation system:\n\n\nMac\n\n\nOpen a Terminal window. You can do this by searching (keyboard shortcut command+space) for “Terminal”.\nInside the terminal window, type the command xcode-select --install. This will prompt you to install Xcode command line tools.\nOnce you have Xcode command line tools installed you should be able to compile.\n\n\n\n\nLinux\n\nFollow these directions: https://phoenixnap.com/kb/install-gcc-ubuntu.\n\n\n\nWindows\n\nInstallation is complicated on Windows. These two guides may help: https://www.guru99.com/c-gcc-install.html and https://dev.to/gamegods3/how-to-install-gcc-in-windows-10-the-easier-way-422j.\n\nIf your compiler is named ‘CC’, the following command\ncc -o hmmIBD -O3 -lm -Wall hmmIBD.c\nshould yield the executable file hmmIBD.\nIf you wish to use them, the Python scripts for extracting VCF data and thinning sites require that Python be installed, version 3.6 or later. Mac and most versions of Linux already have Python3 installed. For Windows, directions can be found here.",
    "crumbs": [
      "Tool resources",
      "hmmIBD",
      "Installation"
    ]
  },
  {
    "objectID": "website_docs/wasabi25.html",
    "href": "website_docs/wasabi25.html",
    "title": "WASABI25",
    "section": "",
    "text": "Workflows for Amplicon Sequencing Analytics and BIoInformatics Hackathon (WASABI), organised by Kathryn Murie, Alfred Hubbard, Bryan Greenhouse and Amy Wesolowski took place at Johns Hopkins Bloomberg School of Public Health in Baltimore, USA from 17-21st January 2025, with 12 participants from 6 institutions.\n\n\n\n\n\nA cool wasabi",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "WASABI25 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/wasabi25.html#background",
    "href": "website_docs/wasabi25.html#background",
    "title": "WASABI25",
    "section": "",
    "text": "Workflows for Amplicon Sequencing Analytics and BIoInformatics Hackathon (WASABI), organised by Kathryn Murie, Alfred Hubbard, Bryan Greenhouse and Amy Wesolowski took place at Johns Hopkins Bloomberg School of Public Health in Baltimore, USA from 17-21st January 2025, with 12 participants from 6 institutions.\n\n\n\n\n\nA cool wasabi",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "WASABI25 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/wasabi25.html#main-aims",
    "href": "website_docs/wasabi25.html#main-aims",
    "title": "WASABI25",
    "section": "Main aims",
    "text": "Main aims\nOur main aim was to focus on generating a core set of scripts and wrapper functions to perform common downstream analysis functionalities for Plasmodium genomic analysis workflows. The aim is for these scripts to serve as foundational components for building robust and reusable bioinformatic and analysis workflows. We focused on writing wrapper scripts for existing tools that estimate complexity of infection (COI) and tools that perform other functionalities (eg estimating relatedness, estimating multi-locus genotype frequencies). We also wrote bespoke code to perform functionalities not currently fulfilled by any available tool. During the hackathon, we prioritised tools for targeted sequencing data.\nScript development was centered around building necessary components for the following analysis workflows:\n\nDrug resistance\nTransmission intensity\n\n\n\nFor a general schematic of the workflows, see analysis worfklows, however the workflows developed during WASABI25 are more detailed to outline specific functionalites necessary for the full end-to-end workflow.\nThe hackathon was divided into two parts:\n\nPart 1: script development\nPart 2: integration of scripts into Nextflow and WDL pipelines",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "WASABI25 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/wasabi25.html#outputs",
    "href": "website_docs/wasabi25.html#outputs",
    "title": "WASABI25",
    "section": "Outputs",
    "text": "Outputs\nThe main output of WASABI25 was PGEcore, a central repository for scripts that integrate and wrap commonly used genomic data analysis tools and bespoke code for common functionalities. This includes wrappers of existing tools and bespoke code to perform common tasks.\nWe now have the necessary components and fully functioning worfklows for 1) drug resistance and 2) transmission intensity. Most of the components also now exist for a third workflow: geographic connectivity.\n\n\n\n\nCheck out the PGEcore GitHub repository at github.com/PlasmoGenEpi/PGEcore!",
    "crumbs": [
      "How to contribute",
      "Events and meetings",
      "WASABI25 Hackathon"
    ]
  },
  {
    "objectID": "website_docs/data_description.html",
    "href": "website_docs/data_description.html",
    "title": "Data",
    "section": "",
    "text": "Understanding the different types of genomic data formats is essential for anyone involved in Plasmodium genomic analysis, especially beginners or end-users of various analysis tools. Here we briefly cover the primary data formats used in malaria research and common input file formats such as .vcf, .fasta, and others.\n\n\nBefore we jump into the different data formats, it is important to understand how malaria genomic data are generated. The variety in data formats arises from different molecular marker panels and available methodologies. The figure below illustrates how different genotyping approaches capture distinct aspects of the Plasmodium genome and how genomic data are ‘generated’ using different techniques and genotyping platforms. Next-generation sequencing is now the most common approach due to significant decreases in cost and its high-throughput nature both for whole genome sequencing but also sequencing of specific molecular markers or genomic regions of interest, also known as targeted sequencing.\n\n\n Targeted sequencing is similar to WGS, but sample preparation workflow requires an extra step: target enrichment either by hybridization capture or PCR amplicons. These methods are often used for enriching longer and shorter genomic regions, respectively.\n\n\n\nFigure sourced from Ruybal-Pesántez et al 2024, Molecular markers for malaria genetic epidemiology: progress and pitfalls\n\n\n\n\n  Raw sequencing data is processed by bioinformatic pipelines to call alleles and haplotypes. The outputs of this process, such as VCF, FASTA, or haplotype tables, serve as input formats for downstream data analysis tools.\n\n\n\nRaw sequence data from next-generation sequencing platforms is processed through bioinformatic pipelines into structured formats suitable for further downstream genomic analysis. Typically this process involves a series of steps. Initially, the raw data undergoes quality control to remove low-quality reads and contaminants. The cleaned data is then aligned to a reference genome, which helps identify the position of each sequence read. From this alignment, variants can be ‘called’, which means variants are identified through differences between the sample and the reference genome. These variants are compiled into Variant Call Format (.vcf) files. Alternatively, the entire cleaned and aligned sequences can be compiled into FASTA format (.fasta) files, which represent the sequences in a text-based format. Additionally, haplotypes may also be called, which means combinations of alleles at multiple loci are identified. These haplotypes are often tabulated in .csv or text-based formats.\n\n\n  Bioinformatic pipelines in malaria genomics need to account for the unique characteristics of Plasmodium genomes, such as high AT content and extensive polymorphism.\n\n\n\nWhen working with Plasmodium genomic data, several input file formats are commonly used across different analysis tools:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.vcf (Variant Call Format): Widely used for storing gene sequence variants, such as SNPs, insertions, deletions, and other types of genetic variants. The standard format includes a mandatory header starting with # or ## in the case of ‘special header keywords’, which provides the metadata such as file format, reference genome. This is followed by the ‘body’, which is tab-separated and each line represents a variant and its relevant information, such as chromosome and position, reference and alternate allele(s), and sample genotype information.\n\n\n\n\n  Figure sourced and modified from Wikipedia\n\n.fasta: Widely used for representing nucleotide and protein sequences, in which nucleotides or amino acids are represented using single-letter codes. Each entry begins with a header line starting with &gt; followed by a unique sequence ID and description line, and then the sequence data itself, with one letter per nucleic or amino acid.\n\n\n\n\n Figure sourced from Wikipedia\n\n.txt or .csv: Simple text and comma-separated values formats that are often used to represent haplotype or count data.",
    "crumbs": [
      "Data"
    ]
  },
  {
    "objectID": "website_docs/data_description.html#overview-of-data-formats-in-plasmodium-genomics",
    "href": "website_docs/data_description.html#overview-of-data-formats-in-plasmodium-genomics",
    "title": "Data",
    "section": "",
    "text": "Understanding the different types of genomic data formats is essential for anyone involved in Plasmodium genomic analysis, especially beginners or end-users of various analysis tools. Here we briefly cover the primary data formats used in malaria research and common input file formats such as .vcf, .fasta, and others.\n\n\nBefore we jump into the different data formats, it is important to understand how malaria genomic data are generated. The variety in data formats arises from different molecular marker panels and available methodologies. The figure below illustrates how different genotyping approaches capture distinct aspects of the Plasmodium genome and how genomic data are ‘generated’ using different techniques and genotyping platforms. Next-generation sequencing is now the most common approach due to significant decreases in cost and its high-throughput nature both for whole genome sequencing but also sequencing of specific molecular markers or genomic regions of interest, also known as targeted sequencing.\n\n\n Targeted sequencing is similar to WGS, but sample preparation workflow requires an extra step: target enrichment either by hybridization capture or PCR amplicons. These methods are often used for enriching longer and shorter genomic regions, respectively.\n\n\n\nFigure sourced from Ruybal-Pesántez et al 2024, Molecular markers for malaria genetic epidemiology: progress and pitfalls\n\n\n\n\n  Raw sequencing data is processed by bioinformatic pipelines to call alleles and haplotypes. The outputs of this process, such as VCF, FASTA, or haplotype tables, serve as input formats for downstream data analysis tools.\n\n\n\nRaw sequence data from next-generation sequencing platforms is processed through bioinformatic pipelines into structured formats suitable for further downstream genomic analysis. Typically this process involves a series of steps. Initially, the raw data undergoes quality control to remove low-quality reads and contaminants. The cleaned data is then aligned to a reference genome, which helps identify the position of each sequence read. From this alignment, variants can be ‘called’, which means variants are identified through differences between the sample and the reference genome. These variants are compiled into Variant Call Format (.vcf) files. Alternatively, the entire cleaned and aligned sequences can be compiled into FASTA format (.fasta) files, which represent the sequences in a text-based format. Additionally, haplotypes may also be called, which means combinations of alleles at multiple loci are identified. These haplotypes are often tabulated in .csv or text-based formats.\n\n\n  Bioinformatic pipelines in malaria genomics need to account for the unique characteristics of Plasmodium genomes, such as high AT content and extensive polymorphism.\n\n\n\nWhen working with Plasmodium genomic data, several input file formats are commonly used across different analysis tools:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.vcf (Variant Call Format): Widely used for storing gene sequence variants, such as SNPs, insertions, deletions, and other types of genetic variants. The standard format includes a mandatory header starting with # or ## in the case of ‘special header keywords’, which provides the metadata such as file format, reference genome. This is followed by the ‘body’, which is tab-separated and each line represents a variant and its relevant information, such as chromosome and position, reference and alternate allele(s), and sample genotype information.\n\n\n\n\n  Figure sourced and modified from Wikipedia\n\n.fasta: Widely used for representing nucleotide and protein sequences, in which nucleotides or amino acids are represented using single-letter codes. Each entry begins with a header line starting with &gt; followed by a unique sequence ID and description line, and then the sequence data itself, with one letter per nucleic or amino acid.\n\n\n\n\n Figure sourced from Wikipedia\n\n.txt or .csv: Simple text and comma-separated values formats that are often used to represent haplotype or count data.",
    "crumbs": [
      "Data"
    ]
  },
  {
    "objectID": "website_docs/data_description.html#available-datasets-in-pgeforge",
    "href": "website_docs/data_description.html#available-datasets-in-pgeforge",
    "title": "Data",
    "section": "Available datasets in PGEforge",
    "text": "Available datasets in PGEforge\nAs part of the PGEforge community resource, we have compiled simulated and empirical datasets of these common data formats. These datasets are used in the tool tutorials to make them fully reproducible and are freely available for anyone to use.\nMore details on the datasets hosted on PGEforge can be found in the links below:\n\nWhole genome sequencing (WGS)\nMicrohaplotype data\nSNP barcoding data\npfhrp2/3 deletion count data\n\nThey can be accessed at the PGEforge/data folder on Github.",
    "crumbs": [
      "Data"
    ]
  },
  {
    "objectID": "website_docs/use_cases_functionalities.html",
    "href": "website_docs/use_cases_functionalities.html",
    "title": "Mapping use cases to core analysis functionalities",
    "section": "",
    "text": "For the eight use cases, we identified distinct analysis functionalities that are required or useful to obtain the desired results from Plasmodium genetic data (as well as contextual data, eg epidemiological variables, travel history). Some of them are required for several use cases, for example, estimating complexity of infection (COI) is required or useful for all, which makes intuitive sense since analysis of genetic data from monoclonal infections will often need to be treated differently than that from polyclonal infections.\nThe table below shows analysis functionalities required for each use case (denoted with X’s).\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Analysis workflows",
      "Analysis functionalities"
    ]
  },
  {
    "objectID": "website_docs/future_work.html#the-pgeforge-vision",
    "href": "website_docs/future_work.html#the-pgeforge-vision",
    "title": "What’s in store for the future?",
    "section": "The PGEforge vision",
    "text": "The PGEforge vision\nPGEforge is a community-driven platform created by and for malaria genomic epidemiology analysis tool developers and end-users. If you are interested in contributing to these efforts, please get involved!",
    "crumbs": [
      "How to contribute",
      "Future work"
    ]
  },
  {
    "objectID": "website_docs/future_work.html#tool-benchmarking",
    "href": "website_docs/future_work.html#tool-benchmarking",
    "title": "What’s in store for the future?",
    "section": "Tool benchmarking",
    "text": "Tool benchmarking\nThe process of evaluating tools involves assessing their adherence to objective software standards from both the end-user and developer perspectives. This is a great way to determine how well we are doing at making our tools accessible and usable by anyone. However, it is equally important to evaluate how well tools perform their intended functions and how they compare to other tools designed for the same purpose.\nThis type of evaluation could be achieved through formal benchmarking of different tools and their analysis functionalities. We can use canonical simulated datasets with known ground truth to evaluate the accuracy, sensitivity, and specificity of the tools in producing desired outcomes. What do we mean by ground truth? For example, we can simulate data to have specific data features that we are interested in benchmarking, including complexity of infection (COI), missingness, sequencing errors and parasitemia. We can then evaluate how well the tools estimate these features. With empirical “real-world” datasets, a ground truth may not be known, but we can use these datasets to compare consistency of each tool obtaining the same outcomes.\nBy systematically benchmarking tools against known datasets, we can ensure that the tools not only meet software standards but also perform reliably in practical applications, such as key use cases for Plasmodium genomic epidemiology. We plan to host these types of benchmarking resources and results in PGEforge in the near future.\n\n\nOngoing work in this space includes benchmarking COI estimation tools.",
    "crumbs": [
      "How to contribute",
      "Future work"
    ]
  },
  {
    "objectID": "website_docs/future_work.html#adding-and-evaluating-more-tools",
    "href": "website_docs/future_work.html#adding-and-evaluating-more-tools",
    "title": "What’s in store for the future?",
    "section": "Adding and evaluating more tools",
    "text": "Adding and evaluating more tools\nOur initial efforts to landscape available tools were focused on tools for downstream Plasmodium genetic analysis and included 40 available tools. However, future work could include the following areas (not an exhaustive list!):\n\nTools with applications to mosquito genetics, as long as they are generally within the scope of PGEforge (i.e. Plasmodium genomic epidemiology tools)\nBioinformatic pre-processing tools/pipelines, such as variant calling and quality filters\nTools that are not specifically engineered for Plasmodium but can be applied to Plasmodiumgenetic data, such as more generic population genetics tools that estimate metrics such as F-statistics, extended haplotype homozygosity, etc",
    "crumbs": [
      "How to contribute",
      "Future work"
    ]
  },
  {
    "objectID": "website_docs/future_work.html#thank-you",
    "href": "website_docs/future_work.html#thank-you",
    "title": "What’s in store for the future?",
    "section": "Thank you!",
    "text": "Thank you!\nThank you for your interest in contributing to PGEforge. Your efforts help us build a stronger, more inclusive research community making Plasmodium genomic analysis accessible to all!",
    "crumbs": [
      "How to contribute",
      "Future work"
    ]
  },
  {
    "objectID": "website_docs/benchmarking.html",
    "href": "website_docs/benchmarking.html",
    "title": "Systematic benchmarking of tools",
    "section": "",
    "text": "Watch this space - coming soon!\n\n\n\n Back to top",
    "crumbs": [
      "Analysis workflows",
      "Benchmarking"
    ]
  },
  {
    "objectID": "website_docs/data_counts.html",
    "href": "website_docs/data_counts.html",
    "title": "pfhrp2/3 deletion count data",
    "section": "",
    "text": "The pfhrp2/3 gene deletion count data is available within the subfolder pfhrp2-3_counts. The data come from a study by (Feleke et al. 2021).\n\n\n\n\n Back to topReferences\n\nFeleke, Sindew M, Emily N Reichert, Hussein Mohammed, Bokretsion G Brhane, Kalkidan Mekete, Hassen Mamo, Beyene Petros, et al. 2021. “Plasmodium Falciparum Is Evolving to Escape Malaria Rapid Diagnostic Tests in Ethiopia.” Nature Microbiology 6 (10): 1289–99.",
    "crumbs": [
      "Data",
      "pfhrp2/3 deletion counts"
    ]
  },
  {
    "objectID": "website_docs/workflows.html",
    "href": "website_docs/workflows.html",
    "title": "Putting it all together: analysis workflows",
    "section": "",
    "text": "For a subset of use cases, we have outlined how specific analysis functionalities could be assembled into analysis workflows to obtain required results from the initial genetic or genomic data. It is worth noting that there may be several viable ways to obtain a given result, based on the features of the Plasmodium genetic data. Within a given analysis workflow path several available tools might interchangably provide a given functionality as output. As both analytical tools and analysis approaches mature, some workflows may not be sequential if multiple parameter values can be estimated jointly (eg COI, phase and frequencies).\n\nMonitoring the prevalence/frequency of drug or diagnostic resistance markers\n\n\n\nTo estimate drug resistance marker prevalence, raw genetic data is processed through allele calling and subsequent steps vary depending on the complexity of infection (COI) and whether the population is monoclonal or polyclonal (including monoclonal and/or polyclonal infections). If the population is monoclonal, allele prevalence (prev) and allele frequencies (AF) at a single locus can be estimated directly and will be identical. Additionally, multi-locus prev, AF and copy number variation (CNV) or deletions (del) can be estimated directly if relevant. If the population is polyclonal and phasing is possible, the analysis follows the same pathway as for a monoclonal population. If phasing is not possible or not performed, additional steps are required depending on whether unambiguous multilocus genotypes can be obtained and whether the sequencing (seq) method utilized detects CNV or deletions in polyclonal infections.\n\n\nEstimating transmission intensity\n\n\n\nTo estimate transmission intensity, raw genetic data is processed through allele calling, complexity of infection (COI) and allele frequency (AF) estimation. Subsequent metric estimation involves measuring the proportion of polyclonal infections in the population, between and within-host relatedness and population diversity, which can all be collectively translated into estimates of transmission. Alternative analysis pathways may be used depending on whether phasing is possible or performed.\n\n\nClassifying malaria cases as locally acquired or imported from another population\n\n\n\n\n\n\n\n\n\nTo classify cases as imported or locally acquired raw genetic data is processed through allele calling, followed by estimation of complexity of infection (COI). Then estimation of between host relatedness, geographic assignment, transmission networks and structure and/or clustering metrics can be translated into estimates of importation or local transmission.\n\n\n Back to top",
    "crumbs": [
      "Analysis workflows",
      "Workflows"
    ]
  },
  {
    "objectID": "website_docs/R_universe.html",
    "href": "website_docs/R_universe.html",
    "title": "The PlasmoGenEpi R-universe",
    "section": "",
    "text": "The rOpenSci R-universe system provides a framework for users and developers of R-packages to create a package ‘ecosystem’ in a simple and accessible way. The R-universe build infrastructure provides a high-performance package server, integrated monitoring tools, and REST APIs, including a front-end dashboard, to provide a dynamics and automated R package repository.\n\n\n\n\n\nrOpenSci R-universe logo\n\n\nThis infrastructure provides pre-built packages, which greatly simplifies installation of non-CRAN packages and those with complex dependencies such as C++ compilers. The R-universe platform is used by professionals from different industries including brain research, climate change, infectious disease research and government data analysis. If you are interested in learning more about the R-universe infrastructure, you can check out this blog post and we encourage you to explore the resources at the rOpenSci R-universe website and the R-universe dashboard.\n\nThe plasmogenepi.r-universe\nThe PlasmoGenEpi community has created a dedicated plasmogenepi.r-universe with R packages relevant to Plasmodium genomic analyses.\nThe dashboard build page can be seen below, where a timeline of updates to the packages is displayed as a bar chart, and below it the general information for each package, including links to the Github repository, who is the main maintaner of the package, and when it was last built.\n\n\n\nThe PlasmoGenEpi R-universe build dashboard\n\n\nYou can navigate through to the ‘Packages’ tab for more detailed descriptions of each package, as seen below.\n\n\n\nThe PlasmoGenEpi R-universe package dashboard\n\n\nYou can navigate through to the ‘API’ tab for more details on how to access the r-universe via its API, as seen below.\n\n\n\nThe PlasmoGenEpi R-universe package dashboard\n\n\n\n\nInstallation instructions\nHaving R packages within the R-universe ecosystem makes the installation process much simpler! This is an important aim of our community to ensure that the R packages available for analysis of Plasmodium genomic analysis are easily accessible to end-users.\nBelow we show an example of how you could install the DRpower package via the plasmogenepi.r-universe.\n\n\n\n\n\nYou can see how easy this will be with one line of code! 🙌\n\n\n\n# Install 'DRpower' in R:\ninstall.packages('DRpower', \n                 repos = c('https://plasmogenepi.r-universe.dev', \n                           'https://cloud.r-project.org'))\n\nAll the tool tutorials in PGEforge have installation instructions, and where applicable, provide instructions for installation via the plasmogenepi.r-universe.\n\n\n\n\n Back to top",
    "crumbs": [
      "Tool resources",
      "Installing via the plasmogenepi.r-universe"
    ]
  },
  {
    "objectID": "website_docs/contributors.html",
    "href": "website_docs/contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "PGEforge was supported by the following people, listed alphabetically.\n\n\n\n\n\n\n\n\nName\n\nAffiliations\n\n\n\n\n\n\nImmunology and Infectious Disease, Harvard T.H. Chan School of Public Health\n\n\n\n\nDepartment of Epidemiology, Johns Hopkins Bloomberg School of Public Health\n\n\n\n\nPGY-1, Internal Medicine, Duke University Medical Center\n\n\n\n\nMedical Research Council Unit The Gambia at the London School of Hygiene and Tropical Medicine\n\n\n\n\nEPPIcenter, University of California San Francisco\n\n\n\n\nDepartment of Medicine, University of Massachusetts Chan Medical School\n\n\n\n\nMax Planck Institute for Infection Biology\n\n\n\n\nCentre for Innovation in Infectious Disease and Immunology Research (CIIDIR) The Institute for Mental and Physical Health and Clinical Translation (IMPACT), School of Medicine, Deakin University\n\n\n\n\nEPPIcenter, University of California San Francisco\n\n\n\n\nEPPIcenter, University of California San Francisco\n\n\n\n\nBrown University\n\n\n\n\nLondon School of Hygiene and Tropical Medicine\n\n\n\n\nMRC Centre for Global Infectious Disease Analysis, Department of Infectious Disease Epidemiology, Imperial College London; Instituto de Microbiología, Universidad San Francisco de Quito\n\n\n\n\nBroad Institute of Harvard and MIT; Department of Organismic and Evolutionary Biology, Harvard University; Department of Immunology and Infectious Diseases, Harvard T.H. Chan School of Public Health, Harvard University\n\n\n\n\nPathology and Laboratory Medicine, Brown University\n\n\n\n\nInstitut Pasteur\n\n\n\n\nMRC Centre for Global Infectious Disease Analysis, Department of Infectious Disease Epidemiology, Imperial College London\n\n\n\n\nDepartment of Epidemiology, Johns Hopkins Bloomberg School of Public Health",
    "crumbs": [
      "How to contribute",
      "Contributors"
    ]
  },
  {
    "objectID": "website_docs/contributors.html#main-contributors",
    "href": "website_docs/contributors.html#main-contributors",
    "title": "Contributors",
    "section": "",
    "text": "PGEforge was supported by the following people, listed alphabetically.\n\n\n\n\n\n\n\n\nName\n\nAffiliations\n\n\n\n\n\n\nImmunology and Infectious Disease, Harvard T.H. Chan School of Public Health\n\n\n\n\nDepartment of Epidemiology, Johns Hopkins Bloomberg School of Public Health\n\n\n\n\nPGY-1, Internal Medicine, Duke University Medical Center\n\n\n\n\nMedical Research Council Unit The Gambia at the London School of Hygiene and Tropical Medicine\n\n\n\n\nEPPIcenter, University of California San Francisco\n\n\n\n\nDepartment of Medicine, University of Massachusetts Chan Medical School\n\n\n\n\nMax Planck Institute for Infection Biology\n\n\n\n\nCentre for Innovation in Infectious Disease and Immunology Research (CIIDIR) The Institute for Mental and Physical Health and Clinical Translation (IMPACT), School of Medicine, Deakin University\n\n\n\n\nEPPIcenter, University of California San Francisco\n\n\n\n\nEPPIcenter, University of California San Francisco\n\n\n\n\nBrown University\n\n\n\n\nLondon School of Hygiene and Tropical Medicine\n\n\n\n\nMRC Centre for Global Infectious Disease Analysis, Department of Infectious Disease Epidemiology, Imperial College London; Instituto de Microbiología, Universidad San Francisco de Quito\n\n\n\n\nBroad Institute of Harvard and MIT; Department of Organismic and Evolutionary Biology, Harvard University; Department of Immunology and Infectious Diseases, Harvard T.H. Chan School of Public Health, Harvard University\n\n\n\n\nPathology and Laboratory Medicine, Brown University\n\n\n\n\nInstitut Pasteur\n\n\n\n\nMRC Centre for Global Infectious Disease Analysis, Department of Infectious Disease Epidemiology, Imperial College London\n\n\n\n\nDepartment of Epidemiology, Johns Hopkins Bloomberg School of Public Health",
    "crumbs": [
      "How to contribute",
      "Contributors"
    ]
  },
  {
    "objectID": "website_docs/contributors.html#community-rules",
    "href": "website_docs/contributors.html#community-rules",
    "title": "Contributors",
    "section": "Community rules",
    "text": "Community rules\nAll contributors to PGEforge adhere to our community rules.",
    "crumbs": [
      "How to contribute",
      "Contributors"
    ]
  },
  {
    "objectID": "website_docs/data_mhaps.html",
    "href": "website_docs/data_mhaps.html",
    "title": "Microhaplotype data",
    "section": "",
    "text": "Targeted amplicon data from analysis for the following paper “Sensitive, Highly Multiplexed Sequencing of Microhaplotypes From the Plasmodium falciparum Heterozygome”(Tessema et al. 2022)\nThis contains 82 field samples gathered from northern and southern Mozambique and had 100 targets (91 diversity targets and 9 targeted drug targets).\nThe results file can be found within directory amplicon/moz2018_heome1_results_fieldSamples.tsv.gz along with metadata amplicon/moz2018_fieldSamples_meta.tsv. Results are in a 4 column format.\n\nsample - The name of the sample\n\ntarget - The name of the amplicon target\n\ntarget_popUID - A population identifier for the haplotype for this target for this sample\n\nreadCnt - The read count for this haplotype for this sample for this target\n\n\n\n\nTargeted amplicon data from the same 100 target panel as above. Mixtures are made of various combinations of 7 lab strains of P. falciparum and with some mixtures done in replicate at different 4 different parasite densities (10, 100, 1k, 10K.\n\n\n\nParasite Densities\n\n\n\n\n\nParasite Mixtures\n\n\nResults are organized in a similar 4 column table as above. The results file can be found within directory amplicon/moz2018_heome1_results_controlSamples.tsv.gz along with metadata amplicon/moz2018_controlSamples_meta.tsv, amplicon/samplesToMixFnp.tab.txt, amplicon/mixSetUpFnp.tab.txt.\n\n\n\nTargeted amplicon data was also simulated in silico to create 100 samples sampled from Mozambique and for a newer diversity panel called MAD^4HatTeR with 50 targets selected for thier diversity.\nResults are organized in a similar 4 column table as above. The results file can be found within directory amplicon/mozSim_MAD4HATTERDiversitySubPanel.tab.txt.gz",
    "crumbs": [
      "Data",
      "Microhaplotypes"
    ]
  },
  {
    "objectID": "website_docs/data_mhaps.html#microhaplotype-data",
    "href": "website_docs/data_mhaps.html#microhaplotype-data",
    "title": "Microhaplotype data",
    "section": "",
    "text": "Targeted amplicon data from analysis for the following paper “Sensitive, Highly Multiplexed Sequencing of Microhaplotypes From the Plasmodium falciparum Heterozygome”(Tessema et al. 2022)\nThis contains 82 field samples gathered from northern and southern Mozambique and had 100 targets (91 diversity targets and 9 targeted drug targets).\nThe results file can be found within directory amplicon/moz2018_heome1_results_fieldSamples.tsv.gz along with metadata amplicon/moz2018_fieldSamples_meta.tsv. Results are in a 4 column format.\n\nsample - The name of the sample\n\ntarget - The name of the amplicon target\n\ntarget_popUID - A population identifier for the haplotype for this target for this sample\n\nreadCnt - The read count for this haplotype for this sample for this target\n\n\n\n\nTargeted amplicon data from the same 100 target panel as above. Mixtures are made of various combinations of 7 lab strains of P. falciparum and with some mixtures done in replicate at different 4 different parasite densities (10, 100, 1k, 10K.\n\n\n\nParasite Densities\n\n\n\n\n\nParasite Mixtures\n\n\nResults are organized in a similar 4 column table as above. The results file can be found within directory amplicon/moz2018_heome1_results_controlSamples.tsv.gz along with metadata amplicon/moz2018_controlSamples_meta.tsv, amplicon/samplesToMixFnp.tab.txt, amplicon/mixSetUpFnp.tab.txt.\n\n\n\nTargeted amplicon data was also simulated in silico to create 100 samples sampled from Mozambique and for a newer diversity panel called MAD^4HatTeR with 50 targets selected for thier diversity.\nResults are organized in a similar 4 column table as above. The results file can be found within directory amplicon/mozSim_MAD4HATTERDiversitySubPanel.tab.txt.gz",
    "crumbs": [
      "Data",
      "Microhaplotypes"
    ]
  },
  {
    "objectID": "website_docs/data_wgs.html",
    "href": "website_docs/data_wgs.html",
    "title": "Whole genome sequencing (WGS) data",
    "section": "",
    "text": "All of the data within the subfolder wgs/pf3k was derived from the Pf3k Project. Currently, there are three VCF files, with corresponding CSVs containing metadata, for samples from: - Democratic Republic of the Congo (\\(n=113\\)) - Vietnam (\\(n=97\\)) - In vitro mixtures of laboratory strains (\\(n=25\\))\nEach VCF contains 247,496 high-quality (VQSLOD&gt;6) biallelic SNPs across all fourteen somatic chromosomes. The VCFs are sorted and an index file is provided. The Fws statistics provided in the metadata CSVs were collected from the Pf7 data set, which contains the Pf3k samples. These were not calculated for the in vitro lab mixtures.\n\n\n\nAll of the data within the subfolder wgs/simulated was simulated. In brief, a simulated sample with a given complexity of infection (COI), \\(K\\), is created by randomly sampling \\(K\\) clonal haplotypes (\\(F_{ws} &gt; 0.95\\)) from a given country within the Pf3k Project, assigning these haplotypes to \\(j \\leq K\\) bites, simulating meiosis if \\(j &lt; K\\), randomly sampling proportions for each haplotype, and then simulating read count data given the proportions and final genotypes. Sequencing error is simulated at a fixed rate and present in the read counts. No variant calling error is simulated; the genotypes are perfect. At present, there is only one VCF file with a corresponding CSV and BED file containing metadata, with samples simulated from: - Democratic Republic of the Congo (\\(n=40\\))\nThe COI of these samples ranges from one to four, and about half of them have within-host relatedness.\n\n\n\nThere are a set of bam files with vcf calls subsetting to just CSP (PF3D7_0304600), CELTOS (PF3D7_1133400), and AMA1 (PF3D7_1216600). These can be found within the wgs/labisolate_subset directory. With metadata describing what is in each file wgs/labisolate_subset/allControlMixtures.tab.txt, wgs/labisolate_subset/allControlSampNameToMixName.tab.txt",
    "crumbs": [
      "Data",
      "WGS"
    ]
  },
  {
    "objectID": "website_docs/data_wgs.html#p.-falciparum-wgs-data",
    "href": "website_docs/data_wgs.html#p.-falciparum-wgs-data",
    "title": "Whole genome sequencing (WGS) data",
    "section": "",
    "text": "All of the data within the subfolder wgs/pf3k was derived from the Pf3k Project. Currently, there are three VCF files, with corresponding CSVs containing metadata, for samples from: - Democratic Republic of the Congo (\\(n=113\\)) - Vietnam (\\(n=97\\)) - In vitro mixtures of laboratory strains (\\(n=25\\))\nEach VCF contains 247,496 high-quality (VQSLOD&gt;6) biallelic SNPs across all fourteen somatic chromosomes. The VCFs are sorted and an index file is provided. The Fws statistics provided in the metadata CSVs were collected from the Pf7 data set, which contains the Pf3k samples. These were not calculated for the in vitro lab mixtures.\n\n\n\nAll of the data within the subfolder wgs/simulated was simulated. In brief, a simulated sample with a given complexity of infection (COI), \\(K\\), is created by randomly sampling \\(K\\) clonal haplotypes (\\(F_{ws} &gt; 0.95\\)) from a given country within the Pf3k Project, assigning these haplotypes to \\(j \\leq K\\) bites, simulating meiosis if \\(j &lt; K\\), randomly sampling proportions for each haplotype, and then simulating read count data given the proportions and final genotypes. Sequencing error is simulated at a fixed rate and present in the read counts. No variant calling error is simulated; the genotypes are perfect. At present, there is only one VCF file with a corresponding CSV and BED file containing metadata, with samples simulated from: - Democratic Republic of the Congo (\\(n=40\\))\nThe COI of these samples ranges from one to four, and about half of them have within-host relatedness.\n\n\n\nThere are a set of bam files with vcf calls subsetting to just CSP (PF3D7_0304600), CELTOS (PF3D7_1133400), and AMA1 (PF3D7_1216600). These can be found within the wgs/labisolate_subset directory. With metadata describing what is in each file wgs/labisolate_subset/allControlMixtures.tab.txt, wgs/labisolate_subset/allControlSampNameToMixName.tab.txt",
    "crumbs": [
      "Data",
      "WGS"
    ]
  }
]